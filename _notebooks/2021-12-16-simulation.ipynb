{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021-12-16-simulation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMttyvO9aaRJ3VBC9KqSAsH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PMzhwKYtm51E"},"source":["# Ising model dynamics\n","\n","- toc: true \n","- badges: true\n","- comments: false\n","- categories: [jupyter]"]},{"cell_type":"markdown","source":["___\n","_project outline_"],"metadata":{"id":"b_qPK0qTPRhK"}},{"cell_type":"markdown","metadata":{"id":"qvFJrPBxalKZ"},"source":["$\\quad$ These notes supplement an ongoing project with a good friend, [Connor Sempek](https://github.com/connorsempek). Our main objective is to re-create the simulations of Ising model dynamics in [this](http://bit-player.org/2021/three-months-in-monte-carlo) blog post by [Brian Hayes](http://bit-player.org/about-the-author). The underlying 'canvas' of the model is two-dimensional, and the simulation generates a sequence of two-color `100x100` pixel images. The two colors represent the two possible _spin_ values in the Ising model, $+1$ and $-1$ (though the term spin is used, nothing about the model is quantum-mechanical). The magnetic interpretation is of the model is that each pixel is an atomic site in some atomic lattice, and the value of each pixel describes one of two opposite magnetic orientations of the site. This is like a microscopic picture of a bar of iron. Ferromagnetism specifically means that spins at neighboring sites are encouraged to align."]},{"cell_type":"markdown","source":["$\\quad$ Importantly, the Ising model comes with a tunable temperature parameter $T > 0$. The model is nicer mathematically when parametrizing it by the _inverse temperature_ $\\beta \\equiv T^{-1}$, so we do this below. Parameter $\\beta$ controls the _degree_ to which the Ising model exhibits ferromagnetic behavior. At high enough temperatures, making $\\beta$ close to zero, the impulse neighboring spins feel to align is nearly washed away by thermal noise. The Ising model exhibits radically different 'collective' behavior at lower temperatures, i.e. when $\\beta$ is very large. This behavior change happens at a specific inverse temperature $\\beta_c$, corresponding to some _critical temperature_ $T_c \\equiv \\beta_c^{-1}$. "],"metadata":{"id":"PxQw7_n8x8SZ"}},{"cell_type":"markdown","source":["$\\quad$ For the same phenomenon in iron, this $T_c$ corresponds to the bar glowing a dull red. An unmagnetized iron bar heated above $T_c$, which is then immersed in a strong enough magnetic field will align itself with the field. Cooling the iron below $T_c$, the bar remains aligned with the field even after it's shut off. This is a way to think about the phase transition, though one doesn't need an external field to formulate this, and our initial simulation will not use one. What is changing about the system across the two temperature regimes is the way in which the spins collectively align."],"metadata":{"id":"9mjlwu8j70tl"}},{"cell_type":"markdown","source":["$\\quad$ The Ising model dynamics considered update the image one pixel value at a time. The purpose of these notes is to develop an understanding of how long the simulation must be run so that, for a sparse enough subsequence of the images:\n","\n","* Each is sampled approximately from the [Gibbs probability measure](https://en.wikipedia.org/wiki/Gibbs_measure) governing the Ising model, and \n","\n","* these samples are nearly independent."],"metadata":{"id":"m-GsQZ6Hx8ex"}},{"cell_type":"markdown","source":["$\\quad$ The latter property is especially important for a longer term goal of the project, which is to use these subsequences of images as a computer vision dataset. Importantly, the Gibbs measure we are sampling from always depends on a global temperature parameter $T$, leading to natural classification and regression tasks."],"metadata":{"id":"SBK7w-YTq3qX"}},{"cell_type":"markdown","source":["___\n","\n","_to explore_"],"metadata":{"id":"1O-HulGHOFgp"}},{"cell_type":"markdown","source":["$\\quad$ Here is a starting point for formulating a classification task. Suppose that we can simulate Ising model dynamics well at two _distinct_ temperatures. It feels natural to choose one of these, say $T_1$, above the critical temperature $T_c$ of the model, and the other, $T_2$ below it. We then create two image datasets $\\mathcal{D}_1$ and $\\mathcal{D}_2$ of equal size by simulating as described by the two points above. Merging these datasets to form what we call $\\mathcal{D}$ leads to a natural classification problem: given an element $x \\in \\mathcal{D}$ chosen uniformly at random, determine whether it came from $\\mathcal{D}_1$ or $\\mathcal{D}_2$. "],"metadata":{"id":"W7YA8ZaTq-Nh"}},{"cell_type":"markdown","source":["$\\quad$ It seems the relevance of the Ising model to vision has been explored. A lot of the text below draws from Govind Menon's [pattern theory](https://www.dam.brown.edu/people/menon/publications/pt-2020.pdf) notes, where a chapter is devoted to this 'energetic' perspective applied to character recognition tasks. Menon's notes also reference a 1984 [paper](http://www.peterbeerli.com/classes/images/a/a1/Geman_geman_1984.pdf) which develops a way to perform image restoration and enhancement using the Ising model. This makes it a natural model to experiment with, and there are a few directions to explore longer term:\n","\n"],"metadata":{"id":"nmyNdro9s0sU"}},{"cell_type":"markdown","source":["1. At the lower temperature, $T_2$, simulating the model is akin to an optimization problem: as $T \\to 0$, the Gibbs measure concentrates on the set of lowest-energy states, or _ground states_. Any simulation designed to sample from a low-temperature Gibbs measure needs to navigate the energy landscape of the system from some starting point to a low altitude, with the temperature dictating a small range of desirable low altitudes $\\equiv$ low energies. This is effectively an optimization problem with cost function corresponding to the energy landscape. There is also an optimization inherent to the process of training any neural network on the classification task suggested. The question to explore, loosely, is whether there are useful connections between these optimization problems. "],"metadata":{"id":"EbzmcHHafKhQ"}},{"cell_type":"markdown","source":["\n","2. We will train a simple convolutional architecture on the dataset $\\mathcal{D}$. Whether it is through pooling layers, or the local averaging inherent in the convolution operation, these architectures seem to apply a kind of [coarse-graining](https://en.wikipedia.org/wiki/Coarse-grained_modeling) to input signals $x \\in \\mathcal{D}$. This has prompted investigation into the connections between deep learning and the [renormalization group](https://en.wikipedia.org/wiki/Renormalization_group) of physics. The latter feels like a kind of microscope for theoretical physicists $-$ the renormalization group formalizes a sequence of scaling transformations acting on a physical system, typically zooming out and thereby coarse-graining the preceding image of the system. This parallels feeding the image of the system into a \"deep\" sequence of convolutional layers. Two references for this are a 2013 [paper](https://arxiv.org/abs/1301.3124) of Cédric Bény, and a 2014 [paper](https://arxiv.org/abs/1410.3831v1) of Pankaj Mehta and David Schwab, and we imagine there is more recent work on this to explore. Dataset $\\mathcal{D}$ is both an image dataset and a natural input to the renormalization group. It's an opportunity to better understand these connections. In particular one can start by repeating the experiments the latter paper runs on the Ising model."],"metadata":{"id":"zJGz1qQt4la7"}},{"cell_type":"markdown","source":["# Markov chains, Monte Carlo methods, pattern theory"],"metadata":{"id":"SZoWmupe1xh4"}},{"cell_type":"markdown","metadata":{"id":"xlMuxTrt1jMI"},"source":["___\n","\n","_references_\n","\n","* [Werner Krauth](http://www.lps.ens.fr/~krauth/index.php/Main_Page) ... Statistical Mechanics: Algorithms and Computations\n","\n","* [Govind Menon](https://www.dam.brown.edu/people/menon/) ...  [Pattern Theory](https://www.dam.brown.edu/people/menon/new_pub.html)\n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"MXliFLeynRga"},"source":["$\\quad$ The Monte Carlo method is increasingly becoming part of the discipline it is meant to study. The Monte Carlo method is a statistical (Krauth: _\"almost experimental\"_) approach to computing integrals using random positions, called __samples__. Some sampling techniques for continuous and discrete variables are discussed below. In probabilistic terms, the integrals we wish to approximate correespond to the expected value of some observable, $\\mathcal{O} : \\mathcal{S} \\to \\mathbb{R}$. Here we assume some implicit probability measure $m$ on $\\mathcal{S}$, the latter called the __state space__. Formally, the expected value $\\mathbb{E}_m \\mathcal{O}$ corresponds to the integral\n","$$\n","\\mathbb{E}_m \\mathcal{O} = \\int_{ \\mathcal{S} } \\mathcal{O}(s) \\,  m( \\textrm{d} s )\n","$$\n","When $m$ is understood, we write this expectation either as $\\mathbb{E} \\mathcal{O}$, or as $\\langle \\mathcal{O} \\rangle$, the latter notation more common in physics.   "]},{"cell_type":"markdown","metadata":{"id":"dW4JtbIKqwVA"},"source":["$\\quad$ We want to simulate most of the objects discussed throughout the notes, so we lose no generality in assuming that $\\mathcal{S}$ is finite. $\\mathcal{S}$ will be treated as a continuous object when it is more natural or convenient notationally. In the setting where $\\mathcal{S}$ is finite, probability measures $m$ on $\\mathcal{S}$ correspond to non-negative vectors $(m_i)_{i=1}^{\\# \\mathcal{S}}$ of length $\\#\\mathcal{S}$ whose entries sum to one. This vector is identical to the probability mass function (pmf) of $m$. We distinguish between two approaches for sampling from $m$: __direct sampling__ and __Markov chain sampling__. We postpone giving the definition of a Markov chain. "]},{"cell_type":"markdown","metadata":{"id":"p7CJ-ZhlrLHu"},"source":["## direct sampling\n","\n","_(Krauth 1.1.1)_"]},{"cell_type":"markdown","metadata":{"id":"Q1ABNGCsrOMt"},"source":["$\\quad$ Krauth explains the concept of direct sampling through a game played with pebbles. The playing field is a large circle is inscribed into a large square, both drawn in the sand. In direct sampling, one has 'access' to the measure $m$ on $\\mathcal{S}$. This means we can draw a collection of i.i.d. random variables $(X_i)_{i=1}^\\infty$ taking values in $\\mathcal{S}$, each with pmf corresponding to $m$. In the context of the game, we will assume we can directly sample from $m$, which here denotes the uniform measure on the square \n","$$\n","\\mathcal{S} \\equiv \\textsf{square} = [-1,1]^2   \\,,\n","$$ \n","a 'birds-eye' view of the playing field. The observable considered in the game is related to the game's objective: to compute an approximation of the number $\\pi$. The observable $\\mathcal{O} : \\mathcal{S} \\to \\mathbb{R}$ is the indicator function of the unit disc, \n","$\\textsf{disc} = \\{ s = (x,y) \\in \\textsf{square} : x^2 + y^2 \\leq 1 \\}$\n","$$\n","\\mathcal{O} \\equiv \\mathbf{1}_{ \\textsf{disc}} =\n","\\begin{cases}\n","1 & \\quad s \\in \\textsf{disc} \\\\\n","0 & \\quad \\textrm{otherwise}\n","\\end{cases} \n","$$"]},{"cell_type":"markdown","metadata":{"id":"_o5xGwUTWM1V"},"source":["$\\quad$ A player throws pebbles at the square, successively. Through the direct sampling, these land uniformly at random. The sampling is performed independently across draws from $m$. In particular, no pebble throw ever misses the square. Let $S_1, \\dots, S_N$ denote the collection of locations sampled from $\\textsf{square}$ through the above procedure: each $S_i$ is a random ordered pair $S_i = (x_i, y_i) \\in \\textsf{square}$, where the collection of all coordinates, across all pairs, is mutually independent, with eeach coordinate a $\\textrm{Unif}([-1,1])$ random variable.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jFnCx8JBXf4W"},"source":["$\\quad$ To describe how the samples $S_i$ are used to approximate the value of $\\pi$, we introduce the __empirical measure__ $m_N^{\\textrm{direct}}$, a random probability measure on $\\mathcal{S}$, defined so as to encode the locations $S_1, \\dots, S_N$: \n","$$\n","m_N^{\\textrm{direct}} := \\frac{1}{N} \\sum_{i=1}^N \\delta_{S_i},\n","$$\n","where in general $\\delta_S$ denotes the delta mass at some random point $S$.  Measures are in a sense dual to sets, and the __delta mass__ $\\delta_X$ is a measure acting on sets in the following way: given measurable $B \\subset \\textsf{square}$, we have\n","$$\n","\\delta_S = \n","\\begin{cases}\n","1 & \\quad \\textrm{ if } S \\in B, \\\\\n","0 & \\quad \\textrm{ otherwise. }\n","\\end{cases}\n","$$\n","The empirical measure $m_N^{\\textrm{direct}}$, by definition, acts on sets $B \\subset \\mathcal{S}$ by reporting the fraction of the direct samples $S_i$ landing in $B$. "]},{"cell_type":"markdown","metadata":{"id":"iFLP2wJTWJoS"},"source":["$\\quad$ There is a sense in which the measures $m_N^{\\textrm{direct}}$ 'converge' to $m$, and the game (or simulation) computes $\\pi$ in a way which leverages this. Let us enumerate the states of $\\mathcal{S} = \\{s_1, \\dots, s_M \\}$. In practice, the size of $M$ will be determined by the resolution of the floating points used in sampling. In accordance with the observable $\\mathcal{O}$, the set we choose for $B$ is $\\textsf{disc}$. This is because\n","$$\n","m_N^{\\textrm{direct}}( \\textsf{disc} ) = \\frac{1}{N} \\sum_{i=1}^N \\delta_{S_i}( \\textsf{disc} ) \\equiv \\frac{1}{N} \\sum_{i=1}^M \\mathcal{O}(S_i)\n","$$\n","The sense in which $m_N^{\\textrm{direct}}$ converges to $m$ implies, in particular, that the sequence of numbers $m_N^{\\textrm{direct}}(\\textsf{disc})$ converges to $m( \\textsf{disc} )$ as $N \\to \\infty$. This convergence can also be seen as a consequence of a law of large numbers. The output of the first algorithm below is \n","$$\n","4 \\cdot m_N^{\\textrm{direct}}(\\textsf{disc}) \\approx 4 \\cdot m( \\textsf{disc}) \\equiv 4 \\cdot \\frac{ \\pi}{4} \n","$$\n","Thus we are approximating $\\pi$ by approximating $\\pi/4$, which is the probability that a pebble sampled from $m$ lands in $\\textsf{disc}$. Python imports:"]},{"cell_type":"code","metadata":{"id":"1LRbgOiezyuu","executionInfo":{"status":"ok","timestamp":1640286057058,"user_tz":360,"elapsed":116,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["#collapse-hide\n","\n","import numpy as np\n","import sys\n","from numpy.random import default_rng\n","\n","rng = default_rng()"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WKFM253wdpI7"},"source":["$\\quad$ The next function embodies the 'direct' sampling we supposedly have access to, with the independence assumptions described above. "]},{"cell_type":"markdown","metadata":{"id":"Co0SE-jU0Dg7"},"source":["___\n","\n","### Unif"]},{"cell_type":"code","metadata":{"id":"oic2NJ1U0Dx-","executionInfo":{"status":"ok","timestamp":1640286057200,"user_tz":360,"elapsed":8,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["#collapse-hide\n","# uniform r.v. over interval [0,1]\n","def Unif():\n","    return rng.uniform()"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2twWt41N0D63"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"wPetKay_dUfe"},"source":["$\\quad$ Algorithm 1.1 is translated from pseudocode (as it is presented in Krauth) to Python."]},{"cell_type":"markdown","metadata":{"id":"U4CGL4fMzoCg"},"source":["___\n","\n","### __Algorithm.__ ( direct_pi )"]},{"cell_type":"code","metadata":{"id":"FfTRHDfSzq9p","executionInfo":{"status":"ok","timestamp":1640286057201,"user_tz":360,"elapsed":7,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["#collapse-hide\n","def direct_pi(N = 4000):\n","\n","    N_hits = 0\n","\n","    \"\"\"\n","    the for loop is basically about computing the sum in\n","    \\mu_N(disc), stored in N_hits\n","    \"\"\"\n","\n","    for i in range(N):\n","\n","        # initialize Unif([-1,1]) r.v.'s\n","        x = 2 * Unif() - 1\n","        y = 2 * Unif() - 1\n","\n","        rad_sq = (x ** 2) + (y ** 2)\n","\n","        if rad_sq <= 1:\n","\n","            N_hits += 1\n","    \"\"\"\n","    \\mu_N(disc) is then computed by dividing by N\n","    \"\"\"\n","    ratio = N_hits / N\n","\n","    return 4 * ratio"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rnV5EJo0Lpb"},"source":["___"]},{"cell_type":"markdown","source":["$\\quad$ Running the above:"],"metadata":{"id":"4NHkpm4_AS_e"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vYSpMWqj0MDg","executionInfo":{"status":"ok","timestamp":1640286057342,"user_tz":360,"elapsed":147,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"cc6cb61b-4dc9-4e6c-cd84-0776e9f951f6"},"source":["#collapse-hide \n","direct_pi()"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.103"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"bio6STkV1zRQ"},"source":["$\\quad$ In Krauth, Table 1.1 gives results of five runs of the above with `N = 4000`. We return to this table later to compare with Monte Carlo methods. He remarks that none of the results of this table have fallen in the error bounds known since Archimedes.\n","\n","| run | `N_hits` | estimate of $\\pi$ |\n","| -------- | -------- | --- |\n","| 1 | 3156 | 3.156 |\n","| 2 | 3150 | 3.150 |\n","| 3 | 3127 | 3.127 |\n","| 4 | 3171 | 3.171 |\n","| 5 | 3148 | 3.148 |\n","\n"," Another remark of Krauth: _\"The people adopt a sensible rule: they decide on the total number of throws, before they start the game.\"_ (This is as in the above python code.) _\"They understood that one must not stop a stochastic calculation simply because the current result appears accurate, nor should they continue to play because the answer they get isn't close enough to their idealized target.\"_\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gUTKvlzO2vOz"},"source":["## Markov-chain sampling\n","\n","_( Krauth 1.1.2 )_"]},{"cell_type":"markdown","metadata":{"id":"psgikwml4MvA"},"source":["$\\quad$ Cconsider a new game played on the same field, but with different rules. We again take a birds-eye view of the playing field, so our state space is again $\\mathcal{S} = \\textsf{square}$. The game starts with a person at the corner of the helipad, corresponding to coordinates $(1,1)$ in $\\mathcal{S}$. The walker takes a step (or attempts to) by adding independent, $\\textrm{Unif}([-\\delta, \\delta])$ random variables to each coordinate. Thus it is possible for a next step which takes the person outside the square. If this is the case, instead of taking the step, the walker does nothing. This step still 'counts' in the aggregation to be performed. Krauth describes this procedure in terms of the pebbles: the person takes another pebble from their bag and places it on top of the one pebble (or perhaps pebble pile) marking their current location. After an 'in-bounds' step is taken, the walker places a pebble at their current position. Each walker is equipped with a bag of infinitely many pebbles. These two cases describe the way the new game is played. "]},{"cell_type":"markdown","metadata":{"id":"kEBBTTSLYW9G"},"source":["$\\quad$ In the present setting, the __samples__ $X_1, \\dots, X_N$ are not independent, but rather have the structure of a Markov chain, specifically a random walk with independent increments. Let us use $\\xi_1, \\dots, \\xi_N$ denote these independent increments. In Krauth's description, each of these is a $\\textrm{Unif}([-\\delta, \\delta])$ random variable. For reasons that Krauth discusses later, and we will comment on this, we instead assume that each $\\xi_j$ is a discrete random variable uniformly distributed on the two-element set $\\{ -\\delta, \\delta \\}$. We have distinguished the starting point of the sampling at $X_0 = (1,1)$. Following the above description, one then has\n","$$\n","X_1 = \n","\\begin{cases}\n","X_0 + \\xi_1\\,, & \\quad \\textrm{ if } X_0 + \\xi_1 \\in \\textsf{square}\\\\\n","X_0 & \\quad \\textrm{ otherwise }\n","\\end{cases}\n","$$\n","\n","and in general,\n","\n","$$\n","X_j = \n","\\begin{cases}\n","X_{j-1} + \\xi_j\\,, & \\quad \\textrm{ if } X_{j-1} + \\xi_j \\in \\textsf{square}\\\\\n","X_{j-1} & \\quad \\textrm{ otherwise }\n","\\end{cases}\n","$$\n","\n","for all $j = 1, \\dots, N$."]},{"cell_type":"markdown","metadata":{"id":"BnBlpKW3lof0"},"source":["$\\quad$ For this new sequence of random $X_i$, we can define the analogous object to $m_N^{\\textrm{direct}}$, \n","$$\n","m_N^{\\textrm{Markov}} := \\frac{1}{N} \\sum_{i=1}^N \\delta_{X_i}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"uNhfAcxrs0Yj"},"source":["The algorithm below returns an approximation to $\\pi$ via the same kind of ratio\n","$$\n","\\pi \\approx 4 \\cdot m_N^{\\textrm{Markov}}(\\textsf{disc}) \n","$$"]},{"cell_type":"markdown","metadata":{"id":"ye3ly8tSzq5H"},"source":["$\\quad$ To comment on avoiding random initial conditions, specifically those which assign independent $\\textrm{Unif}([-1,1])$ random variables to the two coordinates: we avoid this because the Markov chain methods stand out when no direct sampling method exists. It is for exactly this reason that we also avoid directly sampling from $\\textrm{Unif}([-\\delta, \\delta])$ random variables: such objects are a linear transformation away from directly sampling from the $\\textrm{Unif}([-1,1])$ distribution, perhaps at lower resolution. So, our Markov-chain sampling simulations start at the corner $(1,1)$ for concreteness. They can also be assumed to start from where a previous simulation left off. Following this convention throughout these notes, we usually focus on going from configuration $i$ to configuration $i+1$. This is the defining property of (discrete-time) Markov chains: the transition probabilities of the process depend only on the current position. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"SEy5vsQODx9z"},"source":["___\n","\n","### __Algorithm.__ ( markov-pi, unmodified )"]},{"cell_type":"code","metadata":{"id":"Xvvimks0D29z","executionInfo":{"status":"ok","timestamp":1640286057343,"user_tz":360,"elapsed":4,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"source":["#collapse-hide\n","\n","def markov_pi( delta = .05, N = 40000 ):\n","\n","    # initial conditions\n","    x = 1\n","\n","    y = 1\n","\n","    N_hits = 0\n","\n","    num_rej_moves = 0 # number of \"rejected\" moves\n","\n","    for i in range(N):\n","\n","        # get coordinate increments\n","        del_x_big = 2 * Unif() - 1\n","\n","        del_y_big = 2 * Unif() - 1\n","\n","        del_x = delta * del_x_big\n","\n","        del_y = delta * del_y_big\n","\n","        #print(del_x, del_y)\n","\n","        # potential new coordinates\n","        x_new = x + del_x\n","\n","        y_new = y + del_y\n","\n","        # to determine if step leads into disc\n","        rad_sq_new = ( x_new ** 2 ) + ( y_new ** 2 )\n","\n","        # ditto for square\n","        x_in = x_new <= 1 and x_new >= -1\n","\n","        y_in = y_new <= 1 and y_new >= -1\n","\n","        in_square = x_in and y_in \n","\n","        # if move is accepted\n","        if in_square:\n","\n","            x = x_new\n","\n","            y = y_new\n","\n","        else:\n","\n","            num_rej_moves += 1\n","\n","        # if in disc, +1 to number of hits\n","        if rad_sq_new <= 1:\n","\n","            N_hits += 1\n","\n","    # we add in this output to \n","    # better understand how to pick \n","    # delta\n","    print( \"fraction of successful moves: \", 1 - ( num_rej_moves / N) )\n","\n","    return 4 * ( N_hits / N )\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmu4xwohD3F_"},"source":["___"]},{"cell_type":"markdown","source":["$\\quad$ Running:"],"metadata":{"id":"QTki13WNA4Hm"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jq0UNtjTKPYY","executionInfo":{"status":"ok","timestamp":1640286057575,"user_tz":360,"elapsed":236,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"50f66d92-1f8a-40e3-9413-5064ff225989"},"source":["#collapse-hide\n","\n","markov_pi()"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["fraction of successful moves:  0.976\n"]},{"output_type":"execute_result","data":{"text/plain":["3.104"]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"icDEcI4TFLyc"},"source":["$\\quad$ Let's discuss the choice of 'throwing range', namely $\\delta$. It is akin to (largest possible) step size for the walker. Roughly, if $\\delta$ is too small, the acceptance rate may be high, but the claim is that the walk will not be able to explore enough to well-approximate the integral. Choosing $\\delta$ too large means the walk will have a hard time making successful moves, also hindering exploration. Krauth: _\"The time-honored rule of thumb consists in choosing $\\delta$ neither too large, nor too small, so that the acceptance rate turns our to be on the order of $\\frac{1}{2}$.\"_ I didn't observe results agreeing with their rule of thumb. I found much more success with $\\delta = 0.05$ versus their recommendation of $0.3$. I did however observe the approximation worsen as the step size was taken larger past $\\delta = 1$. "]},{"cell_type":"markdown","source":["___\n","\n","### __Algorithm.__ ... markov-pi (modified)"],"metadata":{"id":"mp2-7ijlBgvE"}},{"cell_type":"code","source":["#collapse-hide\n","\n","# in this modification, steps are no longer uniform  \n","\n","def markov_pi_modified( delta = .05, N = 40000 ):\n","\n","    # initial conditions\n","    x = 1\n","\n","    y = 1\n","\n","    N_hits = 0\n","\n","    num_rej_moves = 0 # number of \"rejected\" moves\n","\n","    for i in range(N):\n","        \"\"\"\n","        we use uniform random variables, out of convenience but not to\n","        determine coordinate steps directly with these. Instead, we \n","        extract a pair of fair coin tosses from two uniform random\n","        variables. We use these coin tosses only to determine the sign\n","        of the increment in each coordinate. \n","        \"\"\"\n","\n","        # generate indep uniforms\n","        U_1 = Unif()\n","\n","        U_2 = Unif()\n","\n","        # use these as coin tosses to determine \n","        # del_x...\n","\n","        if U_1 <= 0.5:\n","        \n","            del_x = delta\n","\n","        else:\n","\n","            del_x = (-1) * delta\n","\n","        if U_2 <= 0.5:\n","\n","            del_y = delta\n","\n","        else:\n","\n","            del_y = (-1) * delta\n","\n","        # potential new coordinates\n","        x_new = x + del_x\n","\n","        y_new = y + del_y\n","\n","        # to determine if step leads into disc\n","        rad_sq_new = ( x_new ** 2 ) + ( y_new ** 2 )\n","\n","        # ditto for square\n","        x_in = x_new <= 1 and x_new >= -1\n","\n","        y_in = y_new <= 1 and y_new >= -1\n","\n","        in_square = x_in and y_in \n","\n","        # if move is accepted\n","        if in_square:\n","\n","            x = x_new\n","\n","            y = y_new\n","\n","        else:\n","\n","            num_rej_moves += 1\n","\n","        # if in disc, +1 to number of hits\n","        if rad_sq_new <= 1:\n","\n","            N_hits += 1\n","\n","    # we add in this output to \n","    # better understand how to pick \n","    # delta\n","    print( \"fraction of successful moves: \", 1 - ( num_rej_moves / N) )\n","\n","    return 4 * ( N_hits / N )\n"],"metadata":{"id":"yAU9idoiBg3c","executionInfo":{"status":"ok","timestamp":1640286057718,"user_tz":360,"elapsed":145,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["___"],"metadata":{"id":"_CaWJgq7BhCB"}},{"cell_type":"markdown","source":["$\\quad$ Running:"],"metadata":{"id":"fU1t4nC2BO73"}},{"cell_type":"code","source":["#collapse-hide\n","\n","markov_pi_modified()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAJ5NUq7DyC1","executionInfo":{"status":"ok","timestamp":1640286057996,"user_tz":360,"elapsed":281,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"9cf6d976-2ddb-450a-b827-a764d2b480fe"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["fraction of successful moves:  0.94765\n"]},{"output_type":"execute_result","data":{"text/plain":["3.0008"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"-vd8su1q6NKa"},"source":["___\n","\n","__Q__ $\\quad$ In both the Glauber and Metropolis dynamics for the evolution of the Ising model, the spin variable to be updated ( i.e. the vertex $v \\in \\mathbb{T}$ sampled ) is chosen uniformly, through _direct sampling_. What if this is instead done through Markov-chain sampling? Also, instead of rejecting the moves outside the box, we can play with periodic boundary conditions for the walks in both simulations.\n","___"]},{"cell_type":"markdown","source":["## discussion "],"metadata":{"id":"EY3KxkBeBJUL"}},{"cell_type":"markdown","metadata":{"id":"shleUtBs0yUD"},"source":["$\\quad$ The Monte Carlo 'games' described above epitomize the two basic approaches to sampling from a probability measure $\\mu$ on some space $\\mathcal{S}$. Both approaches to the sampling evaluate an observable $\\mathcal{O} : \\mathcal{S} \\to \\mathbb{R}$. In our examples, $\\mathcal{S} = \\textsf{square}$, and the observable $\\mathcal{O}$ is the indicator function of the disc within the square:\n","$$\n","\\mathbf{1}_{\\textsf{disc}}(s) = \\begin{cases}\n","1 & \\quad s \\in \\textsf{disc} \\\\\n","0 & \\quad \\textrm{otherwise}\n","\\end{cases} \\,, \\quad \\mathbf{1}_{\\textsf{disc}} : \\textsf{square} \\to \\mathbb{R} \n","$$"]},{"cell_type":"markdown","metadata":{"id":"RF4Lz6Bj42Qj"},"source":["In either sampling case, we can define $\\mathcal{O}_i := \\mathcal{O}(X_i)$, and note that one evaluates\n","\n","$$\n","\\frac{N_{\\textrm{hits}}}{N} = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{O}_i \\approx \\mathbb{E} \\mathcal{O},\n","$$\n","\n","where the brackets $\\langle \\, \\cdot \\, \\rangle$ denote the expected value taken with respect to the probability measure $m$, in this case (as before) uniform on $[-1,1]^2$. "]},{"cell_type":"markdown","metadata":{"id":"NZvyXir68l5G"},"source":["$\\quad$ In general, the pmf of $m$ doesn't appear in the approximation $\\tfrac{1}{N} \\sum_{i=1}^N \\mathcal{O}_i$, Krauth: _\"...rather than being evaluated, it is sampled. This is what defines the Monte Carlo method.\"_ He goes on to point out that the multiple integrals also disappear: the expectation is formally an integral in two-variables, but we are approximating it by a path-integral indexed by time. _\"This means that the Monte Carlo method allows the evaluation of high-dimensional integrals, such as appear in statistical physics and other domains, if we can only think of how to generate the samples.\"_\n","\n","$\\quad$ Integration in two variables doesn't feel like a good example a 'high-dimensional' integral to compute. On the other hand, we are already know, through the motivating [blog post](http://bit-player.org/2021/three-months-in-monte-carlo), of a kind of Markov chain on a high-dimensional space, namely the Glauber dynamics for the Ising model on $\\mathbb{T}$, the two-dimensional torus $( \\mathbb{Z}\\, /\\, (n\\cdot \\mathbb{Z}) )^2$, with $n = 100$ for concreteness. All in all, it seems that one is approximating an integral in an arbitrary number of dimensions (number of spins in the case of Ising model) through what is essentially a path integral. The path integral is a sum of observables of the system, as the system evolves. "]},{"cell_type":"markdown","metadata":{"id":"bGrkyqK9H0Rx"},"source":["$\\quad$ Krauth: _\"Notwithstanding the randomness in the problem, direct sampling, in computation, plays a role similar to exact solutions in analyitcal work, and the two are closely related. In direct sampling, there is no throwing-range issue, no worrying about initial conditions, and there is a straightforward error analysis -- at least if $m$ and $\\mathcal{O}$ are well behaved. Many successsful Monte Carlo algorithms contain exact sampling as a key ingredient. Markov-chain sampling, on the other hand, forces us to be much more careful with all aspects of our calculation. The critical issue here is the correlation time, during which the position of the walker retains a memory of the starting configuration. This time can become astronomical. In the usual applications, one is often satisfied with a handful of independent samples, obtained through week-long calculations, but it can require much throught and experience to ensure that even this modest goal is achieved.\"_\n"]},{"cell_type":"markdown","metadata":{"id":"sqDxS0bcM9Vk"},"source":["## Markov chains\n","\n","_(Menon 1.1, 1.2)_"]},{"cell_type":"markdown","metadata":{"id":"Y3W1Q8HaNAFx"},"source":["$\\quad$ The perspective Menon starts his notes with: we model the world as a space of random signals. The world is filled with observers, which record part of these signals and then form inferences about the world. The process of inference is modeled with a Bayesian paradigm. We assume an observer has a stochastic model capable of generating signals similar to the true signal. These assumptions reduce inference to tuning the model parameters to obtain the best match between the generated and observed signals. For instance, if we know we are observing a simulation of the Ising model, inference reduces to estimating the temperature."]},{"cell_type":"markdown","metadata":{"id":"6zvg1jY7a0pa"},"source":["$\\quad$ Menon's notes explore stochastic models of progressively increasing complexity, in tandem with numerical methods for optimization. The simplest class of stochastic models considered are Markov chains. Suppose we are given a finite set $\\mathcal{S}$ called the **state space** of the Markov chain. This will sometimes (such as in text-modeling instances) be called the **alphabet**, in the context of language modeling. In the 'linguistic' interpretation of the Ising model dynamics, either Glauber or Metropolis dynamics are run to generate a sequence of spin configurations $s_1, \\dots, s_N \\in \\mathcal{S}$. Each spin configuration (a binary image) is interpreted as a letter in the alphabet of some abstract language. An alphabet of astronomical size can seem useless, linguistically. The notion of a large alphabet, perhaps not astronomically so, seems to occur naturally when viewing language through a _coarse-grained_ perspective. This is how we naturally read: not one letter at a time, but one word. The set of english words can then interpreted as an alphabet, one with around a million symbols. This touches upon the heirarchical structure of language (I'm not talking about Chomsky's heirarchy of languages, only the fact that our alphabet clusters into words, and those into sentences and so on). "]},{"cell_type":"markdown","metadata":{"id":"W0XWV0LibDTk"},"source":["___\n","\n","### **Definition.** ( stochastic process )\n","\n","$\\quad$ A (discrete-time) **stochastic process** is a sequence of random variables.\n","___"]},{"cell_type":"markdown","metadata":{"id":"tH5Ualw8O_2o"},"source":["$\\quad$ We study a stochastic process $(X_t)_{t \\in \\mathbb{N}}$ taking values in the given finite set $\\mathcal{S}$. Thus, the temporal (given by the index $t$ in the stochastic process) and the spatial (given by the state value in $\\mathcal{S}$ of the process) aspects of this process are discrete. "]},{"cell_type":"markdown","metadata":{"id":"8HUczPeBPCcA"},"source":["___\n","\n","### **Definition.** ( Markov chain )\n","\n","$\\quad$ The stochastic process $(X_t)_{t \\in \\mathbb{N}}$ is a **Markov chain** if for all $t,\\,$ $\\mathbb{P} ( X_{t +1} | X_1, \\dots, X_t ) = \\mathbb{P} (X_{t+1} | X_t )\n","$.\n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"zPHUqLTKbT5k"},"source":["$\\quad$ As the above definition indicates, the modeling assumption in Markov process theory is that the future of the process depends on the current state, but not the past. This is a probabilistic analogue of Newtonian determinism, encoded by the _forward equation_ below. Stationary Markov chains are thus characterized by their __transition matrix__ \n","$$\n","Q(s, \\tilde{s}) = \\mathbb{P}( X_2 = \\tilde{s} | X_1 = s )\\,,\n","$$ \n","with entries indexed by pairs of states $s, \\tilde{s} \\in \\mathcal{S}$. Another interpretation of the Markov property is that the future and past are independent, conditional on the present. For simplicity, we usually assume Markov chains in consideration have the following structure: \n"]},{"cell_type":"markdown","metadata":{"id":"IKcB4VtrPFOr"},"source":["\n","___\n","\n","### **Definition.** ( stationary Markov chain )\n","\n","$\\quad$ The Markov chain $(X_t)_{t \\in \\mathbb{N}}$ is **stationary** if for all $t$, $\\mathbb{P}(X_{t+1} | X_t ) = \\mathbb{P} (X_2 | X_1 )$.\n","___"]},{"cell_type":"markdown","metadata":{"id":"prL0S6GkPIqg"},"source":["$\\quad$ The most basic statistic associated to the Markov chain is the pmf of the random variable $X_t$. The mass assigned to state $s \\in \\mathcal{S}$ under this law is denoted\n","$$\n","m_t(s) := \\mathbb{P}(X_t = s) \\,,\n","$$\n","and one obtains the following recurrence relation using the so-called law of total probability, along with the definitions of $m_t$ and $Q$:\n","$$\n","\\begin{align}\n","m_{t+1}(\\tilde{s}) &= \\mathbb{P} ( X_{t+1} = \\tilde{s}) \\\\\n","&= \\sum_{s \\in \\mathcal{S}} \\mathbb{P}( X_{t+1} = \\tilde{s}, \\, X_t = s ) \\\\\n","&= \\sum_{s \\in \\mathcal{S}} \\mathbb{P} (X_{t+1} = \\tilde{s} | X_t = s ) \\mathbb{P}(X_t = s)  \\\\\n","&= \\sum_{s \\in \\mathcal{S}} Q(s,\\tilde{s}) m_t(s)\\,,\n","\\end{align}\n","$$\n","when we assume the chain is stationary. This calculation yields the **forward equation** \n","$$\n","m_{t+1} = m_t Q , \\quad t \\in \\mathbb{N}\n","$$\n","using the convention that the pmf $m_t$ of $X_t$ is regarded as a row vector.  The forward equation is linear and explicitly solvable when the chain is stationary. We proceed inductively to find\n","$$\n","m_t = m_1 Q^t , \\quad t \\in \\mathbb{N} \\,,\n","$$\n","and one of the central questions in Markov chain theory is to understand the behavior of $m_t$ as $t\\to\\infty$. This explains the importance of the eigenvalue spectrum of $Q$ to Markov chain theory: diagonalizing $Q$ is an efficient way to compute $Q^t$ for $t$ large, and the spectrum of $Q$ largely governs the long term behavior of the system. \n"]},{"cell_type":"markdown","metadata":{"id":"dI_lPROxPT02"},"source":["## equilibrium measures, ergodicity, random walks\n","\n","_(Menon 1.2)_"]},{"cell_type":"markdown","metadata":{"id":"dQbSgDbKPb2G"},"source":["___\n","\n","### **Definition.** ( equilibrium / stationary distribution )\n","\n","$\\quad$ Let $Q$ be the transition matrix of a stationary Markov chain $(X_t)_{t \\in \\mathbb{N}}$ with state space $\\mathcal{S}$, and let $m$ be a probability measure on $\\mathcal{S}$, whose pmf we also denote $m$. The measure $m$ is an **equilibrium distribution** or equivalently, a **stationary distribution** for $(X_t)_{t \\in \\mathbb{N}}$ if \n","$$\n","m = m Q,\n","$$\n","and if $m$ satisfies the normalization conditions $m(s) \\geq 0$ for all $s \\in \\mathcal{S}$ and $\\sum_{s \\in \\mathcal{S}} m(s) =1$.\n","\n","___"]},{"cell_type":"markdown","source":["$\\quad$ The above definition is central to our simulation goals. For Markov chains with a unique stationary distribution $m$, the measures $m_t \\equiv \\mathbb{P}(X_t = \\cdot )$ converge to $m$, in an appropriate sense, as $t \\to \\infty$. When we simulate a Markov chain, we are sampling from $m_t$, and the idea is to run the simulation long enough (taking $t$ large enough) so that $m_t \\approx m$. This is how a Markov chain Monte Carlo (MCMC) simulation can be used to approximate sampling from $m$. The MCMC method then requires _finding_ a stationary ergodic Markov chain in $\\mathcal{S}$, whose stationary measure coincides with some $m$ we started with, and wish to sample from. "],"metadata":{"id":"9440Kf4m94Yc"}},{"cell_type":"markdown","metadata":{"id":"-S2kCOntPd1u"},"source":["$\\quad$ Krauth discussed the initial conditions of a Markov chain in the context of simulation: in the second pebble game, the person playing always had to start at a prescribed corner of the square. These are _deterministic_ initial conditions. In practice, we assume simulations will start deterministically, or from where a previous simulation left off. In theory, it is useful to have a mathematical object associated to the __initial conditions__ of a Markov chain: a probability measure on $\\mathcal{S}$, denoted $m_0$. When initial conditions are deterministic, and start at some distinguished state $s_* \\in \\mathcal{S}$, the probability measure $m_0$ corresponds to the $\\delta$-mass $\\delta_{s_*}$. When $m_0$ is general, it models starting from where some simulation left off, whose information we have no (or only partial) access to. \n","\n","$\\quad$ The first equation in the above definition can be understood as describing the evolution of a Markov chain $(X_t)_{t \\in \\mathbb{N}}$ whose initial conditions are the stationary distribution $m$ associated to $(X_t)_{t \\in \\mathbb{N}}$. In this case $m_t = m$ for all $t$ if $m_1 = m$. That is, while the values of $X_t$ change with time (it's a stochastic process) its probability mass function does not. Menon: _\"the equilibrium distribution of a Markov chain captures the intuitive idea of a dynamic equilibrium.\"_"]},{"cell_type":"markdown","metadata":{"id":"hjZr6ElwPjBy"},"source":["$\\quad$ Considering the task of computing $m$ given $Q$: $\\,$ observe that $m$ is a left-eigenvector of $Q$ with eigenvalue $1$. Further, $1$ must always be an eigenvalue of $Q$ since it corresponds to the right-eigenvector $(1, \\dots, 1)^T$. Thus, if we know how to solve for eigenvectors, we can determine $m$. Menon: _\"what is interesting in practice is the converse: for large transition matrices, say when $\\# \\mathcal{S}$ is $10^6 \\times 10^6$, it is more efficient to use Markov chains to approximate $m$, than to use naive linear algebra. This observation plays an important role in web crawlers and search engines.\"_"]},{"cell_type":"markdown","metadata":{"id":"XUm5ol1yPqDj"},"source":["___\n","\n","### **Definition.** ( ergodic Markov chain )\n","\n","$\\quad$ A stationary Markov chain $X$ with state space $\\mathcal{S}$ and transition matrix $Q$ is **ergodic** if it has a unique equilibrium distribution. A sufficient condition for ergodicity of $X$ is if for every pair $s, \\tilde{s} \\in \\mathcal{S}$ there is a positive integer $j(s,\\tilde{s})$ such that $Q^{j(s,\\tilde{s})} >0 $.\n","___"]},{"cell_type":"markdown","metadata":{"id":"hY9uwPisPqxm"},"source":["$\\quad$ Many other conditions are known that ensure ergodicity. For example, if all terms of $Q$ are strictly positive, it is possible for any state to get to any other state, and this ensures that the Markov chain is ergodic. This assumption is too restrictive in practice, since we are mainly intersted in Markov chains making 'local' moves such as random walks in a large state space. This behavior is captured by the sufficient condition given in the above definition. Intuitively, it means that every state can visit every other state _given enough time_. We assume this property always holds."]},{"cell_type":"markdown","metadata":{"id":"rwhVu157Pt4N"},"source":["$\\quad$ Physicsts define the term ergodic ( describing Markov chain $(X_t)_{t\\in \\mathbb{N}}$ ) to mean the following.\n","\n","$$\n","\\lim_{T \\to \\infty} \\sum_{t =1}^T \\mathcal{O}(X_t) = \\mathbb{E}_m\\mathcal{O},\n","$$\n","\n","where $m = m Q$ and $\\mathcal{O}: \\mathcal{S} \\to \\mathbb{R}$ is arbitrary. This formulation has the interpretation that a Markov chain is ergodic when the left-hand side, the _time-average_, is equal to the _spatial-average_ (with respect to the equilibrium measure), which is the right hand side. Menon notes the following bottlenecks and simplifications:\n","\n","* $\\mathcal{S}$ may be very large. For example (in shuffling dynamics we soon describe), $\\mathcal{S}$ may be the permutation group. \n","\n","* $Q$ may be a sparse matrix.\n","\n","* As discussed previously, we often don't need $m$ explicitly, we instead want to evaluate $\\mathbb{E}_m \\mathcal{O}$, where $\\mathcal{O}: \\mathcal{S} \\to \\mathbb{R}$ is some observable.  "]},{"cell_type":"markdown","metadata":{"id":"2yBpE3mWP9fb"},"source":["___\n","\n","### **Example.** ( simple random walk ) \n","\n","$\\quad$ Let $G = (\\textrm{V},\\textrm{E})$ be a finite graph with vertex set $\\textrm{V}$ and edge set $\\textrm{E}$. Given $x,y \\in \\textrm{V}$, write $x \\sim y$ if the edge $\\{ x, y \\}$ lies in $\\textrm{E}$. Let $\\textrm{deg}(x) \\triangleq \\sum_{x \\sim y } 1$ be the degree of vertex $x$. A **simple random walk** on $G$ is a Markov chain with state space $\\textrm{V}$ and transition matrix\n","$$\n","Q(x,y)=\n","\\begin{cases}\n","\\frac{1}{\\textrm{deg}(x)}, & \\quad \\text{ if } x \\sim y \\\\\n","0 & \\quad \\text{ otherwise. }\n","\\end{cases}\n","$$\n","We leave the initial conditions ambiguous in this definition. \n","___"]},{"cell_type":"markdown","metadata":{"id":"mlgWt7oYQCRU"},"source":["___\n","### **Example.** ( random walks in the permutation group )\n","\n","$\\quad$ Let $\\mathfrak{S}_n$ denote the symmetric group on $n$ symbols, the collection of permutations on the set \n","$$\n","[n] := \\{ \\, 1, \\dots, n \\, \\},\n","$$ A permutation on $[n]$ is a bijective function from $[n]$ to itself, and the group structure is given by function composition. Below we describe two _distinct_ Markovian dynamics on $\\mathfrak{S}_n$ with the same stationary measure. These dynamics arise from putting distinct graph structures (writing $\\sim'$ and $\\sim''$ in this example) on a set of vertices indexed by $\\mathfrak{S}_n$, and considering the associated simple random walk. \n","\n","1. We declare permutations $\\sigma$ and $\\tau$ adjacent and write $\\sigma \\sim' \\tau$ if $\\sigma$ and $\\tau$ are related by a transposition of adjacent elements. The $'$-degree of each permutation in this graph is $n-1$.\n","\n","2. We declare permutations $\\sigma$ and $\\tau$ adjacent and write $\\sigma \\sim'' \\tau$ if $\\sigma$ and $\\tau$ are related by a transposition, not necessarily of adjacent elements. The $''$-degree of each permutation in this graph is ${n \\choose 2}$. \n","\n","These are examples of _distinct_ ergodic random walks on the same state space, namely $\\mathfrak{S}_n$, both of which have the uniform measure as their equilibrium measure. The simulation 'purpose' of both random walks is thus to sample from this uniform measure, and the distinctness of the two walks raises the question of whether one method does the sampling better. This is a well-studied question, when $n = 52$ these two walks describe simple procedures for shuffling a deck of cards. \n","___"]},{"cell_type":"markdown","metadata":{"id":"vTnwbHHbQEXz"},"source":["## Shannon's model of text\n","\n","_( Menon 1.3 )_"]},{"cell_type":"markdown","metadata":{"id":"Fm0wo2s9QGhC"},"source":["$\\quad$ The main idea in Shannon's model (of text / written language) is that text is text is a stationary, ergodic stochastic process $(X_t)$ taking values in an alphabet\n","$$\n","\\mathcal{A} \\triangleq \\{\\, a, \\,b,\\, c,\\, \\dots\\,,\\, z,\\, \\_ \\,\\}.\n","$$\n","We use $\\mathcal{A}$ for this specific alphabet for clarity in examples which follow. This model of language is easily augmented to include punctuation and case, but we ignore this in the first approximation. We are not assuming that this process is a Markov chain. As discussed, the alphabet is the state space $\\mathcal{S}$ from previous sections. Stationarity is distinct from the Markov property $-$ neither property implies the other."]},{"cell_type":"markdown","metadata":{"id":"GFD_0t1eQJ2Z"},"source":["\n","___\n","\n","### **Definition.** ( stationary stochastic process )\n","\n","$\\quad$ A discrete time stochastic process $X \\equiv (X_t)$ is **stationary** if all abitrarily large, but finite, marginal probabilities are invariant under arbitrary shifts in time. More precisely,\n","\n","$$\n","\\mathbb{P} (\\, X_1 = s_1, \\dots, X_n = s_n\\,) \n","    = \n","        \\mathbb{P} ( \\,X_{1+k} = s_1, \\dots, X_{n+k} = s_n \\,)\n","$$\n","\n","for all positive integers $k$ and $n$.\n","___"]},{"cell_type":"markdown","metadata":{"id":"OCZI7p6-QOub"},"source":["$\\quad$ Stationarity means that if we were to observe strings of length $n$, their statistics are not changed by shifting them forward in time by an integer $k$. For such processes, the notion of ergodicity again means that _time averages_ are equal to _spatial averages_. From the perspective of observables $\\mathcal{O} : \\mathcal{S}^n \\to \\mathbb{R}$, which through the above definition requires have an arbitrarily large 'memory' $n$ of the process, one has\n","$$\n","\\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t =0 }^{T-1} \\mathcal{O}(X_{t+1}, \\dots, X_{t+n}) =\n","\\mathbb{E}  \\mathcal{O}( X_1, \\dots, X_n) \n","$$"]},{"cell_type":"markdown","metadata":{"id":"VZqXbh6xQPTj"},"source":["$\\quad$ Any discussion above of language and heirarchy was probably motivated by Menon, who among other things says:  _\"the above assumptions are introduced to conform to the idea that written text contains many heirarchical structures: letters are joined by phonetic rules to form words, words are linked by the rules of grammar into sentences, sentences are organized into paragraphs, and so on. Strange as it may seem at first sight, these rules can be effectively modeled as a stochastic process. The use of stationary ergodic processes provides us with a definition flexible enough to include heirarchical structures (though these may be complex to write down), and simple enough to computationally test.\"_"]},{"cell_type":"markdown","source":["$\\quad$ As discussed, written text is modeled by a stationary stochastic process $Y$, whose law is denoted\n","$$\n","\\mathbb{P}_{\\textrm{true}} .\n","$$\n","The subscript '$\\textrm{true}$' indicates that we view $Y$ not as an approximation to language, but as a stream of 'correct' English text, generated in real-time. This idealization is vague, but the main point is to identify the written language $\\mathcal{L}$, in this case English, with a concrete mathematical object: the stationary, stochastic process $Y$. The law $\\mathbb{P}_{\\textrm{true}}$ of this process is the analogue of $m$ in two previous contexts: the uniform measure on $\\mathcal{S} = \\textsf{square}$, and the uniform measure on $\\mathcal{S} = \\mathfrak{S}_{52}$. In either previous case, the purpose of the Markov chains involved was to approximately sample from $m$. Below, we discuss a family of Markov chains for approximately sampling from $m = \\mathbb{P}_{\\textrm{true}}$. The way in which this sampling happens here seems distinct from before, and is remarked on after we introducing the Markov chains approximating $\\mathcal{L}$ below. Menon: _\"In practice, the probabilities relative to $\\mathbb{P}_{\\textrm{true}}$ below are approximated by mining a large corpus (e.g. the works of Shakespeare) to approximate $\\mathbb{P}_{\\textrm{true}}(a_1, \\dots, a_n)$ for an arbitrary string $(a_1, \\dots, a_n) \\in \\mathcal{A}^n$.\"_"],"metadata":{"id":"Dx25Pxq-xFDM"}},{"cell_type":"markdown","metadata":{"id":"BTVeKOFhQbTv"},"source":["___\n","\n","### **Example.** ( digram model )\n","\n","_( or 2-gram model )_\n","\n","$\\quad$ This is a Markov chain with state space $\\mathcal{A} = \\{ \\, a,\\, b, \\, c, \\, \\dots \\, , \\,z, \\, \\_ \\}$. \n","\n","$$\n","Q^{(2)}(x,y) = \\mathbb{P}_{\\textrm{true}} (\\,X_2 = y | X_1 = x\\,),\n","$$\n","\n","which is to say, we form the character-to-character transition probabilities of a Markov process using the marginals of the full stationary process being approximated. \n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"3q3C3mr0R2W5"},"source":["$\\quad$ We can get better approximations by allowing the Markov chain above to retain more of its history by augmenting the state space to $\\mathcal{A}^n$ for some $ n \\geq 2$. When $n=2$, we obtain the so-called trigram model. "]},{"cell_type":"markdown","metadata":{"id":"kw_auvEwQeqi"},"source":["\n","\n","___\n","\n","### **Example.** ( trigram model ) \n","\n","_( or 3-gram model )_\n","\n","$\\quad$ This is a Markov chain with state space $\\mathcal{A}^2$, which we style as\n","\n","$$\n","\\{ aa, \\,ab,\\,, \\dots, az, \\dots, \\_a, \\, \\dots, \\, \\_\\,\\_ \\} \n","$$\n","\n","Let $x = a_1a_2$ and $y = b_1b_2$. In order to form a text of three letters from $x$ and $y$, we must ensure that $x$ and $y$ agree on their overlap, that is when $a_2 = b_1$, so that $x$ and $y$ combine to give us the string $a_1a_2b_2$. With this restriction on $x$ and $y$, we obtain the associated transition matrix\n","\n","$$\n","Q^{(3)} (x,y) \n","= \n","\\frac{\n","\\mathbb{P}_{\\textrm{true}} ( a_1 a_2 b_2 )\n","}\n","{\n","\\mathbb{P}_{\\textrm{true}} (a_1 a_2) \n","}\n","$$\n","\n","and $Q(x,y) =0$ when $a_2 \\neq b_1$. \n","\n","> Because of the compatability requirement of $a_2 = b_1$, the above Markov chain can still be viewed as a stochastic process taking values in $\\mathcal{A}$, it is just no longer a Markov chain from this perspective. This is the sense, however, in which this model is approximating $\\mathbb{P}_{\\textrm{true}}$.  \n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"0AhvYvFiQfVM"},"source":["\n","\n","___\n","\n","### **Example.** ( $(n+1$)-gram model )\n","\n","$\\quad$ A Markov chain with state space $\\mathcal{A}^{n}$. A state $x$, say $x = (a_1, \\dots, a_n)$ can be followed by a string $y = (b_1, \\dots, b_n)$ only if $a_2 = b_1, a_3 = b_2, \\dots, a_n = b_{n-1}$. This induces transition matrix\n","\n","$$\n","Q^{(n+1)} (x,y) \n","= \n","\\frac{\n","\\mathbb{P}_{\\textrm{true}} ( a_1 a_2 \\dots a_n b_n )\n","}\n","{\n","\\mathbb{P}_{\\textrm{true}} (a_1 a_2 \\dots a_n) \n","}\n","$$\n","\n","as above.\n","\n","> Following the previous remark, for similar compatibility reasons, this Markov chain can also be interpreted as a stationary stochastic process taking values in $\\mathcal{A}$. As before, this is the sense in which the above process approximates $\\mathbb{P}_{\\textrm{true}}$.\n","___"]},{"cell_type":"markdown","metadata":{"id":"2MuSJ6_8Qj6z"},"source":["$\\quad$ Menon: _\"Shannon proved that as $n \\to \\infty$, the law of text generated by the above Markov approximations converges to the law of the true language. Note however that a good proof may not correspond to a good algorithm. For example, as $n$ increases, the size of the state space ( of the approximating Markov process ) grows exponentially. Thus the dimensions of the transition matrix are $\\#\\mathcal{A}^{2n}$, and worse yet, the size of the training data ( to determine $Q^{(n+1)}$ by mining ) also expands exponentially. This follows the rules of everyday language. Once $n$ gets large enough, say $5$ or $6$ in practice, it is much more efficient to make our fundamental unit words, rather than letters, since the number of true words of length $5$ is much lower than the $26^5$ combinations that are possible. This reflects the true nature of the space $\\_$ character. In effect, we are still using the digram model, though we have switched to a new 'alphabet' whose fundamental units are words.\"_"]},{"cell_type":"markdown","source":["## information theory"],"metadata":{"id":"XMdaQwHF3BCx"}},{"cell_type":"markdown","source":["_references_\n","\n","* [Ivan Matic](https://mfe.baruch.cuny.edu/maticivan/), _Information Theory and Sanov's Theorem_ ([his notes](https://the-ninth-wave.github.io/matic.pdf) from July 21, 2018)"],"metadata":{"id":"Cs_rrvxo58L6"}},{"cell_type":"markdown","source":["$\\quad$ Before going further, let us return to the object $\\mathbb{P}_{\\textrm{true}}$, playing the role of $m$ in previous examples. When recalling previous examples, we identified $m$ as the uniform measure on $\\mathcal{S}$, in either case. To be formal, $m = \\mathbb{P}_{\\textsf{true}}$ is a probability measure on $\\mathcal{S} \\equiv \\mathcal{A}^\\infty$, where\n","$$\n","\\mathcal{A}^\\infty := \\{ a \\equiv (a_j)_{j=1}^\\infty : a_j \\in \\mathcal{A} \\textrm{ for all } j \\}\\,,\n","$$\n","the space of infinite sequences of elements of $\\mathcal{A}$. Any discrete-time $\\mathcal{A}$-valued stochastic process, given some initial conditions which we ignore, induces a probability measure on $\\mathcal{A}^\\infty$. This latter space has the interpretation of a 'path-space', as it is meant to contain all possible trajectories of the process. "],"metadata":{"id":"p5rjLgel2mUO"}},{"cell_type":"markdown","source":["$\\quad$ The digram model naturally gives rise to a Markov chain with state space $\\mathcal{A}$. Let us say that it induces measure $\\mathbb{P}_{(2)}$ on $\\mathcal{A}^\\infty$. The trigram model is defined as a Markov chain with state space $\\mathcal{A}^2$, but following the above remark, it can also be interpreted as a stationary stochastic process taking values in $\\mathcal{A}$, inducing a measure $\\mathbb{P}_{(3)}$ on the path-space $\\mathcal{A}^\\infty$. In general, the $(n+1)$-gram model induces measure $\\mathbb{P}_{(n+1)}$ on $\\mathcal{A}^\\infty$. Our interpretation of what Menon says at the beginning of the most recent quote, is _\"Shannon proved that...\"_ $\\mathbb{P}_{(n+1)}$ converges to $\\mathbb{P}_{\\textrm{true}}$, in an appropriate sense. \n","\n","> The approximation of $\\mathbb{P}_{\\textrm{true}}$ by $\\mathbb{P}_{(n+1)}$ does _not at all_ seeem to be an approximation of the same nature was is described in Krauth. By this I mean that $\\mathcal{A}$-valued Markov chains do not play the same role in each case. In earlier cases, they served as a way to approximately sample from a probability measure on $\\mathcal{A}$. In the present context, these objects induce a probability measure on the _path-space_ $\\mathcal{A}^\\infty$, towards approximating $\\mathbb{P}_{\\textrm{true}}$, the probability measure on $\\mathcal{A}^\\infty$ encoding the '$\\textrm{true}$' language.  "],"metadata":{"id":"elemrLyvIpAm"}},{"cell_type":"markdown","source":["$\\quad$ Matic's add to the above discussion, as well as to Menon's upcoming discussion of entropy. Menon: _\"the entropy of a random variable describes its uncertainty. In particular, the notion of an optimal code tells us that entropy describes the optimal search procedure to determine the value of $X$ through a series of yes / no questions.\"_ Matic's notes touch on this optimality, from the perspective of data compression. "],"metadata":{"id":"R8c-1spJ56Jf"}},{"cell_type":"markdown","source":["$\\quad$ To integrate Matic's perspective, he starts by imagining _\"...we want to store some big quantity of data, such as a picture, a text document, or a book. We start with a collection of uncompressed data, and we identify the smallest building blocks of each datum as symbols. The set of all symbols is called the alphabet.\"_ Matic also denotes this alphabet as $\\mathcal{A}$. \n","\n","> It feels worth remarking that languages like Chinese are pictoral, and that Chinese characters can be decomposed into 'brushstrokes'. A space character `_` perhaps corresponds to moving to the next character. This lends itself to the notion that pictoral data can be decomposed into symbols. For images in general, I think 'contours' become the analogue of brushstrokes. Perhaps an efficient way to use a space character `_` is as denoting the start of a new transparent canvas layer, sitting over the previously drawn layers (copying the functionality of applications like photoshop). My impression of $3$-dim spatial data (e.g. virtual environments for games) is that environment information is efficiently stored in surfaces, the higher-dimensional analogue (by one dimension) of contours. "],"metadata":{"id":"0F_C2IEb-tjd"}},{"cell_type":"markdown","source":["> The above decomposition reminds me of something I heard about the clustering algorithm [t-SNE](https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a): when applid to MNIST, the number of clusters the algorithm found was more than the number of categories (labeled by digits $0 - 9$). Some digits corresponded to multiple clusters _because_ of there being distinct ways to write this digit, with or without a horizontal slash in the case of $7$, for example. In a sense, the clustering works too well. "],"metadata":{"id":"BGTZsfyuM0SO"}},{"cell_type":"markdown","source":["$\\quad$ Uncompressed data is called a __signal__. Mathematically, we model signals as elements of $\\mathcal{A}^M$. Ideally, the signal is sampled from the measure $\\mathbb{P}_{\\mathcal{L}}$ on $\\mathcal{A}^\\infty$ corresponding to abstract language $\\mathcal{L}$ over alphabet $\\mathcal{A}$. This measure corresponds to a stochastic process $(X_t)_{t \\in \\mathbb{N}}$, which we suppose is stationary. Because of the 'optimal encoding' perspective, it is important to be able to quantify the relationship between the lengths of a signal and its encoding. In particular, we assume some upper bound $M$ on the signal length to begin with, modeling signals as elements of $\\mathcal{A}^M$ for some $M \\in \\mathbb{N}$. When referring to $\\mathbb{P}_{\\mathcal{L}}$ in this context, it is understood as the projection of this measure onto $\\mathcal{A}^M$, corresponding to the 'truncated' process $(X_t)_{t = 1}^{M}$, which is a random signal."],"metadata":{"id":"S2IUhIoF_siL"}},{"cell_type":"markdown","source":["$\\quad$ Our analysis takes place within an even simpler setting versus the language models discussed. Suppose $\\mathbb{P}_{\\mathcal{L}}$ corresponds to an i.i.d. sequence of random variables $(X_t)_{t = 1}^M$, where each $X_t$ has the same distribution as some 'model' random variable $X$, corresponding to probability measure $m$ on $\\mathcal{A}$. Because $\\mathcal{A}$ is finite, so is the space of signals $\\mathcal{A}^M$. A lossless data compression is an injective function on this signal space. "],"metadata":{"id":"jVaWDQjuJIog"}},{"cell_type":"markdown","source":["___\n","### __Definition.__ ( lossless data compression )\n","\n","$\\quad$ A __lossless data compression__ is an injective function\n","$$\n","f : \\mathcal{A}^M \\to \\bigcup_{j=1}^N \\{ 0 , 1 \\} ^j\n","$$\n","for some $N \\in \\mathbb{N}$.  \n","___"],"metadata":{"id":"fLWVherAKqks"}},{"cell_type":"markdown","source":["$\\quad$ Given some lossless data compression $f$, and for each signal $s \\equiv (s_1, \\dots, s_M) \\in \\mathcal{A}^M$, we let $\\textsf{length}_f(s)$ denote the length of the signal encoding $f(s)$. Letting $\\mathbb{E}$ denote expectation with respect to the 'model' random variable $X$, desirable propereties of the compression $f$ make\n","$$\n","\\mathbb{E} \\, \\textsf{length}_f(X) \n","$$\n","as small as possible. Heuristically, this means __(1)__ identifying the signals in $\\mathcal{A}^M$ which are _typical_ and _atypical_, according to the sampling $(X_t)_{t=1}^M$, and then __(2)__ using shorter binary sequences to encode typical signals, leaving longer binary sequences for atypical signals. This means the expectation is placing as little probablistic mass as possible on the longest encodings. "],"metadata":{"id":"ZebmNnuHLxQ7"}},{"cell_type":"markdown","source":["$\\quad$ Let us use $m$ to denote the probability measure on $\\mathcal{A}$ induced by the random variable $X$, our template for the i.i.d. sequence. Enumerating $\\mathcal{A}$ as $\\{ a_1, \\dots, a_n\\}$, let us write $m_i$ for the number $m( \\{ a_i \\})$. We identify $m$ with the vector $(m_1, \\dots, m_n)$, which is the probability mass function of $X$. It is $m$ that determines how 'typical' a given sequence is."],"metadata":{"id":"t6KJM4XH6Hkk"}},{"cell_type":"markdown","source":["$\\quad$ Let $s \\equiv (s_1, \\dots, s_M) \\in \\mathcal{A}^M$. Write $\\underline{X}$ to denote the random vector $(X_t)_{t=1}^M$. The mutual independence of the $X_t$ implies\n","$$\n","\\begin{align}\n","\\mathbb{P}( \\, \\underline{X} = s ) &= \\mathbb{P} (X_1 = s_1) \\dots \\mathbb{P}(X_M = s_M) \\\\\n","&= 2^{ \\sum_{t=1}^M \\log_2\\mathbb{P}(X_t = s_t)}\\\\\n","&= 2^{ \\sum_{i=1}^n n_i \\log_2 m_i} \\,,\n","\\end{align}\n","$$\n","where, in going from the second to third line, we have reindexed the sum over the alphabet, using that $m_i \\equiv \\mathbb{P}(X = a_i)$, and defining\n","$$\n","n_i := \\# \\{ \\, t \\in \\{ 1, \\dots, M \\} : X_t = a_i \\, \\}.\n","$$\n","Thus, $n_i$ is the number of times symbol $a_i$ appears in the random signal $\\underline{X}$. A typical message is one  in which the frequency of symbol $a_i$ is close to the corresponding probability $m_i$. Thus, for typical messages, we have $n_i \\approx m_i M $, and the probability of such a message is approximately\n","$$\n","2^{ M \\sum_{i=1}^n m_i \\log m_i }\n","$$"],"metadata":{"id":"qRYQhBpA7yj4"}},{"cell_type":"markdown","source":["___\n","\n","### __Definition.__ ( entropy )\n","\n","$\\quad$ Let $m$ be a probability measure on finite state space $\\mathcal{A}$ with $\\# \\mathcal{A} = n$, corresponding to probability mass function $(m_1, \\dots, m_n)$. The __entropy__ of $m$ is \n","$$\n","\\textsf{S}(m) := - \\sum_{i=1}^n m_i \\log_2 m_i\n","$$\n","\n","___"],"metadata":{"id":"jZBqWwRdBBaH"}},{"cell_type":"markdown","source":["$\\quad$ Note that $\\textsf{S}(m) \\geq 0$, and is $=0$ if and only if $m$ corresponds to a deterministic object, namely a $\\delta$-mass at some symbol in the alphabet. In the above setting, we are imagining the length $M$ of the signal is much larger than the size $n$ of the alphabet $\\mathcal{A}$. This is so that the law of large numbers has a chance to kick in for each symbol. In this setting, typical messages appear with probability on the order of $2^{-M \\textsf{S}(m)}$. When $m$ is non-trivial, the positivity of $\\textsf{S}(m)$ implies that typical signals are _exponentially rare_, in the signal length $M$. "],"metadata":{"id":"rob5a2xzCisV"}},{"cell_type":"markdown","source":["$\\quad$ A consquence of the intrinsic high-dimensionality of $\\underline{X}$ -- which corresponds to a probability measure on $\\mathbb{R}^M$, with $M$ large -- is that signals with symbol frequencies _close_ to the corresponding $m_i$ are very common. The next definition formalizes this notion of closeness. "],"metadata":{"id":"pLqgl8COGLNx"}},{"cell_type":"markdown","source":["___\n","\n","### __Definition.__ ( $\\epsilon$-typical signal )\n","\n","$\\quad$ For fixed $\\epsilon > 0$, we say $s \\in \\mathcal{A}^M$ is __$\\epsilon$-typical__ if\n","$$\n","\\mathbb{P}( \\, \\underline{X} = s ) \\in \\left( 2^{-M(\\textsf{S}(m) +\\epsilon) }, \\, 2^{-M(\\textsf{S}(m) -\\epsilon) } \\right) \\,,\n","$$\n","and the set of all $\\epsilon$-typical signals is denoted $\\mathcal{A}^M_\\epsilon$. \n","\n","___"],"metadata":{"id":"ZABh6jFoHmgQ"}},{"cell_type":"markdown","source":["$\\quad$ As discussed, the goal is to efficiently compress signals $\\underline{X}$ drawn from the measure $m$. The relevance of the above defininition to compression is:\n","\n","1. we first prove that $\\underline{X}$ is $\\epsilon$-typical with high probability\n","\n","2. we will see that the set of typical signals is nonetheless small: the number of all possible signals is $\\# \\mathcal{A}^M \\equiv n^M$, and there are only approximately $2^{M\\textsf{S}(m)}$ \n","\n","these facts combine to form a compression strategy: when we see a typical message, we begin its binary encoding (towards defining the lossless compression function $f$) with `0`, and we use $M \\textsf{S}(m)$ bits to compress it. If the message is atypical, we begin its binary encoding with a `1`, and are more sloppy with how many bits we use to store. The formal statement of the first point is as follows."],"metadata":{"id":"x__jPn1uJOK3"}},{"cell_type":"markdown","source":["___\n","### __Theorem.__ (Matic, Theorem 1) \n","\n","$\\quad$ For each $\\delta > 0$, there is $M_0 \\in \\mathbb{N}$ such that for every $M \\geq M_0$ there is a lossless data compression $f$ in which the average length of a message satisfies\n","\n","$$\n","\\mathbb{E} \\textsf{length}_f ( \\,\\underline{X} ) \\leq M \\textsf{S}(m) + \\delta \n","$$\n","___\n","\n","_( we defer recording the proof )_"],"metadata":{"id":"4HwISpc7MofF"}},{"cell_type":"markdown","source":["$\\quad$ Matic: _\"The algorithm constructed (in the above proof) creates a lossless compression. It is very effective when the raw data consists of independent components. The `png` format uses a better algorithm.\"_ To discuss this algorithm, Matic introduces what he calls _types_. These objects were previously introduced as the frequencies $n_i$ above."],"metadata":{"id":"r8sljptVON5A"}},{"cell_type":"markdown","source":["___\n","\n","### __Definition.__ ( the type of a signal )\n","\n","$\\quad$ Given a signal $s \\equiv (s_t)_{t=1}^M \\in \\mathcal{A}^M$, the __type__ of $s$ is the following probability measure on $\\mathcal{A}$, constructed through the symbol frequencies of $s$: for $a \\in \\mathcal{A}$, \n","$$\n","m_s( \\{ a \\} ) := \\frac{1}{M} \\sum_{t = 1}^M \\mathbf{1}\\{ s_t = a \\}\n","$$\n","\n","___"],"metadata":{"id":"xL3bIejUQmCs"}},{"cell_type":"markdown","source":["$\\quad$ As a direct consequence of the above definition, two signals $s, \\tilde{s} \\in \\mathcal{A}^M$ have the same type if and only if the coordinates of $\\tilde{s}$ form a permutation of the coordinates of $s$. Let $\\textsf{types}(\\mathcal{A},M)$ denote the collection of all types induced by signals in $\\mathcal{A}^M$. The map $\\mathcal{A}^M \\to \\textsf{types}(\\mathcal{A},M)$ is effectively a quotient map, and given a probability measure $m \\in \\textsf{types}( \\mathcal{A}, M)$, we let $\\textsf{signals}(m)$ denote the collection of signals whose type is $m$:\n","$$\n","\\textsf{signals}(m) = \\{ s \\in \\mathcal{A}^M : m_s \\equiv m \\}\n","$$"],"metadata":{"id":"OnAhHXdSBKut"}},{"cell_type":"markdown","source":["___\n","\n","### __Theorem.__ ( Matic, Theorem 2 )\n","\n","Consider the set of types, namely $\\textsf{types}( \\mathcal{A}, M)$, with $\\# \\mathcal{A} = n$. One has\n","\n","1. $\\#\\, \\textsf{types}( \\mathcal{A},M) = {M + n -1 \\choose M }$\n","\n","2. $\\# \\, \\textsf{types}( \\mathcal{A}, M) \\leq (M+1)^n$\n","\n","___"],"metadata":{"id":"hcdz4BXhIFrt"}},{"cell_type":"markdown","source":["$\\quad$ Let $\\mu$ be a general probability measure on $\\mathcal{A}$, corresponding to a random variable $Y$ and let $\\underline{Y} \\equiv (Y_t)_{t=1}^M$ be a sequence of i.i.d. random variables. Given fixed signal $s \\in \\mathcal{A}^M$, one has\n","$$\n","\\begin{align}\n","\\mathbb{P}( \\, \\underline{Y} = s ) &= \\prod_{t=1}^M \\mu( Y_t = s_t) \\\\\n","&= 2^{ \\sum_{t=1}^M \\log_2 \\mu( Y_t = s_t ) } \\\\\n","&= 2^{ \\sum_{a \\in \\mathcal{A} } \\sum_{t =1}^M \\mathbf{1} \\{ X_t = a \\} \\log_2 \\mu(Y_t = s_t)} \\\\\n","&= 2^{ \\sum_{a \\in \\mathcal{A} } \\sum_{t =1}^M \\mathbf{1} \\{ X_t = a \\} \\log_2 \\mu(Y_t = a)} \\\\ \n","&= 2^{ M \\left[ \\sum_{a \\in \\mathcal{A} } \\left( \\frac{1}{M} \\sum_{t =1}^M \\mathbf{1} \\{ X_t = a \\} \\right) \\log_2 \\mu(Y_t = a) \\right]} \\\\\n","&= 2^{ M \\left[m_s(a) \\log_2 \\mu(a) \\right] }\n","\\end{align}\n","$$\n","\n","The coefficient of $M$ in the exponent directly above can be written in terms of the relative entropy."],"metadata":{"id":"0oqQOS1AKJLT"}},{"cell_type":"markdown","source":["_next in section_\n","\n","* relative entropy or KL divergence"],"metadata":{"id":"cu-uL4NMVEEX"}},{"cell_type":"markdown","source":["## Gibbs measures\n","\n","_( Menon 1.4 )_"],"metadata":{"id":"TZWMvH5zYpub"}},{"cell_type":"markdown","source":["$\\quad$ Let us abstract away from the above settings, briefly. In particular, we forget everything about the state space $\\mathcal{S}$, except that it is a finite set, let us say $\\# \\mathcal{S} = N$, and we enumerate the states of $\\mathcal{S}$ as \n","$$\n","s_1, \\dots, s_N .\n","$$\n","\n","> In the context of the Ising model simulation, $N = 2^{\\# \\mathbb{T}} \\equiv 2^{100^2}$."],"metadata":{"id":"vsBhKN6CYtS9"}},{"cell_type":"markdown","source":["$\\quad$ We suppose there is a distinguished function, $h : \\mathcal{S} \\to \\mathbb{R}$, called the __Hamiltonian__. The Hamiltonian assigns to each state $s_i$ a corresponding __energy__ $h(s_i)$. A Gibbs measure is a probability measure on $\\mathcal{S}$, defined from $h$, designed to favor the sampling of low-energy states. In practice, one cares about how the collective behavior of the system changes as a function of the amount of thermal noise. It is traditional to parametrize the amount of thermal noise by an __inverse temperature__ parameter $\\beta \\in [0, \\infty ]$, which is the multiplicative inverse of the system's temperature. Thus $\\beta = 0$ corresponds to an infinite temperature limit, and taking $\\beta \\to \\infty$ corresponds to approaching absolute zero.\n"],"metadata":{"id":"pD_n6ISYfn_p"}},{"cell_type":"markdown","source":["$\\quad$ To equip $\\mathcal{S}$ with a $\\beta$-dependent probability measure $m_\\beta$, it suffices to specify a collection of functions of $\\beta$\n","$$\n","p_1(\\beta), \\dots, p_N(\\beta)\n","$$\n","corresponding to the probability mass function of $m_\\beta$. These depend implicitly on $h$ and are defined as follows."],"metadata":{"id":"aPNTabI6heRF"}},{"cell_type":"markdown","source":["___\n","\n","### __Definition.__ ( Gibbs measure )\n","\n","$\\quad$ The __Gibbs measure__ on $\\mathcal{S}$ associated to Hamiltonian $h$ and inverse temperature $\\beta > 0$, is defined by\n","\n","$$\n","p_i( \\beta) \\propto \\exp\\left( - \\beta h( s_i) \\right)\n","$$\n","\n","___"],"metadata":{"id":"FqMDzwBOjSsP"}},{"cell_type":"markdown","source":["$\\quad$ Above, $\\propto$ is denotes 'proportional to'. The implicit proportionality constant is non-ambiguous because of the constraint that these numbers sum to one. Being more explicit leads to an object of central importance called the partition function. The __partition function__ is:\n","$$\n","Z(h,\\beta) : = \\sum_{ i = 1 }^N \\exp ( - \\beta h(s_i) )\n","$$\n"],"metadata":{"id":"67RcLt3Wj0pw"}},{"cell_type":"markdown","source":["$\\quad$ We can thus write the $p_i(\\beta)$ in the above definition explicitly as\n","$$\n","p_i(\\beta) = \\frac{ \\exp( - \\beta h(s_i) ) }{ Z(h,\\beta)} \n","$$\n","It is useful to consider the endpoint cases $\\beta = 0, \\infty$. Without much loss of generality, we may assume the $s_i$ are ordered so that \n","\n","$$\n","h(s_1) < h(s_2) < \\dots < h(s_M) \\,,\n","$$\n","and one can check that as $\\beta \\to 0$, namely in the \"infinite-temperature\" limit, the measure $m_\\beta$ becomes uniform over $\\mathcal{S}$. The intuition is that thermal noise completely washes away any bias coming from the Hamiltonian.\n"],"metadata":{"id":"9iW7Z87clJTO"}},{"cell_type":"markdown","source":["$\\quad$ On the other hand, in the zero-temperature limit $\\beta \\to \\infty$, The measures $m_\\beta$ tend towards concentrating all of their mass on the first state, which has lowest energy. In the limit, we do not even get a random object, provided that the above strict inequalities hold. A state with lowest energy is called a __ground state__. In the above setting, there is a unique ground state. In general, it's possible for the set of ground states to be larger than one, this can arise naturally from symmetries in the model. For instance, the Ising model always has two ground states, corresponding to all $(+1)$ spins and all $(-1)$ spins."],"metadata":{"id":"LTETVc_fnD6w"}},{"cell_type":"markdown","source":["___\n","\n","_next sections_\n","\n","* derivation of the Gibbs distribution (continuing Menon $\\S$ 1.4)\n","\n","* decoding scrambled text (Menon $\\S$ 1.5)\n","\n","* sampling from a Gibbs distribution (Menon $\\S$ 1.6)\n","\n","* the Metropolis algorithm (Menon $\\S$ 1.7, Krauth $\\S$ 1.1.4 and $\\S$ 1.1.5)\n","\n","* Markov chains convergence to stationary distribution (Menon $\\S$ 1.8, 1.9)"],"metadata":{"id":"0YQdZEkDVqPp"}},{"cell_type":"markdown","source":["> The point of building up to this last section is that it lays the theoretical groundwork for how the MCMC method can be used to sample from a Gibbs distribution, abstracting the simulations run in the motivating [blog post](http://bit-player.org/2021/three-months-in-monte-carlo). What's next is to describe the Hamiltonian for the Ising model, from which the Gibbs measures and dynamics are automatically defined. "],"metadata":{"id":"i-fduhtxcc1i"}},{"cell_type":"markdown","source":["# simulation"],"metadata":{"id":"slc5kSBtjE6G"}},{"cell_type":"markdown","source":["## Ising model"],"metadata":{"id":"UgFDoDdIfUgo"}},{"cell_type":"markdown","source":["$\\quad$ We are simulating the Ising model on the discrete, two-dimensional torus, of side-length 100:\n","\n","$$\n","\\mathbb{T} := \\left( \\, \\mathbb{Z} \\, / \\, (100 \\cdot  \\mathbb{Z}) \\, \\right)^2\n","$$\n","\n","The torus is displayed as a large square grid, with the understanding of \"periodic\" boundary conditions. We use $\\mathbb{T}$ interchangeably to denote the graph and its vertex set. We write $\\textrm{E}(\\mathbb{T})$ for the edge set of the graph. We write $v \\sim w$ if vertices $v$ and $w$ are joined by an edge in $\\textrm{E}(\\mathbb{T})$. "],"metadata":{"id":"h9vskbfCW2lq"}},{"cell_type":"markdown","source":["$\\quad$ In the Ising model, where the __spin__ at a given vertex $v \\in \\mathbb{T}$ is a random variable, $s(v)$, taking values in $\\{ \\pm 1 \\}$. A __state__ of the Ising model is a possible realization of every $s(v)$, for $v \\in \\mathbb{T}$. The __state space__ of the Ising model is \n","\n","$$\n","\\begin{align}\n","\\mathcal{S}_{\\textrm{Ising}} &= \\{ \\pm 1 \\}^{ \\mathbb{T} }\\\\ \n","&\\equiv ( s(v) \\in \\{ \\pm 1\\} : v \\in \\mathbb{T} ).\n","\\end{align}\n","$$\n","\n","We do not define the random variables $s(v)$ individually -- instead, we equip the whole state space $\\mathcal{S}$ with a family of Gibbs probability measures $( m_\\beta : \\beta \\in [0, \\infty] )$. These are completely determined by the Ising Hamiltonian."],"metadata":{"id":"zdyG8wkpXH0H"}},{"cell_type":"markdown","source":["___\n","\n","### __Definition.__ ( Ising Hamiltonian )\n","\n","The __Ising Hamiltonian__ (in its simplest form) is defined as \n","\n","$$\n","H_{\\textrm{Ising}}(s) = - \\sum_{ v \\sim w } s(v) s(w) \n","$$\n","\n","for $s \\equiv ( s(v) : v \\in \\mathbb{T} ) \\in \\mathcal{S}_{\\textrm{Ising}}$. \n","\n","___"],"metadata":{"id":"q4KvIbzxX0GX"}},{"cell_type":"markdown","source":["$\\quad$ The coefficient of $-1$ in each summand is sometimes written $-J$, for some _coupling constant_ $J > 0$. Here we are setting $J = 1$. The sum is indexed by the edges $e = \\{v,w\\} \\in \\textrm{E}(\\mathbb{T})$ of $\\mathbb{T}$. The negative sign is what makes this a model of _ferromagnetism_, in which neighboring spins are encouraged to align. To see why, note that for the associated Gibbs measure $m_{\\,\\textrm{Ising},\\beta}$, and for $s \\in \\mathcal{S}_{\\textrm{Ising}}$,\n","\n","$$\n","m_{\\,\\textrm{Ising},\\beta}(s) \\propto \\exp \\left( \\beta \\sum_{v \\sim w } s(v) s(w) \\right),\n","$$\n","\n","where the negative sign in the Hamiltonian has cancelled with the negative sign in the definition of Gibbs measure. The double negative allows us to interpret the Hamiltonian as energy: it is physical for low-energy states to be preferred, a kind of nod to the principle of least action, and this is implemented by the exponential biasing the measure towards states whose energy is smallest (perhaps negative).  "],"metadata":{"id":"ZvIoqWvOp-aO"}},{"cell_type":"markdown","source":["$\\quad$ Regardless of $\\beta$, the exponent in the above display is maximized at two configurations: the first in which $s(v) \\equiv 1$ for each $v \\in \\mathbb{T}$, and the second in which $s(v) \\equiv -1$ for each $v \\in \\mathbb{T}$. We call these states $s_+$ and $s_-$ respectively. That there are two ground states is due to the invariance of the Hamiltonian under 'spin-flip symmetry', i.e. the transformation taking each spin of a configuration to its negative."],"metadata":{"id":"WizgUNa4qT51"}},{"cell_type":"markdown","source":["$\\quad$ Moreover, the system is always biased towards these two ground states when $\\beta > 0$, it's just that the degree of this bias is mediated by $\\beta$. One can interpret this in a 'signal-to-noise' framework: the ground states are like signals, and $\\beta$ controls the signal-to-noise ratio. The measures $m_\\beta$ exhibit qualitatively different behavior for different ranges of $\\beta$, effectively the concept of a phase transition. The Ising model is one of the simplest models that can exhibit this behavior. In particular, there is a __critical inverse temperature__, $\\beta_c$, such that the Ising model exhibits distinct behavior for $\\beta > \\beta_c$ and $\\beta < \\beta_c$. This value implicitly depends on the fact that we are working with the Ising Hamiltonian on the specific graph $\\mathbb{T}$. The case $\\beta = \\beta_c$ is special, and for now we only concern ourselves with the two regimes away from the critical point. _(If it's the case that simulation becomes more difficult near the critical point, how close to $\\beta_c$ does one start to become seriously impeded by these difficulties?)_\n","\n","| ..... $\\beta$ range ..... | temperature range | name of regime |\n","| -------- | -------- | --- |\n","| $[0, \\beta_c)$ | high-temperature | subcritical |\n","| $(\\beta_c, \\infty]$ | low-temperature | supercritical |"],"metadata":{"id":"80KphcVerp9P"}},{"cell_type":"markdown","source":["## dynamics"],"metadata":{"id":"3gobfA_Bw69N"}},{"cell_type":"markdown","source":["$\\quad$ We revisit the description of the dynamics. We always need some initial conditions in the form of spin configuration in $\\mathcal{S}_{\\textrm{Ising}}$. A spin configuration $s \\in \\mathcal{S}_{\\textrm{Ising}}$ is a function $\\mathbb{T} \\to \\{ \\pm 1\\}$, and we choose to store this data in the computer as an $n \\times n$ matrix. We also call this matrix $s$. The periodicity will be encoded in the Hamiltonian, as well as in the dynamics (the neighboring vertices of each site are defined according to this periodicity). "],"metadata":{"id":"eXQsYWafxAl4"}},{"cell_type":"markdown","source":["$\\quad$ Let us first suppose that updates to the configuration happen at discrete, integer times \n","$$\n","1, 2, \\dots, \\mathfrak{t},\n","$$\n","at each such time a (possibly) new spin configuration is created in a way we describe later. This leads to the sequence of spin configurations \n","$$\n","s_0, s_1, \\dots, s_{\\mathfrak{t}} \\in \\mathcal{S}_{\\textrm{Ising}}.\n","$$\n","We avoid using $T$, because it already has the meaning of temperature, $T \\equiv \\beta^{-1}$."],"metadata":{"id":"78rZqfStyZU1"}},{"cell_type":"markdown","source":["$\\quad$ In the case that it is later easier to think of updates as happening in continuous time (according to some collection of $\\textrm{Exponential}$ clocks, for instance), we will write\n","\n","$$\n","t_1 < t_2 < \\dots < \\mathfrak{t}\n","$$\n","\n","for the sequence of update times. The final time $\\mathfrak{t}$ may no longer be integer, but we can still use the same notation for the resulting sequence of spin configurations, in either case. "],"metadata":{"id":"1-3teWsYzCUz"}},{"cell_type":"markdown","source":["$\\quad$ From an earlier [blog post](http://bit-player.org/2019/glaubers-dynamics) of Brian Hayes: _\"There’s no evidence Glauber was thinking of his method as an algorithm suitable for computer implementation. The subject of simulation doesn’t come up in his 1963 paper, where his primary aim is to find analytic expressions for the distribution of up and down spins as a function of time. (He did this only for the one-dimensional model.) Nevertheless, Glauber dynamics offers an elegant approach to programming an interactive version of the Ising model. \"_ What Hayes calls 'Glauber' dynamics in the first post is actually 'Metropolis-Hastings' dynamics $-$ this was pointed out in the comments. Hayes wanted to understand the distinction between the two better, and the more recent blog post linked to at the top is the really nice result. "],"metadata":{"id":"SVw3AF8dzDoE"}},{"cell_type":"markdown","source":["___\n","_important simulation ingredients and notation_"],"metadata":{"id":"2KpAsuDA00md"}},{"cell_type":"markdown","source":["* $\\quad$ Write $\\# \\mathbb{T}$ for the number of sites ('pixels') in the torus $\\mathbb{T}$. \n","\n","* $\\quad$ For $v \\in \\mathbb{T}$, let $\\textsf{neigh}(v)$ denote the neighbors of $v$ within $\\mathbb{T}$. On a torus, every vertex has four neighbors. In contrast to the blog (which uses $\\sigma$ to denote spin configurations, where we use $s$), we use $\\sigma$ to denote the random field obtained from $s$ by, at each $v \\in \\mathbb{T}$, assigning the local aggregation\n","$$\n","\\sigma(v) := \\sum_{ w \\, \\in \\, \\textsf{neigh}(v) } s(w)\\,,\n","$$\n","so that each $\\sigma(v) \\in \\{ -4, -2, 0 , 2, 4\\}$. "],"metadata":{"id":"vfbXgfk72C_t"}},{"cell_type":"markdown","source":["\n","* $\\quad$ The dynamics rely on comparing the current energy of the lattice with the energy resulting from flipping the spin at $v$. We wish to write down the energy difference concisely. To this end, let $s^{[v]}$ denote the spin configuration obtained from $s$ via\n","$$\n","s^{[v]} = \\begin{cases}\n","s(w) & \\quad \\text{ if } w \\neq v\\\\\n","-s(w) & \\quad \\text{ if} w = v\n","\\end{cases}\n","$$\n","\n","* $\\quad$ Let $\\delta_v H_{\\textrm{Ising}}$ denote the change in energy in going from $H_{\\textrm{Ising}}(s)$ to $H_{\\textrm{Ising}}(s^{[v]})$:\n","$$\n","\\delta_v H_{\\textrm{Ising}}(s) = H_{\\textrm{Ising}}(s^{[v]}) - H_{\\textrm{Ising}}(s)  \n","$$\n"],"metadata":{"id":"g9yW1CXZ2qXL"}},{"cell_type":"markdown","source":["___\n","_on computing $\\delta_vH(s)$_"],"metadata":{"id":"en7l_G5R27nd"}},{"cell_type":"markdown","source":["$\\quad$ Let us see why we have the following equality, which is used in the pseudocode below. \n","$$\n","\\delta_vH_{\\textrm{Ising}}(s) = 2 s(v) \\sigma(v) \n","$$\n","In taking the above difference, we can expand each of $H_{\\textrm{Ising}}(s)$ and $H_{\\textrm{Ising}}(s^{[v]})$. The sum in the Ising Hamiltonian is always indexed by the edges of the torus, $e \\in \\textrm{E}( \\mathbb{T})$. We can partition this edge set according to $v$. Let $\\textrm{E}_1(v)$ denote all edges having $v$ as an endpoint, and let $\\textrm{E}_2(v)$ denote the complementary set in $\\textrm{E}(\\mathbb{T})$. Then,\n","$$\n","\\begin{align}\n","H_{\\textrm{Ising}}(s) &=  - \\sum_{ \\, \\{ u, w \\} \\,  \\equiv \\, e \\, \\in \\, \\textrm{E}(\\mathbb{T}) } s(u)s(w) \\\\\n","&= - \\left( \\sum_{ \\, \\{ u, w \\} \\,  \\equiv \\, e \\, \\in \\, \\textrm{E}_1(v) } s(u) s(w) \\right) - \\left( \\sum_{ \\, \\{ u, w \\} \\,  \\equiv \\, e \\, \\in \\, \\textrm{E}_2(v) } s(u) s(w) \\right) \\\\\n","&= - s(v) \\sigma(v) - \\left( \\sum_{ \\, \\{ u, w \\} \\,  \\equiv \\, e \\, \\in \\, \\textrm{E}_2(v) } s(u) s(w) \\right)\n","\\end{align}\n","$$\n","and when we compute the analogous quantity for $H_{\\textrm{Ising}}(s^{[v]})$, the second term remains the same, while the first term has its sign flipped. Subtracting the \"flipped Hamiltonian\" from the original, the second terms cancel. "],"metadata":{"id":"3T6W_12-3O9E"}},{"cell_type":"markdown","source":["___\n","\n","### __Algorithm.__ ( Metropolis-Hastings )\n","\n","Repeat $\\# \\mathbb{T}$ times:\n","\n","1. Choose a site $v \\in \\mathbb{T}$ uniformly at random. \n","\n","2. Compute $\\sigma(v)$ by summing the spins of all vertices neighboring $v$. \n","\n","3. Compute $\\delta_v H_{\\textrm{Ising}}(s) = 2 s(v) \\sigma(v)$,\n","\n","4. Spin flip protocol:\n","\n","    a. If $\\delta_v H_{\\textrm{Ising}}(s) < 0$, set $s(v) = -s(v)$. This ensures that we visit the configuration $s^{[v]}$ whenever flipping the spin at $v$ lowers the energy of the spin configuration\n","    \n","    b. If $\\delta_v H_{\\textrm{Ising}}(s) \\geq 0$, flip the spin at $v$ with probability \n","    $$\n","    \\exp ( - \\beta \\delta_v H_{\\textrm{Ising}}(s)  )\n","    $$\n","\n","___"],"metadata":{"id":"GDJb9K4H3gAV"}},{"cell_type":"markdown","source":["___\n","\n","### __Algorithm.__ ( Glauber )\n","\n","Repeat $\\#\\mathbb{T}$ times:\n","\n","1. Choose a site $v \\in \\mathbb{T}$ uniformly at random.\n","\n","2. Compute $\\sigma(v)$\n","\n","3. Compute $\\delta_v H_{\\textrm{Ising}}(s) = 2s(v) \\sigma(v)$\n","\n","4. Flip the spin at $v$ with probability\n","$$\n","\\frac{ \\exp(\\, - \\beta \\delta_v H_{\\textrm{Ising}}(s) \\,) }{1 + \\exp( \\, -\\beta \\delta_vH_{\\textrm{Ising}}(s)\\,)}\n","$$\n","\n","___\n"],"metadata":{"id":"frZL2aia3yJ-"}},{"cell_type":"markdown","source":["## implementation"],"metadata":{"id":"CX-FYXyX4bL5"}},{"cell_type":"markdown","source":["_some resources_\n","\n","* https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html\n","\n","* https://numpy.org/doc/stable/reference/random/index.html#random-quick-start"],"metadata":{"id":"45nTJsiD5Anm"}},{"cell_type":"markdown","source":["_imports_"],"metadata":{"id":"QCNzNDvHBgcB"}},{"cell_type":"code","source":["#collapse-hide\n","import numpy as np\n","import sys\n","from numpy.random import default_rng\n","\n","rng = default_rng()"],"metadata":{"id":"mLW6dyHu5Ib7","executionInfo":{"status":"ok","timestamp":1640286057997,"user_tz":360,"elapsed":7,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["___\n","\n","### coin_toss"],"metadata":{"id":"y8yCSvlS5TwF"}},{"cell_type":"code","source":["#collapse-hide\n","\n","def coin_toss(p = .5):\n","\n","    if Unif() <= p:\n","\n","        return True\n","\n","    else:\n","\n","        return False"],"metadata":{"id":"iou1sbR-5VXq","executionInfo":{"status":"ok","timestamp":1640286057998,"user_tz":360,"elapsed":6,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["___"],"metadata":{"id":"yzsACUVS5Vhh"}},{"cell_type":"markdown","source":["___\n","\n","### grid"],"metadata":{"id":"XsiXIXKa5e0g"}},{"cell_type":"code","source":["#collapse-hide\n","\n","class grid():\n","\n","    ##\n","    #   initialize\n","    #\n","    def __init__(self, \n","                 temperature,\n","                 n = 100, \n","                 starting_spin_config = 'inf_temp'):\n","\n","        self.n = n \n","        #   ^ side-length of torus\n","\n","        self.d = 2 \n","        #   ^ a two-dimensional torus, by default\n","\n","        self.N = self.n ** self.d\n","\n","        self.shape = (n, n)\n","\n","        self.spins = np.zeros(shape = self.shape, \n","                              dtype = int)\n","        \n","        ##\n","        # initial conditions\n","        #\n","        # starting_spin_config =\n","        #     * 'inf_temp'\n","        #     * 'plus'\n","        #     * 'minus'\n","        self.starting = starting_spin_config\n","        \n","        ##\n","        #   'inf_temp'\n","        if self.starting == 'inf_temp':\n","\n","            for x in range(n):\n","\n","                for y in range(n):\n","\n","                    coin = coin_toss()\n","\n","                    if coin:\n","\n","                        self.spins[x][y] = 1\n","\n","                    else:\n","\n","                        self.spins[x][y] = -1\n","\n","        ##\n","        #    'plus'\n","        elif self.starting == 'plus':\n","\n","            for x in range(n):\n","\n","                for y in range(n):\n","\n","                    self.spins[x][y] = 1\n","\n","        ##\n","        #    'minus'\n","        elif self.starting == 'minus':\n","\n","            for x in range(n):\n","\n","                for y in range(n):\n","\n","                    self.spins[x][y] = -1\n","\n","        \"\"\"\n","        later we implement some kind of loading\n","        from the end of a previous simulation\n","        \"\"\"\n","\n","        # regardless of the initial conditions,\n","        # we need a starting temperature, because\n","        # it governs each step of dynamics\n","        self.temperature = temperature\n","\n","        self.beta = 1 / self.temperature\n","\n","        # we need this to implement code related to\n","        # github author's \"note on boltzmann weight\"\n","        self.max_size = sys.maxsize\n","\n","        \"\"\"\n","        on storing dynamics history.\n","        \"\"\"\n","\n","        # we will track how many microsteps\n","        # the dynamics take. Number of microsteps\n","        # initialized at 0. \n","        self.current_time_step = 0\n","\n","        self.history = {}\n","\n","        # this records initial conditions in history\n","\n","        first_key = str( self.current_time_step )\n","\n","        first_spin_list = self.spins.tolist()\n","\n","        self.history[first_key] = first_spin_list\n","\n","    ##\n","    #   given vertex (x,y) return a list of its \n","    #   four neighbors\n","    #\n","    def neighbors(self, x, y):\n","\n","        # coordinates\n","        y_north = (y + 1) % self.n\n","\n","        y_south = (y - 1) % self.n\n","\n","        x_east = (x + 1) % self.n\n","\n","        x_west = (x - 1) % self.n\n","\n","        # tuples\n","        north = ( x, y_north )\n","\n","        south = ( x, y_south )\n","\n","        east = ( x_east, y )\n","\n","        west = ( x_west, y )\n","\n","        return [ north, south, east, west ]\n","\n","    ##\n","    #   using the neighbors method, we compute\n","    #   $\\delta_v H(s)$ given v = (x,y)\n","    #\n","    def delta_v_H(self, x, y):\n","\n","        # s(v)\n","        site_spin = self.spins[x][y]\n","\n","        # get neighbors\n","        nbs = self.neighbors(x,y)\n","\n","        # get their spins\n","        north_spin = self.spins[ nbs[0][0] ][ nbs[0][1] ]\n","\n","        south_spin = self.spins[ nbs[1][0] ][ nbs[1][1] ]\n","\n","        east_spin = self.spins[ nbs[2][0] ][ nbs[2][1] ]\n","\n","        west_spin = self.spins[ nbs[3][0] ][ nbs[3][1] ]\n","\n","        # compute sigma(v)\n","        sigma_v = north_spin + south_spin + east_spin + west_spin\n","\n","        # return\n","        return 2 * site_spin * sigma_v\n","\n","    \"\"\"\n","\n","    ( from the first github program, why we \n","      introduce self.max_size )\n","\n","    NOTE ON BOLTZMANN WEIGHT: After calculating the weight as\n","    exp(dE/T), why the Math.min and \n","    MAX_VALUE? If dE has a large negative value,\n","    and T is very small, exp(dE/T) can cause \n","    floating-point overflow. For example, exp(8/0.01) \n","    is > 10^347. JavaScript barfs on anything greater \n","    than 10^308.\n","\n","    \"\"\"\n","\n","    ##\n","    #   this function returns the boltzmann weight\n","    #   used to determine whether spin is flipped\n","    #   ( it is used in both types of dynamics )\n","    #\n","    def boltzmann_weight( self, x, y ):\n","\n","        energy_change = self.delta_v_H(x,y)\n","\n","        exponent = (-1) * self.beta * energy_change\n","\n","        raw_weight = np.exp( exponent )\n","\n","        # to_return = np.min( raw_weight, self.max_size )\n","\n","        return raw_weight # to_return\n","\n","    ##\n","    #   the next method handles the uniform selection\n","    #   of site v from the torus.\n","    #\n","    def site_select( self ):\n","\n","        # first define two random arrays\n","        # they are Gaussian random variables,\n","        # but this is going to be incidental. \n","\n","        # to select a coordinate uniformly at random,\n","        # we pick the one with the largest value.\n","\n","        array_1 = rng.standard_normal( self.n )\n","\n","        array_2 = rng.standard_normal( self.n )\n","\n","        first_coord = np.argmax( array_1 )\n","\n","        second_coord = np.argmax( array_2 )\n","\n","        return first_coord, second_coord\n","\n","    ##\n","    #   we perform one step ( a \"microstep\" )\n","    #   of the metropolis dynamics. This will \n","    #   later be repeated N times to form a \n","    #   \"macrostep\"\n","    #\n","    def metropolis_step(self):\n","\n","        # new time step\n","        self.current_time_step += 1\n","\n","        ##\n","        # step 1: select site 𝑣 ∈ 𝕋 uniformly\n","        x, y = self.site_select()\n","\n","        ##\n","        # step 2: compute 𝜎(𝑣) \n","        # step 3: compute 𝛿_𝑣 𝐻(𝑠) = 2 𝑠(𝑣) 𝜎(𝑣)\n","        #\n","        # ( both steps built into the function called )\n","        delta_H = self.delta_v_H(x,y)\n","\n","        ##\n","        # step 4: spin flip protocol\n","        #\n","        #\n","        #    a.   If 𝛿_𝑣 𝐻(𝑠) < 0, set 𝑠(𝑣) = −𝑠(𝑣)\n","        if delta_H < 0:\n","\n","            self.spins[x][y] = (-1) * self.spins[x][y]\n","        #    \n","        # \n","        #    b.   If 𝛿_𝑣 𝐻(𝑠) ≥ 0, \n","        #   flip the spin at 𝑣 with probability\n","        #      p := exp( − 𝛽 𝛿_𝑣 𝐻(𝑠) )\n","        else:\n","\n","            p = self.boltzmann_weight(x,y)\n","\n","            boo = coin_toss( p )\n","\n","            if boo:\n","\n","                self.spins[x][y] = (-1) * self.spins[x][y]\n","\n","            else:\n","\n","                pass\n","        \n","        self.write_state()\n","        \n","\n","    ##\n","    # Now we do the analogous thing for Glauber,\n","    # the following is one \"microstep\" of Glauber \n","    # dynamics\n","    #\n","    def glauber_step(self):\n","\n","        # new time step\n","        self.current_time_step += 1\n","\n","        ##\n","        # step 1: select site 𝑣 ∈ 𝕋 uniformly\n","        x, y = self.site_select()\n","\n","        ##\n","        # step 2: compute 𝜎(𝑣)\n","        # step 3: compute 𝛿_𝑣 𝐻(𝑠) = 2 𝑠(𝑣) 𝜎(𝑣)\n","        delta_H = self.delta_v_H(x,y)\n","\n","        ## \n","        # step 4: spin flip protocol:\n","        #\n","        #     flip the spin at 𝑣 with probability\n","        #\n","        #       exp( −𝛽 𝛿_𝑣 𝐻(𝑠) ) /\n","        #                         / ( 1 + exp( −𝛽 𝛿_𝑣 𝐻(𝑠) ) )\n","        #\n","        weight = self.boltzmann_weight(x,y)\n","\n","        p = weight / ( 1 + weight )\n","\n","        boo = coin_toss( p )\n","\n","        if boo:\n","\n","            self.spins[x][y] = (-1) * self.spins[x][y]\n","\n","        else:\n","\n","            pass\n","\n","        self.write_state()\n","\n","\n","    \"\"\"\n","    before introducing methods that run a macrostep\n","    of each type of dynamics, we first provide a method\n","    that stores the current state of the system.\n","\n","    In examining how to do this, it makes sense to \n","    provide `grid` with another attribute: \n","    `current_time_step` .. these numbers will serve \n","    as keys for the dictionary that stores the\n","    history of the dynamics\n","    \"\"\"\n","    def write_state(self):\n","\n","        dict_key = str( self.current_time_step )\n","\n","        spin_list = self.spins.tolist()\n","\n","        self.history[dict_key] = spin_list\n","\n","    # the above stores spin state to dict\n","    # we also need a method that writes\n","    # the dict to a file whose name is provided\n","    # we store as .json \n","\n","    def write_history(self, fp):\n","      \"\"\"dump the results of the simulation stored in `self.history` as json\n","      to the output file path specfied by the parameter `fp`\n","      \"\"\"\n","      import json\n","      with open(fp, 'w') as f:\n","        json.dump(self.history, f)\n","        print(f'Wrote results of simulation as json to {fp}.')"],"metadata":{"id":"2R13GYA25pFr","executionInfo":{"status":"ok","timestamp":1640286058447,"user_tz":360,"elapsed":455,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["___"],"metadata":{"id":"q1TZSC7D5qmu"}},{"cell_type":"markdown","source":["$\\quad$ We run the simulation for 10 microsteps of Metropolis dynamics, to test. \n"],"metadata":{"id":"C4zObJAW5y_l"}},{"cell_type":"code","source":["#collapse-hide\n","g = grid( temperature = 100 )\n","\n","for i in range(10):\n","  g.metropolis_step()\n","\n","g.write_history( fp = \"./test.json\" )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yuM2S3NE5umo","executionInfo":{"status":"ok","timestamp":1640286058744,"user_tz":360,"elapsed":301,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"26ebf2e5-e810-4f30-ceca-a5157fa60e63"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Wrote results of simulation as json to ./test.json.\n"]}]},{"cell_type":"code","source":["#collapse-hide\n","import json\n","\n","fil = open( \"./test.json\", 'r' )\n","\n","history_dict = json.load( fil )\n","\n","print( type(history_dict) )\n","\n","\"\"\"\n","for key, value in history_dict.items():\n","    print(f\"\\n\\nKey: {key}\")\n","    print(f\"\\nValue: {value}\\n\")\n","\"\"\"\n","\n","fil.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kh3w-Ss458u4","executionInfo":{"status":"ok","timestamp":1640286058745,"user_tz":360,"elapsed":12,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"44a9cb39-0012-4429-cdb4-7085b0658bd7"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'dict'>\n"]}]}]}