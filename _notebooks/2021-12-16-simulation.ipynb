{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2021-12-16-simulation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPq2gv+MygvQ1eIpwgVoKf9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PMzhwKYtm51E"},"source":["# Markov chains, Monte Carlo methods, pattern theory"]},{"cell_type":"markdown","metadata":{"id":"qvFJrPBxalKZ"},"source":["$\\quad$ These notes serve as background for an ongoing project with a good friend, [Connor Sempek](https://github.com/connorsempek). We are following [this](http://bit-player.org/2021/three-months-in-monte-carlo) blog post by [Brian Hayes](http://bit-player.org/about-the-author). Our current objective is to re-create Hayes' simulations of Ising model dynamics. The underlying canvas of the model is two-dimensional, so the simulation can be thought of as a way to generate a sequence of `100x100` pixel images, with pixel values either black or white. The Ising model dynamics update the current image by changing the value of one pixel at a time. The purpose of these notes is to get a better understanding of how long the simulation must be run so that, for a sparse enough subsequence of the images:\n","\n","* each is sampled approximately from the [Gibbs probability measure](https://en.wikipedia.org/wiki/Gibbs_measure) governing the Ising model, and \n","\n","* these are nearly independent."]},{"cell_type":"markdown","source":["$\\quad$ The latter property is important for a longer term goal of the project, which is to use these subsequences of images as a computer vision dataset. Importantly, the Gibbs measure we are sampling from always depends on a global temperature parameter $T$, leading to natural classification and regression tasks."],"metadata":{"id":"SBK7w-YTq3qX"}},{"cell_type":"markdown","source":["$\\quad$ Here is a starting point for formulating a classification task. Suppose that we can simulate Ising model dynamics well at two _distinct_ temperatures. It feels natural to choose one of these, say $T_1$, above the critical temperature $T_c$ of the model, and the other, $T_2$ below it. We then create two image datasets $\\mathcal{D}_1$ and $\\mathcal{D}_2$ of equal size by simulating as described by the two points above. Merging these datasets to form what we call $\\mathcal{D}$ leads to a natural classification problem: given an element $x \\in \\mathcal{D}$ chosen uniformly at random, determine whether it came from $\\mathcal{D}_1$ or $\\mathcal{D}_2$. "],"metadata":{"id":"W7YA8ZaTq-Nh"}},{"cell_type":"markdown","source":["$\\quad$ The relevance of the Ising model to vision has been explored heavily, it seems. A lot of the text below draws from Govind Menon's [pattern theory](https://www.dam.brown.edu/people/menon/publications/pt-2020.pdf) notes, where a chapter is devoted to this \"energetic\" perspective applied to character recognition tasks. Menon's notes also reference a 1984 [paper](http://www.peterbeerli.com/classes/images/a/a1/Geman_geman_1984.pdf) which develops a way to perform image restoration and enhancement using the Ising model. This makes it a natural model to experiment with, and there are a few directions I'd like to explore longer term:\n","\n","1. At the lower temperature, $T_2$, simulating the model is akin to an optimization problem: as $T \\to 0$, the Gibbs measure concentrates on the set of lowest-energy states, or _ground states_. Any simulation designed to sample from a low-temperature Gibbs measure needs to navigate the energy landscape of the system from some starting point to a low altitude, with the temperature dictating a small range of desirable low altitudes $\\equiv$ low energies. This is effectively an optimization problem with cost function corresponding to the energy landscape. There is also an optimization inherent to the process of training any neural network on the classification task suggested. The question I would like to explore, loosely, is whether there are useful connections between these optimization problems. "],"metadata":{"id":"nmyNdro9s0sU"}},{"cell_type":"markdown","source":["\n","2. We will train a simple convolutional architecture on the dataset $\\mathcal{D}$. Whether it is through pooling layers, or the local averaging inherent in the convolution operation, these architectures seem to apply a kind of [coarse-graining](https://en.wikipedia.org/wiki/Coarse-grained_modeling) to input signals $x \\in \\mathcal{D}$. This has prompted investigation into the connections between deep learning and the [renormalization group](https://en.wikipedia.org/wiki/Renormalization_group) of physics. The latter feels like a kind of microscope for theoretical physicists $-$ the renormalization group formalizes a sequence of scaling transformations acting on a physical system, typically zooming out and thereby coarse-graining the preceding image of the system. This parallels feeding the image of the system into a \"deep\" sequence of convolutional layers. Two references for this are a 2013 [paper](https://arxiv.org/abs/1301.3124) of Cédric Bény, and a 2014 [paper](https://arxiv.org/abs/1410.3831v1) of Pankaj Mehta and David Schwab, and I imagine there is more recent work on this to explore. Dataset $\\mathcal{D}$ is both an image dataset and a natural input to the renormalization group. It will give us an opportunity to better understand these connections, in particular we can start by repeating the experiments the latter paper runs on the Ising model."],"metadata":{"id":"zJGz1qQt4la7"}},{"cell_type":"markdown","source":["3. A goal less directly related to ML is to augment the simulation to reproduce the simulations of Lubetzky and Sly on _information percolation_ ([main article](https://math.nyu.edu/~eyal/papers/cutoffbhZd.pdf) with visualization, and [this](https://cims.nyu.edu/~eyal/papers/cutoffbh_expo.pdf) is a shorter expository version, still technical). Information percolation stacks each time-slice of the simulation described above (at some fixed temperature) into a 'space-time cylinder'. Multi-colored edges or 'bonds' are drawn _between_ pixels in successive time-slices, based on how the simulation dynamics progress. The color of the bonds reflects different possibilities for how the update step is executed at a given pixel: sometimes, the randomness involved in the update is completely independent of everything else, and this is how the simulation gradually loses its memory of its starting point. The information percolation process outputs a graph or network inside the space-time cylinder encoding all aspects of the simulation. The relevance of information percolation is that becoming decoupled from the simulation starting point has the interpretation that the top slice of the space-time cylinder is _not_ connected to the bottom slice through the information perecolation process. This is a way to graphically represent the 'mixing' undergone by the Ising dynamics. Lubetzky and Sly use this perspective to deduce lower bounds on how long the simulation dynamics must be run in order to sample effectively from the Gibbs measure of the Ising model. So this is a tool relevant to both bullet points above, and could look cool to simulate and visualize."],"metadata":{"id":"HR1N84qk4myI"}},{"cell_type":"markdown","metadata":{"id":"xlMuxTrt1jMI"},"source":["___\n","\n","_references_\n","\n","* Krauth (2006) _statistical mechanics: algorithms and computations_\n","\n","* Menon (2020) _[pattern theory](http://www.dam.brown.edu/people/menon/publications/pt-2020.pdf)_\n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"MXliFLeynRga"},"source":["$\\quad$ The Monte Carlo method is increasingly becoming part of the discipline it is meant to study. The Monte Carlo method is a statistical, _\"almost experimental\"_, approach to computing integrals using random positions, called __samples__. Below, we introduce some sampling techniques for continuous and discrete variables. We also discuss the basic principles of statistical data analysis: how to extract results from well-behaved simulations. We also discuss at length the simulations where something goes wrong. "]},{"cell_type":"markdown","metadata":{"id":"QCkfLftjgUnR"},"source":["$\\quad$ In probabilistic terms, the integrals we desire to approximate correespond to the expected value of some observable, $\\mathcal{O} : \\mathcal{S} \\to \\mathbb{R}$. Here we assume some implicit probability measure $\\mu$ on $\\mathcal{S}$, the latter called the __state space__. Formally, the expected value $\\mathbb{E} \\mathcal{O}$ corresponds to the integral\n","$$\n","\\mathbb{E} \\mathcal{O} = \\int_{ \\mathcal{S} } \\mathcal{O}(s) \\,  \\mu( \\textrm{d} s )\n","$$\n","We write this expectation either as $\\mathbb{E} \\mathcal{O}$, or as $\\langle \\mathcal{O} \\rangle$, the latter notation more common in physics.  "]},{"cell_type":"markdown","metadata":{"id":"dW4JtbIKqwVA"},"source":["$\\quad$ We want to simulate most of the objects discussed throughout the notes, so we lose no generality in assuming that $\\mathcal{S}$ is finite. $\\mathcal{S}$ will be treated as a continuous object when it is more natural, or becomes convenient.\n","\n","> I am tempted to jump to conclusions in the following way: first, note that if $\\mathcal{S}$ is finite with $\\# \\mathcal{S} = M$, then $\\mu$ corresponds to its probability mass function $p = (p_1, \\dots, p_M)$. In this case, the above integral becomes the $\\sum_{i=1}^M o_i p_i$ for a given vector $o = (o_1, \\dots, o_M)$. The conclusion I jumped to was that approximating this sum happens first by estimating each $p_i$ through a statistic (like a histogram) $\\hat{p}$ of some long-running simulation, and then taking a dot product of $\\hat{p}$ with $o$. \n","\n","> To be clear, in both direct sampling and Markov chain sampling, it seems instead that our approximation comes from taking measurements of a system as it evolves, and only after it has been allowed to evolve for some time. The measurements applied to each state of $\\mathcal{S}$ are encoded in the measurement of the system, and the probability vector $p$ (of $\\mu$) is encoded in the system evolution itself. \n","\n","$\\quad$ We wish to distinguish between two distinct sampling approaches: __direct sampling__ and __Markov chain sampling__. We postpone the definition of a Markov chain until later; for now it is understood as a random walk."]},{"cell_type":"markdown","metadata":{"id":"p7CJ-ZhlrLHu"},"source":["## ... direct sampling ...\n","\n","_(Krauth $\\S$ 1.1.1)_"]},{"cell_type":"markdown","metadata":{"id":"Q1ABNGCsrOMt"},"source":["$\\quad$ Krauth explains direct sampling through a game played with small pebbles. The playing field is a large circle is inscribed into a large square, both drawn in the sand. In direct sampling, one has 'access' to the measure $\\mu$ sampling from $\\mathcal{S}$. In the context of the game, we will assume we can directly sample from $\\mu$, which denotes the uniform measure on the square \n","$$\n","\\mathcal{S} \\equiv \\textsf{square} = [-1,1]^2   \\,,\n","$$ \n","a 'birds-eye' view of the playing field. Not only this, but we assume that we can also sample directly from $\\mu$ multiple times, _independently_. \n","\n","> The observable considered in the game is related to its goal, namely to compute an approximation of the number $\\pi$. The observable $\\mathcal{O} : \\mathcal{S} \\to \\mathbb{R}$ is the indicator function of the unit disc, \n","$\\textsf{disc} = \\{ s = (x,y) \\in \\textsf{square} : x^2 + y^2 \\leq 1 \\}$\n","$$\n","\\mathcal{O} \\equiv \\mathbf{1}_{ \\textsf{disc}} =\n","\\begin{cases}\n","1 & \\quad s \\in \\textsf{disc} \\\\\n","0 & \\quad \\textrm{otherwise}\n","\\end{cases} \n","$$"]},{"cell_type":"markdown","metadata":{"id":"_o5xGwUTWM1V"},"source":["$\\quad$ A player throws pebbles at the square, successively. Through the direct sampling, these land uniformly at random. The sampling is performed independently across draws from $\\mu$. In particular, no pebble throw ever misses the square. Let $S_1, \\dots, S_N$ denote the collection of locations sampled from $\\textsf{square}$ in the above procedure: each $S_i$ is a random ordered pair $S_i = (x_i, y_i) \\in \\textsf{square}$, where all coordinates are mutually independent, $\\textrm{Unif}([-1,1])$ random variables.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jFnCx8JBXf4W"},"source":["$\\quad$ To describe how the samples $S_i$ are used to approximate the value of $\\pi$, we introduce the __empirical measure__ $\\mu_N^{\\textrm{direct}}$, a random probability measure on $\\mathcal{S}$, defined to encode the locations $S_1, \\dots, S_N$: \n","$$\n","\\mu_N^{\\textrm{direct}} := \\frac{1}{N} \\sum_{i=1}^N \\delta_{S_i},\n","$$\n","where in general $\\delta_S$ denotes the delta mass at the random point $S$.  Measures are in a sense dual to sets, and the __delta mass__ $\\delta_X$ is a measure acting on sets in the following way: given measurable $B \\subset \\textsf{square}$, we have\n","$$\n","\\delta_S = \n","\\begin{cases}\n","1 & \\quad \\textrm{ if } S \\in B, \\\\\n","0 & \\quad \\textrm{ otherwise. }\n","\\end{cases}\n","$$\n","The empirical measure $\\mu_N^{\\textrm{direct}}$, by definition, acts on sets $B \\subset \\mathcal{S}$ by reporting the fraction of the direct samples $S_i$ landing in $B$. "]},{"cell_type":"markdown","metadata":{"id":"iFLP2wJTWJoS"},"source":["$\\quad$ There is a sense in which the measures $\\mu_N^{\\textrm{direct}}$ 'converge' to $\\mu$, and the game, or simulation, is designed to exploit this. Let us enumerate the states of $\\mathcal{S} = \\{s_1, \\dots, s_M \\}$. In practice, the size of $M$ will be determined by the 'resolution' of the floating points used in sampling. In accordance with the observable $\\mathcal{O}$, the set we choose for $B$ is $\\textsf{disc}$. This is because\n","$$\n","\\mu_N^{\\textrm{direct}}( \\textsf{disc} ) = \\frac{1}{N} \\sum_{i=1}^N \\delta_{S_i}( \\textsf{disc} ) \\equiv \\frac{1}{N} \\sum_{i=1}^M \\mathcal{O}(S_i)\n","$$\n","The sense in which $\\mu_N^{\\textrm{direct}}$ converges to $\\mu$ implies, in particular, that the sequence of numbers $\\mu_N^{\\textrm{direct}}(\\textsf{disc})$ converges to $\\mu( \\textsf{disc} )$ as $N \\to \\infty$. This convergence can also be seen as a consequence of a law of large numbers. \n","\n","\n","In the present context, the relevant subset of $\\textsf{square}$ is the disc of radius one, centered at the origin. We refer to this as $\\textsf{disc}$. The output of the first algorithm below is \n","\n","$$\n","4 \\cdot \\mu_N^{\\textrm{direct}}(\\textsf{disc}) \\approx 4 \\cdot \\mu( \\textsf{disc}) \\equiv 4 \\cdot \\frac{ \\pi}{4} \n","$$\n","\n","Thus we are approximating $\\pi$ by approximating $\\pi/4$, which is the probability that a pebble sampled from $\\mu$ lands in $\\textsf{disc}$."]},{"cell_type":"code","metadata":{"id":"1LRbgOiezyuu"},"source":["import numpy as np\n","import sys\n","from numpy.random import default_rng\n","\n","rng = default_rng()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WKFM253wdpI7"},"source":["The next function embodies the 'direct' sampling we supposedly have access to, with the independence assumptions described above. "]},{"cell_type":"markdown","metadata":{"id":"Co0SE-jU0Dg7"},"source":["___\n","\n","### method ... Unif"]},{"cell_type":"code","metadata":{"id":"oic2NJ1U0Dx-"},"source":["# uniform r.v. over interval [0,1]\n","def Unif():\n","    return rng.uniform()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2twWt41N0D63"},"source":["___"]},{"cell_type":"markdown","metadata":{"id":"wPetKay_dUfe"},"source":["Algorithm 1.1 is translated from pseudocode (as it is presented in Krauth) to Python."]},{"cell_type":"markdown","metadata":{"id":"U4CGL4fMzoCg"},"source":["___\n","\n","### Algorithm 1.1 ... direct_pi"]},{"cell_type":"code","metadata":{"id":"FfTRHDfSzq9p"},"source":["def direct_pi(N = 4000):\n","\n","    N_hits = 0\n","\n","    \"\"\"\n","    the for loop is basically about computing the sum in\n","    \\mu_N(disc), stored in N_hits\n","    \"\"\"\n","\n","    for i in range(N):\n","\n","        # initialize Unif([-1,1]) r.v.'s\n","        x = 2 * Unif() - 1\n","        y = 2 * Unif() - 1\n","\n","        rad_sq = (x ** 2) + (y ** 2)\n","\n","        if rad_sq <= 1:\n","\n","            N_hits += 1\n","    \"\"\"\n","    \\mu_N(disc) is then computed by dividing by N\n","    \"\"\"\n","    ratio = N_hits / N\n","\n","    return 4 * ratio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rnV5EJo0Lpb"},"source":["___"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vYSpMWqj0MDg","executionInfo":{"status":"ok","timestamp":1638916904038,"user_tz":360,"elapsed":6,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"cabb5035-91a1-40ae-c502-fde6cb4fa58c"},"source":["direct_pi()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.134"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"bio6STkV1zRQ"},"source":["In Krauth, Table 1.1 gives results of five runs of the above with `N = 4000`. We return to this table later to compare with Monte Carlo methods. \n","\n","| run | `N_hits` | estimate of $\\pi$ |\n","| -------- | -------- | --- |\n","| 1 | 3156 | 3.156 |\n","| 2 | 3150 | 3.150 |\n","| 3 | 3127 | 3.127 |\n","| 4 | 3171 | 3.171 |\n","| 5 | 3148 | 3.148 |\n","\n","Krauth remarks that none of the results of this table have fallen in the error bounds known since Archimedes. Another remark of Krauth: _\"The people adopt a sensible rule: they decide on the total number of throws, before they start the game.\"_ (This is as in the above python code.) _\"They understood that one must not stop a stochastic calculation simply because the current result appears accurate, nor should they continue to play because the answer they get isn't close enough to their idealized target.\"_\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gUTKvlzO2vOz"},"source":["## ... Markov-chain sampling ...\n","\n","_( Krauth $\\S$ 1.1.2 )_"]},{"cell_type":"markdown","metadata":{"id":"psgikwml4MvA"},"source":["$\\quad$ We now consider a new game being played on the same field, with different rules. We again take a birds-eye view of the playing field, so our state space is again $\\mathcal{S} = \\textsf{square}$. The game starts with a person at the corner of the helipad, corresponding to coordinates $(1,1)$ in $\\mathcal{S}$. In Krauth's description, in each coordinate, the walker takes a step via adding independent, $\\textrm{Unif}([-\\delta, \\delta])$ random variables. Thus it is possible for a next step which takes the person outside the square. If this is the case, instead of taking the step, the walker does nothing. This step still 'counts' in the aggregation we will perform. Krauth describes this procedure in terms of the pebbles: the person takes another pebble from their bag and places it on top of the one pebble (or perhaps pebble pile) marking their current location."]},{"cell_type":"markdown","metadata":{"id":"dW7FC6nH5Vaz"},"source":["$\\quad$ After an 'in-bounds' step is taken, the walker places a pebble at their current position. Each walker is equipped with a bag of infinitely many pebbles. These two cases describe the way the new game is played. We make one modification to this, which is that "]},{"cell_type":"markdown","metadata":{"id":"kEBBTTSLYW9G"},"source":["$\\quad$ In the present setting, the __samples__ $X_1, \\dots, X_N$ are not independent, but rather have the structure of a Markov chain, specifically a random walk with independent increments. Let us use $\\xi_1, \\dots, \\xi_N$ denote these independent increments. In Krauth's description, each of these is a $\\textrm{Unif}([-\\delta, \\delta])$ random variable. For reasons that Krauth discusses later, and we will comment on this, we instead assume that each $\\xi_j$ is a discrete random variable uniformly distributed on the two-element set $\\{ -\\delta, \\delta \\}$. We have distinguished the starting point of the sampling at $X_0 = (1,1)$. Following the above description, one then has\n","$$\n","X_1 = \n","\\begin{cases}\n","X_0 + \\xi_1\\,, & \\quad \\textrm{ if } X_0 + \\xi_1 \\in \\textsf{square}\\\\\n","X_0 & \\quad \\textrm{ otherwise }\n","\\end{cases}\n","$$\n","\n","and in general,\n","\n","$$\n","X_j = \n","\\begin{cases}\n","X_{j-1} + \\xi_j\\,, & \\quad \\textrm{ if } X_{j-1} + \\xi_j \\in \\textsf{square}\\\\\n","X_{j-1} & \\quad \\textrm{ otherwise }\n","\\end{cases}\n","$$\n","\n","for all $j = 1, \\dots, N$."]},{"cell_type":"markdown","metadata":{"id":"BnBlpKW3lof0"},"source":["$\\quad$ For this new sequence of random $X_i$, we can define the analogous object to $\\mu_N^{\\textrm{direct}}$, \n","$$\n","\\mu_N^{\\textrm{Markov}} := \\frac{1}{N} \\sum_{i=1}^N \\delta_{X_i}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"uNhfAcxrs0Yj"},"source":["The algorithm below returns an approximation to $\\pi$ via the same kind of ratio\n","\n","$$\n","\\pi \\approx 4 \\cdot \\mu_N^{\\textrm{Markov}}(\\textsf{disc}) \n","$$"]},{"cell_type":"markdown","metadata":{"id":"ye3ly8tSzq5H"},"source":["$\\quad$ One might be tempted to use random initial conditions, assigning independent $\\textrm{Unif}([-1,1])$ random variables to the two coordinates. We avoid this because the Markov chain methods become most useful when no direct sampling method exists. It is for exactly this reason that we also avoid directly sampling from $\\textrm{Unif}([-\\delta, \\delta])$ random variables: such objects are a linear transformation away from directly sampling from the $\\textrm{Unif}([-1,1])$ distribution, perhaps at lower resolution. \n","\n","$\\quad$  Consequently, our Markov-chain sampling simulations start at the corner $(1,1)$ for concreteness. They can also be assumed to start from where a previous simulation left off. Following this convention throughout these notes, we usually focus on going from configuration $i$ to configuration $i+1$. This is the defining property of (discrete-time) Markov chains: the transition probabilities of the process depend only on the current position. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"SEy5vsQODx9z"},"source":["___\n","\n","### Algorithm 1.2 ... markov-pi (unmodified)"]},{"cell_type":"code","metadata":{"id":"Xvvimks0D29z"},"source":["def markov_pi( delta = .05, N = 40000 ):\n","\n","    # initial conditions\n","    x = 1\n","\n","    y = 1\n","\n","    N_hits = 0\n","\n","    num_rej_moves = 0 # number of \"rejected\" moves\n","\n","    for i in range(N):\n","\n","        # get coordinate increments\n","        del_x_big = 2 * Unif() - 1\n","\n","        del_y_big = 2 * Unif() - 1\n","\n","        del_x = delta * del_x_big\n","\n","        del_y = delta * del_y_big\n","\n","        #print(del_x, del_y)\n","\n","        # potential new coordinates\n","        x_new = x + del_x\n","\n","        y_new = y + del_y\n","\n","        # to determine if step leads into disc\n","        rad_sq_new = ( x_new ** 2 ) + ( y_new ** 2 )\n","\n","        # ditto for square\n","        x_in = x_new <= 1 and x_new >= -1\n","\n","        y_in = y_new <= 1 and y_new >= -1\n","\n","        in_square = x_in and y_in \n","\n","        # if move is accepted\n","        if in_square:\n","\n","            x = x_new\n","\n","            y = y_new\n","\n","        else:\n","\n","            num_rej_moves += 1\n","\n","        # if in disc, +1 to number of hits\n","        if rad_sq_new <= 1:\n","\n","            N_hits += 1\n","\n","    # we add in this output to \n","    # better understand how to pick \n","    # delta\n","    print( \"fraction of successful moves: \", 1 - ( num_rej_moves / N) )\n","\n","    return 4 * ( N_hits / N )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmu4xwohD3F_"},"source":["___"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jq0UNtjTKPYY","executionInfo":{"status":"ok","timestamp":1638916909861,"user_tz":360,"elapsed":312,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"2cc6af5f-7989-4359-8707-752d259080ab"},"source":["markov_pi()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fraction of successful moves:  0.9751\n"]},{"output_type":"execute_result","data":{"text/plain":["3.0945"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"icDEcI4TFLyc"},"source":["$\\quad$ Let us discuss how to choose the  'throwing range' is $\\delta$. It is akin to (largest possible) step size for the walker. Roughly, if $\\delta$ is too small, the acceptance rate may be high, but the claim is that the walk will not be able to explore enough to well-approximate the integral. Choosing $\\delta$ too large means the walk will have a hard time making successful moves, also hindering exploration. Krauth: _\"The time-honored rule of thumb consists in choosing $\\delta$ neither too large, nor too small, so that the acceptance rate turns our to be on the order of $\\frac{1}{2}$.\"_ I didn't observe results agreeing with their rule of thumb. I found much more success with $\\delta = 0.05$ versus their recommendation of $0.3$. I did however observe the approximation worsen as the step size was taken larger past $\\delta = 1$. "]},{"cell_type":"markdown","source":["___\n","\n","### Algorithm 1.2' ... markov-pi (modified)"],"metadata":{"id":"mp2-7ijlBgvE"}},{"cell_type":"code","source":["# in this modification, steps are no longer uniform  \n","\n","def markov_pi_modified( delta = .05, N = 40000 ):\n","\n","    # initial conditions\n","    x = 1\n","\n","    y = 1\n","\n","    N_hits = 0\n","\n","    num_rej_moves = 0 # number of \"rejected\" moves\n","\n","    for i in range(N):\n","        \"\"\"\n","        we use uniform random variables, out of convenience but not to\n","        determine coordinate steps directly with these. Instead, we \n","        extract a pair of fair coin tosses from two uniform random\n","        variables. We use these coin tosses only to determine the sign\n","        of the increment in each coordinate. \n","        \"\"\"\n","\n","        # generate indep uniforms\n","        U_1 = Unif()\n","\n","        U_2 = Unif()\n","\n","        # use these as coin tosses to determine \n","        # del_x...\n","\n","        if U_1 <= 0.5:\n","        \n","            del_x = delta\n","\n","        else:\n","\n","            del_x = (-1) * delta\n","\n","        if U_2 <= 0.5:\n","\n","            del_y = delta\n","\n","        else:\n","\n","            del_y = (-1) * delta\n","\n","        # potential new coordinates\n","        x_new = x + del_x\n","\n","        y_new = y + del_y\n","\n","        # to determine if step leads into disc\n","        rad_sq_new = ( x_new ** 2 ) + ( y_new ** 2 )\n","\n","        # ditto for square\n","        x_in = x_new <= 1 and x_new >= -1\n","\n","        y_in = y_new <= 1 and y_new >= -1\n","\n","        in_square = x_in and y_in \n","\n","        # if move is accepted\n","        if in_square:\n","\n","            x = x_new\n","\n","            y = y_new\n","\n","        else:\n","\n","            num_rej_moves += 1\n","\n","        # if in disc, +1 to number of hits\n","        if rad_sq_new <= 1:\n","\n","            N_hits += 1\n","\n","    # we add in this output to \n","    # better understand how to pick \n","    # delta\n","    print( \"fraction of successful moves: \", 1 - ( num_rej_moves / N) )\n","\n","    return 4 * ( N_hits / N )\n"],"metadata":{"id":"yAU9idoiBg3c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["___"],"metadata":{"id":"_CaWJgq7BhCB"}},{"cell_type":"code","source":["markov_pi_modified()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAJ5NUq7DyC1","executionInfo":{"status":"ok","timestamp":1638916923161,"user_tz":360,"elapsed":319,"user":{"displayName":"Julian Gold","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh-WPTDjzM9OoEUntvRpUTCsTmARyNgiBOgQHfQ=s64","userId":"01597584131251118338"}},"outputId":"b0002122-8d77-4c5d-f754-650f32865750"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fraction of successful moves:  0.953025\n"]},{"output_type":"execute_result","data":{"text/plain":["3.1668"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"-vd8su1q6NKa"},"source":["___\n","\n","__Q__ $\\quad$ In both the Glauber and Metropolis dynamics for the evolution of the Ising model, the spin variable to be updated ( i.e. the vertex $v \\in \\mathbb{T}$ sampled ) is chosen uniformly, through _direct sampling_. What if this is instead done through Markov-chain sampling? \n","\n","Also: instead of rejecting the moves outside the box, we can play with periodic boundary conditions for the walks in both simulations.\n","___"]},{"cell_type":"markdown","source":["## ... discussion ...  "],"metadata":{"id":"EY3KxkBeBJUL"}},{"cell_type":"markdown","metadata":{"id":"shleUtBs0yUD"},"source":["$\\quad$ The Monte Carlo 'games' described above epitomize the two basic approaches to sampling from a probability measure $\\mu$ on some space $\\mathcal{S}$. Both approaches to the sampling evaluate an observable $\\mathcal{O} : \\mathcal{S} \\to \\mathbb{R}$. In our examples, $\\mathcal{S} = \\textsf{square}$, and the observable $\\mathcal{O}$ is the indicator function of the disc within the square:\n","$$\n","\\mathbf{1}_{\\textsf{disc}}(s) = \\begin{cases}\n","1 & \\quad s \\in \\textsf{disc} \\\\\n","0 & \\quad \\textrm{otherwise}\n","\\end{cases} \\,, \\quad \\mathbf{1}_{\\textsf{disc}} : \\textsf{square} \\to \\mathbb{R} \n","$$"]},{"cell_type":"markdown","metadata":{"id":"RF4Lz6Bj42Qj"},"source":["In either sampling case, we can define $\\mathcal{O}_i := \\mathcal{O}(X_i)$, and note that one evaluates\n","\n","$$\n","\\frac{N_{\\textrm{hits}}}{N} = \\frac{1}{N} \\sum_{i=1}^N \\mathcal{O}_i \\approx \\langle \\mathcal{O} \\rangle,\n","$$\n","\n","where the brackets $\\langle \\, \\cdot \\, \\rangle$ denote the expected value of $\\mathcal{O}$ taken with respect to the probability measure $\\mu$, which in this case is uniform on $[-1,1]^2$. "]},{"cell_type":"markdown","metadata":{"id":"80aPGL5H7cgi"},"source":["$\\quad$ Krauth remarks that $\\langle \\mathcal{O} \\rangle$ is an integral,\n","$$\n","\\langle \\mathcal{O} \\rangle = \\int_{-1}^1 \\textrm{d} x \\int_{-1}^1 \\textrm{d}y \\, \\mu (x,y) \\mathcal{O}(x,y)\n","$$\n","In his notes, Krauth includes a denominator on the right-hand side above, it is:\n","$$\n","\\int_{-1}^1 \\textrm{d} x \\int_{-1}^1 \\textrm{d}y \\, \\mu (x,y) \n","$$\n","\n","My understanding of $\\mu(x,y)$ is that it is used to denote the density of probability measure $\\mu$, so it integrates to one. Within the above bounds of integration, it can be taken as a constant, namely $1/4$, which is the reciprocal of the area of $[-1,1]^2$. Regardless, there is an integral to be computed, and this is an aspect Krauth remarks on next. "]},{"cell_type":"markdown","metadata":{"id":"NZvyXir68l5G"},"source":["$\\quad$ The density $\\mu(x,y)$ doesn't appear in the approximation $\\tfrac{1}{N} \\sum_{i=1}^N \\mathcal{O}_i$, Krauth: _\"...rather than being evaluated, it is sampled. This is what defines the Monte Carlo method.\"_ Krauth goes on to point out that the multiple integrals also disappear. _\"This means that the Monte Carlo method allows the evaluation of high-dimensional integrals, such as appear in statistical physics and other domains, if we can only think of how to generate the samples.\"_\n","\n","$\\quad$ The multiple integral above is integration in two variables, so it doesn't feel like it is genuinely a 'high-dimensional' integral to compute. On the other hand, we are already know of a kind of Markov chain on a high-dimensional space, namely the Glauber dynamics for the Ising model on $\\mathbb{T}$, the two-dimensional torus $( \\mathbb{Z}\\, /\\, (n\\cdot \\mathbb{Z}) )^2$.  All in all, it seems that one is approximating an integral in an arbitrary number of dimensions (number of spins in the case of Ising model) through what is essentially a path integral. The path integral is a sum of observables of the system, as the system evolves. "]},{"cell_type":"markdown","metadata":{"id":"bGrkyqK9H0Rx"},"source":["$\\quad$ Krauth: _\"Direct sampling, the approach inspired by the game, is like pure gold: a subroutine provides an independent hit at the distribution function $\\pi$. Notwithstanding thee randomness in the problem, direct sampling, in computation, plays a role similar to exact solutions in analyitcal work, and the two are closely related. In direct sampling, there is no throwing-range issue, no worrying about initial conditions, and there is a straightforward error analysis -- at least if $\\mu$ and $\\mathcal{O}$ are well behaved. Many successsful Monte Carlo algorithms contain exact sampling as a key ingredient.\"_\n","\n","$\\quad$ _\"Markov-chain sampling, on the other hand, forces us to be much more careful with all aspects of our calculation. The critical issue here is the correlation time, during which the position of the walker retains a memory of the starting configuration. This time can become astronomical. In the usual applications, onee is often satisfied with a handful of independent samples, obtained through week-long calculations, but it can require much throught and experience to ensure that even this modest goal is achieved. We shall continue our discussion of these methods in a later section. We first look briefly at the history of stochastic computing.\"_\n","\n","before this, we draw background on Markov chains and Gibbs measures from Menon. \n"]},{"cell_type":"markdown","metadata":{"id":"sqDxS0bcM9Vk"},"source":["## ... Markov chains ...\n","\n","_(Menon $\\S$ 1.1, 1.2)_"]},{"cell_type":"markdown","metadata":{"id":"Y3W1Q8HaNAFx"},"source":["$\\quad$ We model the world as a space of random signals. The world is filled with observers, which record part of these signals and then form inferences about the world. The process of inference is modeled with a Bayesian paradigm. We assume an observer has a stochastic model capable of generating signals similar to the true signal. Inference reduces to tuning the model parameters to obtain the best match between the generated and observed signals."]},{"cell_type":"markdown","metadata":{"id":"6zvg1jY7a0pa"},"source":["$\\quad$ These notes explore stochastic models of progressively increasing complexity, in tandem with numeraical algorithms for optimization. The simplest class of stochastic models we consider are Markov chains. Suppose we are given a finite set $\\mathcal{S}$ called the **state space** of the Markov chain. This will sometimes (such as in text-modeling instances) be called the **alphabet**, in which case we may use $\\mathcal{A}$ in place of $\\mathcal{S}$. \n"]},{"cell_type":"markdown","metadata":{"id":"W0XWV0LibDTk"},"source":["___\n","\n","### **Definition.** ( stochastic process )\n","\n","$\\quad$ A **stochastic process** is a collection of random variables.\n","___"]},{"cell_type":"markdown","metadata":{"id":"tH5Ualw8O_2o"},"source":["$\\quad$ We study a stochastic process $(X_t)_{t \\in \\mathbb{N}}$ taking values in the given finite set $\\mathcal{S}$. Thus, the temporal (given by the index $t$ in the stochastic process) and the spatial (given by the state value in $\\mathcal{S}$ of the process) aspects of this process are discrete. "]},{"cell_type":"markdown","metadata":{"id":"8HUczPeBPCcA"},"source":["___\n","\n","### **Definition.** ( Markov chain )\n","\n","$\\quad$ The stochastic process $(X_t)_{t \\in \\mathbb{N}}$ is a **Markov chain** if for all $t$, $\\mathbb{P} ( X_{t +1} | X_1, \\dots, X_t ) = \\mathbb{P} (X_{t+1} | X_t )\n","$.\n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"zPHUqLTKbT5k"},"source":["$\\quad$ As the above definition indicates, the modeling assumption in Markov process theory is that the future of the process depends on the current state, but not the past. This is the probabilistic analogue of the idea of Newtonian determinism, and is encoded in the _forward equation_ below. Stationary Markov chains are thus characterized by their __transition matrix__ \n","$$\n","Q(s, \\tilde{s}) = \\mathbb{P}( X_2 = \\tilde{s} | X_1 = s )\\,,\n","$$ \n","with entries indexed by pairs of states $s, \\tilde{s} \\in \\mathcal{S}$. Another interpretation of the Markov property is that the future and past are independent, conditional on the present. We typically simplify the structure of Markov chains with the following assumption.\n"]},{"cell_type":"markdown","metadata":{"id":"IKcB4VtrPFOr"},"source":["\n","___\n","\n","### **Definition.** ( stationary Markov chain )\n","\n","$\\quad$ The Markov chain $(X_t)$ is **stationary** if $\\mathbb{P}(X_{t+1} | X_t ) = \\mathbb{P} (X_2 | X_1 )$ for all $t \\in \\mathbb{N}$. \n","___"]},{"cell_type":"markdown","metadata":{"id":"prL0S6GkPIqg"},"source":["$\\quad$ The most basic statistic associated to the Markov chain is the pmf( $\\equiv$ probability mass function ), or law of the random variable $X_t$. The mass assigned to state $s \\in \\mathcal{S}$ under this law is denoted\n","$$\n","m_t(s) := \\mathbb{P}(X_t = s) \\,,\n","$$\n","and one obtains the following recurrence relation using the so-called law of total probability, along with the definitions of $m_t$ and $Q$:\n","$$\n","\\begin{align}\n","m_{t+1}(\\tilde{s}) &= \\mathbb{P} ( X_{t+1} = \\tilde{s}) \\\\\n","&= \\sum_{s \\in \\mathcal{S}} \\mathbb{P}( X_{t+1} = \\tilde{s}, \\, X_t = s ) \\\\\n","&= \\sum_{s \\in \\mathcal{S}} \\mathbb{P} (X_{t+1} = \\tilde{s} | X_t = s ) \\mathbb{P}(X_t = s)  \\\\\n","&= \\sum_{s \\in \\mathcal{S}} Q(s,\\tilde{s}) m_t(s),\n","\\end{align}\n","$$\n","when we assume the chain is stationary. This calculation yields the **forward equation** \n","$$\n","m_{t+1} = m_t Q , \\quad t \\in \\mathbb{N}\n","$$\n","using the convention that the pmf $m_t$ of $X_t$ is regarded as a row vector.  The forward equation is linear and explicitly solvable when the chain is stationary. We proceed inductively to find\n","$$\n","m_t = m_1 Q^t , \\quad t \\in \\mathbb{N} \\,,\n","$$\n","and one of the central questions in Markov chain theory is to understand the behavior of $m_t$ as $t\\to\\infty$. This question explains the importance of the eigenvalue spectrum of $Q$ to Markov chain theory: diagonalizing $Q$ is an efficient way to compute $Q^t$ for $t$ large, and the spectrum of $Q$ largely governs the long term behavior of the system. \n"]},{"cell_type":"markdown","metadata":{"id":"dI_lPROxPT02"},"source":["## ... equilibrium measures, ergodicity, random walks ...\n","\n","_(Menon $\\S$ 1.2)_"]},{"cell_type":"markdown","metadata":{"id":"dQbSgDbKPb2G"},"source":["___\n","\n","### **Definition.** ( equilibrium / stationary distribution )\n","\n","$\\quad$ Let $Q$ be the transition matrix of a stationary Markov chain $X = (X_t)$ with state space $\\mathcal{S}$, and let $m$ be a probability measure on $\\mathcal{S}$, whose pmf we also denote $m$. The measure $m$ is an **equilibrium distribution** or equivalently, a **stationary distribution** for $X$ if \n","$$\n","m = m Q,\n","$$\n","and if $m$ satisfies the normalization conditions $m(s) \\geq 0$ for all $s \\in \\mathcal{S}$ and $\\sum_{s \\in \\mathcal{S}} m(s) =1$.\n","\n","___"]},{"cell_type":"markdown","source":["$\\quad$ The above definition is of central importance to our simulation goals. The reason is that, for Markov chains with a unique stationary distribution $m$, the measures $m_t \\equiv \\mathbb{P}(X_t = \\cdot )$ converge to $m$, in an appropriate sense, as $t \\to \\infty$. When we simulate a Markov chain, we are sampling from $m_t$, and we want to run the simulation long enough (taking $t$ large enough) so that $m_t \\approx m$. This is how a Markov chain Monte Carlo (MCMC) simulation can be used to approximate sampling from $m$. In practice, instead of starting with some Markov chain dynamics, we will always start with a measure $m$ on $\\mathcal{S}$, which we would like to sample from.\n","\n","$\\quad$ The MCMC method then requires _finding_ a stationary, ergodic Markov chain $X$ with state space $\\mathcal{S}$, whose stationary measure coincides with $m$. This is how $X$ is used to simulate sampling from $m$. "],"metadata":{"id":"9440Kf4m94Yc"}},{"cell_type":"markdown","metadata":{"id":"-S2kCOntPd1u"},"source":["$\\quad$ In Krauth's pebble games, there was some discussion of initial conditions of a Markov chain: in the second pebble game, the person playing always had to start at a prescribed corner of the square. These are _deterministic_ initial conditions. In practice, we agreed to assume simulations will start deterministically, or from where a previous simulation left off. In theory, it is useful to have a mathematical object associated to the __initial conditions__ of a Markov chain: a probability measure on $\\mathcal{S}$, denoted $m_0$. When initial conditions are deterministic, and start at some distinguished state $s_* \\in \\mathcal{S}$, the probability measure $m_0$ corresponds to the $\\delta$-mass $\\delta_{s_*}$. When $m_0$ is general, it models starting from where some simulation left off, whose information we have no (or only partial) access to. \n","\n","$\\quad$ The first equation in the above definition can be understood as describing the evolution of a Markov chain $X$ whose initial conditions are the stationary distribution $m$ associated to $X$. In this case $m_t = m$ for all $t$ if $m_1 = m$. That is, while the values of $X_t$ change with time (it's a stochastic process) its probability mass function does not. Thus the equilibrium distribution of a Markov chain captures the intuitive idea of a dynamic equilibrium . "]},{"cell_type":"markdown","metadata":{"id":"hjZr6ElwPjBy"},"source":["$\\quad$ Considering the task of computing $m$ given $Q$: observe that $m$ is a left eigenvector of $Q$ with eigenvalue $1$. Further, $1$ must always be an eigenvalue of $Q$ since it corresponds to the right eigenvector $(1, \\dots, 1)^T$. Thus, if we know how to solve for eigenvectors, we can determine $m$. Menon: _\"What is interesting in practice is the converse: for large transition matrices, say when $\\# \\mathcal{S}$ is $10^6 \\times 10^6$, it is more efficient to use Markov chains to approximate $m$, than to use naive linear algebra This observation plays an important role in web crawlers and search engines.\"_"]},{"cell_type":"markdown","metadata":{"id":"lSsCHkdTPlmQ"},"source":["$\\quad$ Returning to the main question of the Ising simulation: knowing how long to run the MCMC simulation for, or knowing how quickly $m_t$ converges to $m$ is related to how fast the Markov chain _mixes_. We now explore two examples of Markov chains whose state space is the symmetric group $\\mathfrak{S}_{52}$. Each group element can be thought of as a possible shuffling of a standard deck of playing cards. The group identity of $\\mathfrak{S}_{52}$ corresponds to some distinguished initial ordering. For each of the two shuffling methods described, the main question can be rephrased as: _\"how many shuffles does it take to obtain a truly random (uniform) distribution if one begins with a perfectly ordered deck of cards?\"_  The following term will be used. "]},{"cell_type":"markdown","metadata":{"id":"XUm5ol1yPqDj"},"source":["___\n","\n","### **Definition.** ( ergodic Markov chain )\n","\n","$\\quad$ A stationary Markov chain $X$ with state space $\\mathcal{S}$ and transition matrix $Q$ is **ergodic** if it has a unique equilibrium distribution. A sufficient condition for ergodicity of $X$ is if for every pair $s, \\tilde{s} \\in \\mathcal{S}$ there is a positive integer $j(s,\\tilde{s})$ such that $Q^{j(s,\\tilde{s})} >0 $.\n","___"]},{"cell_type":"markdown","metadata":{"id":"hY9uwPisPqxm"},"source":["$\\quad$ Many other conditions are known that ensure ergodicity. For example, if all terms of $Q$ are strictly positive, it is possible for any state to get to any other state, and this ensures that the Markov chain is ergodic. This assumption is too restrictive in practice, since we are mainly intersted in Markov chains making 'local' moves such as random walks in a large state space. This behavior is captured by the sufficient condition given in the above definition. Intuitively, it means that every state can visit every other state given enough time. We assume this property always holds."]},{"cell_type":"markdown","metadata":{"id":"rwhVu157Pt4N"},"source":["$\\quad$ Physicsts define the term ergodic ( describing Markov chain $(X_t)_{t\\in \\mathbb{N}}$ ) to mean the following.\n","\n","$$\n","\\lim_{T \\to \\infty} \\sum_{t =1}^T \\mathcal{O}(X_t) = \\mathbb{E}_m\\mathcal{O},\n","$$\n","\n","where $m = m Q$ and $\\mathcal{O}: \\mathcal{S} \\to \\mathbb{R}$ is arbitrary. "]},{"cell_type":"markdown","metadata":{"id":"Sxt7fUa5P48j"},"source":["$\\quad$ This formulation has the interpretation that a Markov chain is ergodic when the left-hand side, the _time-average_, is equal to the _spatial-average_ (with respect to the equilibrium measure), which is the right hand side. For Markov chains ergodic in the first sense, the second notion of ergodicity follows from an _ergodic theorem_. Menon notes the following bottlenecks and simplifications:\n","\n","* $\\mathcal{S}$ may be very large. For example (in shuffling dynamics we soon describe), $\\mathcal{S}$ may be the permutation group. \n","\n","* $Q$ may be a sparse matrix.\n","\n","* As discussed previously, we often don't need $m$ explicitly, we instead want to evaluate $\\mathbb{E}_m \\mathcal{O}$, where $\\mathcal{O}: \\mathcal{S} \\to \\mathbb{R}$ is some observable. "]},{"cell_type":"markdown","metadata":{"id":"2yBpE3mWP9fb"},"source":["___\n","\n","### **Example.** ( simple random walk ) \n","\n","$\\quad$ Let $G = (\\textrm{V},\\textrm{E})$ be a finite graph with vertex set $\\textrm{V}$ and edge set $\\textrm{E}$. Given $x,y \\in \\textrm{V}$, write $x \\sim y$ if the edge $\\{ x, y \\}$ lies in $\\textrm{E}$. Let $\\textrm{deg}(x) \\triangleq \\sum_{x \\sim y } 1$ be the degree of vertex $x$. A **simple random walk** on $G$ is a Markov chain with state space $\\textrm{V}$ and transition matrix\n","$$\n","Q(x,y)\n","    =\n","        \\begin{cases}\n","        \\frac{1}{\\textrm{deg}(x)}, & \\quad \\text{ if } x \\sim y \\\\\n","        0 & \\quad \\text{ otherwise. }\n","        \\end{cases}\n","$$\n","We leave the initial conditions ambiguous in this definition. \n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"mlgWt7oYQCRU"},"source":["___\n","### **Example.** ( random walks in the permutation group )\n","\n","$\\quad$ Let $\\mathfrak{S}_n$ denote the permutation group on $n$ symbols. To be clear, let\n","\n","$$\n","[n] := \\{ \\, 1, \\dots, n \\, \\},\n","$$\n","\n","and recall that a permutation on $[n]$ is a bijection from $[n]$ to itself. The group structure on the collection of such functions, denoted $\\mathfrak{S}_n$, is given by function composition. Below we describe two distinct rules for Markovian dynamics on $\\mathfrak{S}_n$. Each set of dynamics can be thought of as arising from forgetting the implicit Cayley graph structure on $\\mathfrak{S}_n$: this is the complete graph on $n!$ vertices. Below, we consider adjacency structures on $\\mathfrak{S}_n$ distinct from this and from one another. \n","\n","$\\quad$ Through the adjacency matrix from the previous example, these new graph structures (writing $\\sim'$ and $\\sim''$ in this example) on $\\mathfrak{S}_n$ induce simple random walk dynamics, and so each describes a concrete Markov chain on $\\mathfrak{S}_n$, modulo initial conditions.\n","\n","1. We declare permutations $\\sigma$ and $\\tau$ adjacent and write $\\sigma \\sim' \\tau$ if $\\sigma$ and $\\tau$ are related by a transposition of adjacent elements. The $'$-degree of each permutation in this graph is $n-1$.\n","\n","2. We declare permutations $\\sigma$ and $\\tau$ adjacent and write $\\sigma \\sim'' \\tau$ if $\\sigma$ and $\\tau$ are related by a transposition, not necessarily of adjacent elements. The $''$-degree of each permutation in this graph is ${n \\choose 2}$. \n","\n","These are examples of _distinct_ ergodic random walks on the same state space, namely $\\mathfrak{S}_n$, both of which have the uniform measure, let us still denote this $\\mu$, as their equilibrium measure. The simulation 'purpose' of both random walks is thus to sample from this uniform measure, and the distinctness of the two walks raises the question of whether one method does the sampling better. \n","___"]},{"cell_type":"markdown","metadata":{"id":"vTnwbHHbQEXz"},"source":["## ... Shannon's model of text ...\n","\n","_( Menon $\\S$ 1.3 )_"]},{"cell_type":"markdown","metadata":{"id":"Fm0wo2s9QGhC"},"source":["$\\quad$ The main idea in Shannon's model (of text / written language) is that text is text is a stationary, ergodic stochastic process $(X_t)$ taking values in an alphabet\n","$$\n","\\mathcal{A} \\triangleq \\{\\, a, \\,b,\\, c,\\, \\dots\\,,\\, z,\\, \\_ \\,\\}\n","$$\n","This model is easily augmented to include punctuation and case, but we ignore these concepts in the first approximation. Note that we are not assuming that this process is a Markov chain. As discussed, the alphabet is the state space $\\mathcal{S}$ from previous sections, but we use this terminology and notation to emphasize the language modeling aspect of the discussion. Note also that stationarity is distinct from the Markov property, in that neither property implies the other."]},{"cell_type":"markdown","metadata":{"id":"GFD_0t1eQJ2Z"},"source":["\n","___\n","\n","### **Definition.** ( stationary stochastic process )\n","\n","$\\quad$ A discrete time stochastic process $X \\equiv (X_t)$ is **stationary** if all abitrarily large, but finite, marginal probabilities are invariant under arbitrary shifts in time. More precisely,\n","\n","$$\n","\\mathbb{P} (\\, X_1 = s_1, \\dots, X_n = s_n\\,) \n","    = \n","        \\mathbb{P} ( \\,X_{1+k} = s_1, \\dots, X_{n+k} = s_n \\,)\n","$$\n","\n","for all positive integers $k$ and $n$.\n","___"]},{"cell_type":"markdown","metadata":{"id":"OCZI7p6-QOub"},"source":["$\\quad$ Intuitively, this means that if we were to observe strings of length $n$, their statistics are not changed by shifting them forward in time by an integer $k$. For such processes, the notion of ergodicity again means that _time averages_ are equal to _spatial averages_. From the perspective of observables $\\mathcal{O} : \\mathcal{S}^n \\to \\mathbb{R}$, which through the above definition requires have an arbitrarily large 'memory' $n$ of the process, one has\n","$$\n","\\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t =0 }^{T-1} \\mathcal{O}(X_{t+1}, \\dots, X_{t+n}) =\n","\\mathbb{E}  \\mathcal{O}( X_1, \\dots, X_n) \n","$$"]},{"cell_type":"markdown","metadata":{"id":"VZqXbh6xQPTj"},"source":["$\\quad$ Menon: _\"The above assumptions are introduced to conform to the idea that written text contains many heirarchical structures: letters are joined by phonetic rules to form words, words are linked by the rules of grammar into sentences, sentences are organized into paragraphs, and so on. Strange as it may seem at first sight, these rules can be effectively modeled as a stochastic process. The use of stationary ergodic processes provides us with a definition flexible enough to include heirarchical structures (though these may be complex to write down), and simple enough to computationally test.\"_"]},{"cell_type":"markdown","source":["$\\quad$ As discussed, written text is modeled by a stationary stochastic process $Y$, whose law is denoted\n","$$\n","\\mathbb{P}_{\\textrm{true}} .\n","$$\n","The subscript '$\\textrm{true}$' indicates that we view $Y$ not as an approximation to language, but as a stream of 'correctly written' English text, generated in real-time. This idealization is vague, but the main point is to identify the written language $\\mathcal{L}$, in this case English, with a concrete mathematical object. In this case, the object is the stationary, stochastic process $Y$. The law $\\mathbb{P}_{\\textrm{true}}$ of this process is the analogue of $\\mu$ in two previous contexts: the uniform measure on $\\mathcal{S} = \\textsf{square}$, and the uniform measure on $\\mathcal{S} = \\mathfrak{S}_{52}$. In either previous case, the purpose of the Markov chains involved was to approximately sample from $\\mu$. Below, we discuss a family of Markov chains for approximately sampling from $\\mu = \\mathbb{P}_{\\textrm{true}}$. The way in which this sampling happens seems distinct from before, and worth remarking on after introducing the approximating Markov chains. "],"metadata":{"id":"Dx25Pxq-xFDM"}},{"cell_type":"markdown","metadata":{"id":"iNJJR9w7QU_T"},"source":["$\\quad$ In practice, the probabilities relative to $\\mathbb{P}_{\\textrm{true}}$ below are approximated by mining a large corpus (e.g. the works of Shakespeare) to approximate $\\mathbb{P}_{\\textrm{true}}(a_1, \\dots, a_n)$ for an arbitrary string $(a_1, \\dots, a_n) \\in \\mathcal{A}^n$."]},{"cell_type":"markdown","metadata":{"id":"BTVeKOFhQbTv"},"source":["___\n","\n","### **Example.** ( digram model )\n","\n","_( or 2-gram model )_\n","\n","$\\quad$ This is a Markov chain with state space $\\mathcal{A} = \\{ \\, a,\\, b, \\, c, \\, \\dots \\, , \\,z, \\, \\_ \\}$. \n","\n","$$\n","Q^{(2)}(x,y) = \\mathbb{P}_{\\textrm{true}} (\\,X_2 = y | X_1 = x\\,),\n","$$\n","\n","which is to say, we form the character-to-character transition probabilities of a Markov process using the marginals of the full stationary process being approximated. \n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"3q3C3mr0R2W5"},"source":["$\\quad$ We can get better approximations by allowing the Markov chain above to retain more of its history. This means defining a Markov process on $\\mathcal{A}^n$ for some $ n \\geq 2$. When $n=2$, we obtain the so-called trigram model. "]},{"cell_type":"markdown","metadata":{"id":"kw_auvEwQeqi"},"source":["\n","\n","___\n","\n","### **Example.** ( trigram model ) \n","\n","_( or 3-gram model )_\n","\n","$\\quad$ This is a Markov chain with state space $\\mathcal{A}^2$, which we style as\n","\n","$$\n","\\{ aa, \\,ab,\\,, \\dots, az, \\dots, \\_a, \\, \\dots, \\, \\_\\,\\_ \\} \n","$$\n","\n","Let $x = a_1a_2$ and $y = b_1b_2$. In order to form a text of three letters from $x$ and $y$, we must ensure that $x$ and $y$ agree on their overlap, that is when $a_2 = b_1$, so that $x$ and $y$ combine to give us the string $a_1a_2b_2$. With this restriction on $x$ and $y$, we obtain the associated transition matrix\n","\n","$$\n","Q^{(3)} (x,y) \n","= \n","\\frac{\n","\\mathbb{P}_{\\textrm{true}} ( a_1 a_2 b_2 )\n","}\n","{\n","\\mathbb{P}_{\\textrm{true}} (a_1 a_2) \n","}\n","$$\n","\n","and $Q(x,y) =0$ when $a_2 \\neq b_1$. \n","\n","> Because of the compatability requirement of $a_2 = b_1$, the above Markov chain can still be viewed as a stochastic process taking values in $\\mathcal{A}$, it is just no longer a Markov chain from this perspective. This is the sense, however, in which this model is approximating $\\mathbb{P}_{\\textrm{true}}$.  \n","\n","___"]},{"cell_type":"markdown","metadata":{"id":"0AhvYvFiQfVM"},"source":["\n","\n","___\n","\n","### **Example.** ( $(n+1$)-gram model )\n","\n","$\\quad$ A Markov chain with state space $\\mathcal{A}^{n}$. A state $x$, say $x = (a_1, \\dots, a_n)$ can be followed by a string $y = (b_1, \\dots, b_n)$ only if $a_2 = b_1, a_3 = b_2, \\dots, a_n = b_{n-1}$. This induces transition matrix\n","\n","$$\n","Q^{(n+1)} (x,y) \n","= \n","\\frac{\n","\\mathbb{P}_{\\textrm{true}} ( a_1 a_2 \\dots a_n b_n )\n","}\n","{\n","\\mathbb{P}_{\\textrm{true}} (a_1 a_2 \\dots a_n) \n","}\n","$$\n","\n","as above.\n","\n","> Following the previous remark, for similar compatibility reasons, this Markov chain can also be interpreted as a stationary stochastic process taking values in $\\mathcal{A}$, and as before, this is the sense in which the above process approximates $\\mathbb{P}_{\\textrm{true}}$.\n","___"]},{"cell_type":"markdown","metadata":{"id":"2MuSJ6_8Qj6z"},"source":["$\\quad$ Menon: _\"Shannon proved that as $n \\to \\infty$, the law of text generated by the above Markov approximations converges to the law of the true language. Note however that a good proof may not correspond to a good algorithm. For example, as $n$ increases, the size of the state space ( of the approximating Markov process ) grows exponentially. Thus the dimensions of the transition matrix are $\\#\\mathcal{A}^{2n}$, and worse yet, the size of the training data ( to determine $Q^{(n+1)}$ by mining ) also expands exponentially.\"_"]},{"cell_type":"markdown","metadata":{"id":"3t51rLRWQnE8"},"source":["$\\quad$ _\"This follows the rules of everyday language. Once $n$ gets large enough, say $5$ or $6$ in practice, it is much more efficient to make our fundamental unit words, rather than letters, since the number of true words of length $5$ is much lower than the $26^5$ combinations that are possible. This reflects the true nature of the space $\\_$ character. In effect, we are still using the digram model, though we have switched to a new 'alphabet' whose fundamental units are words.\"_"]},{"cell_type":"markdown","source":["## ... information theory ..."],"metadata":{"id":"XMdaQwHF3BCx"}},{"cell_type":"markdown","source":["_references_\n","\n","* Ivan Matic, _Information Theory. Sanov's Theorem_ (notes from July 21, 2018)"],"metadata":{"id":"Cs_rrvxo58L6"}},{"cell_type":"markdown","source":["$\\quad$ Before going further, let us return to the object $\\mathbb{P}_{\\textrm{true}}$, playing the role of $\\mu$ in previous examples. When recalling previous examples, we identifid $\\mu$ as the uniform measure on $\\mathcal{S}$, in either case. To be formal, $\\mu = \\mathbb{P}_{\\textsf{true}}$ is a probability measure on $\\mathcal{S}^\\infty \\equiv \\mathcal{A}^\\infty$,\n","$$\n","\\mathcal{A}^\\infty := \\{ a \\equiv (a_j)_{j=1}^\\infty : a_j \\in \\mathcal{A} \\textrm{ for all } j \\}\\,,\n","$$\n","the space of infinite sequences of elements of $\\mathcal{A}$. Any discrete-time $\\mathcal{A}$-valued stochastic process, given some initial conditions which we ignore, induces a probability measure on $\\mathcal{A}^\\infty$. This latter space has the interpretation of a 'path-space', as it is meant to contain all possible trajectories of the process. "],"metadata":{"id":"p5rjLgel2mUO"}},{"cell_type":"markdown","source":["$\\quad$ The digram model naturally gives rise to a Markov chain with state space $\\mathcal{A}$. Let us say that it induces measure $\\mathbb{P}_{(2)}$ on $\\mathcal{A}^\\infty$. The trigram model is defined as a Markov chain with state space $\\mathcal{A}^2$, but following the above remark, can be interpreted as a stationary stochastic process taking values in $\\mathcal{A}$, inducing a measure $\\mathbb{P}_{(3)}$ on the path-space $\\mathcal{A}^\\infty$. In general, the $(n+1)$-gram model induces measure $\\mathbb{P}_{(n+1)}$ on $\\mathcal{A}^\\infty$. My interpretation of what Menon says at the beginning of the most recent quote, is _\"Shannon proved that...\"_ $\\mathbb{P}_{(n+1)}$ converges to $\\mathbb{P}_{\\textrm{true}}$, in an appropriate sense. \n","\n","> The approximation of $\\mathbb{P}_{\\textrm{true}}$ by $\\mathbb{P}_{(n+1)}$ does _not at all_ seeem to be an approximation of the same nature was is described in Krauth. By this I mean that $\\mathcal{A}$-valued Markov chains do not play the same role in each case. In earlier cases, they served as a way to approximately sample from a probability measure on $\\mathcal{A}$. In the present context, these objects induce a probability measure on the _path-space_ $\\mathcal{A}^\\infty$, towards approximating $\\mathbb{P}_{\\textrm{true}}$, the probability measure on $\\mathcal{A}^\\infty$ encoding the '$\\textrm{true}$' language.  "],"metadata":{"id":"elemrLyvIpAm"}},{"cell_type":"markdown","source":["$\\quad$ I want to take a detour through some notes of [Ivan Matic](https://mfe.baruch.cuny.edu/maticivan/), because I think they add to the above discussion, as well as to Menon's upcoming discussion of entropy. Menon: _\"the entropy of a random variable describes its uncertainty. In particular, the notion of an optimal code tells us that entropy describes the optimal search procedure to determine the value of $X$ through a series of yes / no questions.\"_ Matic's notes touch on this optimality, from the perspective of data compression. "],"metadata":{"id":"R8c-1spJ56Jf"}},{"cell_type":"markdown","source":["$\\quad$ To integrate Matic's perspective, he starts by imagining _\"...we want to store some big quantity of data, such as a picture, a text document, or a book. We start with a collection of uncompressed data, and we identify the smallest building blocks of each datum as symbols. The set of all symbols is called the alphabet.\"_ Matic also denotes this alphabet as $\\mathcal{A}$. \n","\n","> It feels worth remarking that languages like Chinese are pictoral, and that Chinese characters can be decomposed into 'brushstrokes'. A space character `_` perhaps corresponds to moving to the next character. I think this lends itself to the notion that pictoral data can be decomposed into symbols. For images in general, I think 'contours' become the analogue of brushstrokes. Perhaps an efficient way to use a space character `_` is as denoting the start of a new transparent canvas layer, sitting over the previously drawn layers (copying the functionality of applications like photoshop). My impression of $3$-dim spatial data (e.g. virtual environments for games) is that environment information is efficiently stored in surfaces, the higher-dimensional analogue (by one dimension) of contours. "],"metadata":{"id":"0F_C2IEb-tjd"}},{"cell_type":"markdown","source":["> The above decomposition reminds me of something I heard about the clustering algorithm [t-SNE](https://towardsdatascience.com/t-sne-clearly-explained-d84c537f53a): when applid to MNIST, the number of clusters the algorithm found was more than the number of categories (labeled by digits $0 - 9$). Some digits corresponded to multiple clusters _because_ of there being distinct ways to write this digit, with or without a horizontal slash in the case of $7$, for example. In a sense, the clustering worked too well. "],"metadata":{"id":"BGTZsfyuM0SO"}},{"cell_type":"markdown","source":["$\\quad$ A raw, uncompressed datum will be called a __signal__. The signal is sampled from the measure $\\mathbb{P}_{\\mathcal{L}}$ on $\\mathcal{A}^\\infty$ corresponding to abstract language $\\mathcal{L}$ with alphabet $\\mathcal{A}$. This measure corresponds to a stochastic process $(X_t)_{t \\geq 1}$, which we suppose is stationary. Because of the 'optimal encoding' perspective, it is important to be able to quantify the relationship between the lengths of a signal and its encoding. In particular, we assume some upper bound $M$ on the signal length to begin with, modeling signals as elements of $\\mathcal{A}^M$ for some $M \\in \\mathbb{N}$. When referring to $\\mathbb{P}_{\\mathcal{L}}$ in this context, it is understood as the projection of this measure onto $\\mathcal{A}^M$, corresponding to the 'truncated' process $(X_t)_{t = 1}^{M}$. Elements of $\\mathcal{A}^M$ will also be called signals (Matic calls them _messages_). "],"metadata":{"id":"S2IUhIoF_siL"}},{"cell_type":"markdown","source":["$\\quad$ We begin in an even simpler setting than the above language models. Here, we suppose $\\mathbb{P}_{\\mathcal{L}}$ corresponds to an i.i.d. sequence of random variables $(X_t)_{t = 1}^M$, where each $X_t$ has the same distribution as some 'model' random variable $X$, corresponding to probability measure $m$ on $\\mathcal{A}$. Because $\\mathcal{A}$ is finite, so is the space of signals $\\mathcal{A}^M$. A lossless data compression is an injective function on this signal space. "],"metadata":{"id":"jVaWDQjuJIog"}},{"cell_type":"markdown","source":["___\n","### __Definition.__ ( lossless data compression )\n","\n","A __lossless data compression__ is an injective function\n","$$\n","f : \\mathcal{A}^M \\to \\bigcup_{j=1}^N \\{ 0 , 1 \\} ^j\n","$$\n","for some $N \\in \\mathbb{N}$. \n","___"],"metadata":{"id":"fLWVherAKqks"}},{"cell_type":"markdown","source":["$\\quad$ Given some lossless data compression $f$, and for each signal $s \\equiv (s_1, \\dots, s_M) \\in \\mathcal{A}^M$, we let $\\textsf{length}_f(s)$ denote the length of the signal encoding $f(s)$. Letting $\\mathbb{E}$ denote expectation with respect to the 'model' random variable $X$, desirable propereties of the compression $f$ make\n","$$\n","\\mathbb{E} \\, \\textsf{length}_f(X) \n","$$\n","as small as possible. Heuristically, this means __(1)__ identifying the signals in $\\mathcal{A}^M$ which are _typical_ and _atypical_, according to the sampling $(X_t)_{t=1}^M$, and then __(2)__ using shorter binary sequences to encode typical signals, leaving longer binary sequences for atypical signals. This means the expectation is placing as little probablistic mass as possible on the longest encodings. "],"metadata":{"id":"ZebmNnuHLxQ7"}},{"cell_type":"markdown","source":["$\\quad$ Let us use $m$ to denote the probability measure on $\\mathcal{A}$ induced by the random variable $X$, our template for the i.i.d. sequence. Enumerating $\\mathcal{A}$ as $\\{ a_1, \\dots, a_n\\}$, let us write $m_i$ for the number $m( \\{ a_i \\})$. We identify $m$ with the vector $(m_1, \\dots, m_n)$, which is the probability mass function of $X$. It is $m$ that determines how 'typical' a given sequence is."],"metadata":{"id":"t6KJM4XH6Hkk"}},{"cell_type":"markdown","source":["$\\quad$ Let $s \\equiv (s_1, \\dots, s_M) \\in \\mathcal{A}^M$. Write $\\underline{X}$ to denote the random vector $(X_t)_{t=1}^M$. The mutual independence of the $X_t$ implies\n","$$\n","\\begin{align}\n","\\mathbb{P}( \\, \\underline{X} = s ) &= \\mathbb{P} (X_1 = s_1) \\dots \\mathbb{P}(X_M = s_M) \\\\\n","&= 2^{ \\sum_{t=1}^M \\log_2\\mathbb{P}(X_t = s_t)}\\\\\n","&= 2^{ \\sum_{i=1}^n n_i \\log_2 m_i} \\,,\n","\\end{align}\n","$$\n","where, in going from the second to third line, we have reindexed the sum over the alphabet, using that $m_i \\equiv \\mathbb{P}(X = a_i)$, and defining\n","$$\n","n_i := \\# \\{ \\, t \\in \\{ 1, \\dots, M \\} : X_t = a_i \\, \\}.\n","$$\n","Thus, $n_i$ is the number of times symbol $a_i$ appears in the random signal $\\underline{X}$. A typical message is one  in which the frequency of symbol $a_i$ is close to the corresponding probability $m_i$. Thus, for typical messages, we have $n_i \\approx m_i M $, and the probability of such a message is approximately\n","$$\n","2^{ M \\sum_{i=1}^n m_i \\log m_i }\n","$$"],"metadata":{"id":"qRYQhBpA7yj4"}},{"cell_type":"markdown","source":["___\n","\n","### __Definition.__ ( entropy )\n","\n","$\\quad$ Let $m$ be a probability measure on finite state space $\\mathcal{A}$ with $\\# \\mathcal{A} = n$, corresponding to probability mass function $(m_1, \\dots, m_n)$. The __entropy__ of $m$ is \n","$$\n","\\textsf{S}(m) := - \\sum_{i=1}^n m_i \\log_2 m_i\n","$$\n","\n","___"],"metadata":{"id":"jZBqWwRdBBaH"}},{"cell_type":"markdown","source":["$\\quad$ Note that $\\textsf{S}(m) \\geq 0$, and is $=0$ if and only if $m$ corresponds to a deterministic object, namely a $\\delta$-mass at some symbol in the alphabet. In the above setting, we are imagining the length $M$ of the signal is much larger than the size $n$ of the alphabet $\\mathcal{A}$. This is so that the law of large numbers has a chance to kick in for each symbol. In this setting, typical messages appear with probability on the order of $2^{-M \\textsf{S}(m)}$. When $m$ is non-trivial, the positivity of $\\textsf{S}(m)$ implies that typical signals are _exponentially rare_, in the signal length $M$. "],"metadata":{"id":"rob5a2xzCisV"}},{"cell_type":"markdown","source":["$\\quad$ A consquence of the intrinsic high-dimensionality of $\\underline{X}$ -- which corresponds to a probability measure on $\\mathbb{R}^M$, with $M$ large -- is that signals with symbol frequencies _close_ to the corresponding $m_i$ are very common. The next definition formalizes this notion of closeness. "],"metadata":{"id":"pLqgl8COGLNx"}},{"cell_type":"markdown","source":["___\n","\n","### __Definition.__ ( $\\epsilon$-typical signal )\n","\n","For fixed $\\epsilon > 0$, we say $s \\in \\mathcal{A}^M$ is __$\\epsilon$-typical__ if\n","$$\n","\\mathbb{P}( \\, \\underline{X} = s ) \\in \\left( 2^{-M(\\textsf{S}(m) +\\epsilon) }, \\, 2^{-M(\\textsf{S}(m) -\\epsilon) } \\right) \\,,\n","$$\n","and the set of all $\\epsilon$-typical signals is denoted $\\mathcal{A}^M_\\epsilon$. \n","\n","___"],"metadata":{"id":"ZABh6jFoHmgQ"}},{"cell_type":"markdown","source":["$\\quad$ As discussed, the goal is to efficiently compress signals $\\underline{X}$ drawn from the measure $m$. The relevance of the above defininition to compression is:\n","\n","1. we first prove that $\\underline{X}$ is $\\epsilon$-typical with high probability\n","\n","2. we will see that the set of typical signals is nonetheless small: the number of all possible signals is $\\# \\mathcal{A}^M \\equiv n^M$, and there are only approximately $2^{M\\textsf{S}(m)}$ \n","\n","these facts combine to form a compression strategy: when we see a typical message, we begin its binary encoding (towards defining the lossless compression function $f$) with `0`, and we use $M \\textsf{S}(m)$ bits to compress it. If the message is atypical, we begin its binary encoding with a `1`, and are more sloppy with how many bits we use to store. The formal statement of the first point is as follows."],"metadata":{"id":"x__jPn1uJOK3"}},{"cell_type":"markdown","source":["___\n","### __Theorem.__ (Matic, Theorem 1) \n","\n","$\\quad$ For each $\\delta > 0$, there is $M_0 \\in \\mathbb{N}$ such that for every $M \\geq M_0$ there is a lossless data compression $f$ in which the average length of a message satisfies\n","\n","$$\n","\\mathbb{E} \\textsf{length}_f ( \\,\\underline{X} ) \\leq M \\textsf{S}(m) + \\delta \n","$$\n","___\n","\n","_( we defer recording the proof )_"],"metadata":{"id":"4HwISpc7MofF"}},{"cell_type":"markdown","source":["Matic: _\"The algorithm constructed (in the above proof) creates a lossless compression. It is very effective when the raw data consists of independent components. The `png` format uses a better algorithm.\"_"],"metadata":{"id":"r8sljptVON5A"}},{"cell_type":"markdown","source":["$\\quad$ To discuss this algorithm, Matic introduces what he calls _types_. These objects were previously introduced as the frequencies $n_i$ above."],"metadata":{"id":"UgT7upEsPzfs"}},{"cell_type":"markdown","source":["___\n","\n","### __Definition.__ ( the type of a signal )\n","\n","$\\quad$ Given a signal $s \\equiv (s_t)_{t=1}^M \\in \\mathcal{A}^M$, the __type__ of $s$ is the following probability measure on $\\mathcal{A}$, constructed through the symbol frequencies of $s$: for $a \\in \\mathcal{A}$, \n","$$\n","m_s( \\{ a \\} ) := \\frac{1}{M} \\sum_{t = 1}^M \\mathbf{1}\\{ s_t = a \\}\n","$$\n","\n","___"],"metadata":{"id":"xL3bIejUQmCs"}},{"cell_type":"markdown","source":["$\\quad$ As a direct consequence of the above definition, two signals $s, \\tilde{s} \\in \\mathcal{A}^M$ have the same type if and only if the coordinates of $\\tilde{s}$ form a permutation of the coordinates of $s$. Let $\\textsf{types}(\\mathcal{A},M)$ denote the collection of all types induced by signals in $\\mathcal{A}^M$. The map $\\mathcal{A}^M \\to \\textsf{types}(\\mathcal{A},M)$ is effectively a quotient map, and given a probability measure $m \\in \\textsf{types}( \\mathcal{A}, M)$, we let $\\textsf{signals}(m)$ denote the collection of signals whose type is $m$:\n","$$\n","\\textsf{signals}(m) = \\{ s \\in \\mathcal{A}^M : m_s \\equiv m \\}\n","$$"],"metadata":{"id":"OnAhHXdSBKut"}},{"cell_type":"markdown","source":["___\n","\n","### __Theorem.__ ( Matic, Theorem 2 )\n","\n","Consider the set of types, namely $\\textsf{types}( \\mathcal{A}, M)$, with $\\# \\mathcal{A} = n$. One has\n","\n","1. $\\#\\, \\textsf{types}( \\mathcal{A},M) = {M + n -1 \\choose M }$\n","\n","2. $\\# \\, \\textsf{types}( \\mathcal{A}, M) \\leq (M+1)^n$\n","\n","___"],"metadata":{"id":"hcdz4BXhIFrt"}},{"cell_type":"markdown","source":["$\\quad$ Let $\\mu$ be a general probability measure on $\\mathcal{A}$, corresponding to a random variable $Y$ and let $\\underline{Y} \\equiv (Y_t)_{t=1}^M$ be a sequence of i.i.d. random variables. Given fixed signal $s \\in \\mathcal{A}^M$, one has\n","$$\n","\\begin{align}\n","\\mathbb{P}( \\, \\underline{Y} = s ) &= \\prod_{t=1}^M \\mu( Y_t = s_t) \\\\\n","&= 2^{ \\sum_{t=1}^M \\log_2 \\mu( Y_t = s_t ) } \\\\\n","&= 2^{ \\sum_{a \\in \\mathcal{A} } \\sum_{t =1}^M \\mathbf{1} \\{ X_t = a \\} \\log_2 \\mu(Y_t = s_t)} \\\\\n","&= 2^{ \\sum_{a \\in \\mathcal{A} } \\sum_{t =1}^M \\mathbf{1} \\{ X_t = a \\} \\log_2 \\mu(Y_t = a)} \\\\ \n","&= 2^{ M \\left[ \\sum_{a \\in \\mathcal{A} } \\left( \\frac{1}{M} \\sum_{t =1}^M \\mathbf{1} \\{ X_t = a \\} \\right) \\log_2 \\mu(Y_t = a) \\right]} \\\\\n","&= 2^{ M \\left[m_s(a) \\log_2 \\mu(a) \\right] }\n","\\end{align}\n","$$\n","\n","The coefficient of $M$ in the exponent directly above can be written in terms of the relative entropy."],"metadata":{"id":"0oqQOS1AKJLT"}},{"cell_type":"markdown","source":["_next in section_\n","\n","* relative entropy or KL divergence"],"metadata":{"id":"cu-uL4NMVEEX"}},{"cell_type":"markdown","source":["___\n","\n","_next sections_\n","\n","* Gibbs measures (Menon $\\S$ 1.4)\n","\n","* derivation of the Gibbs distribution (from principles of information theory, continuing with Menon $\\S$ 1.4)\n","\n","* decoding scrambled text (from a Bayesian perspective, Menon $\\S$ 1.5)\n","\n","* sampling from a Gibbs distribution (Menon $\\S$ 1.6)\n","\n","* the Metropolis algorithm (Menon $\\S$ 1.7, also consulting parts of first chapter of Krauth on detailed balance and Metropolis, respectively $\\S$ 1.1.4 and $\\S$ 1.1.5)\n","\n","* Markov chains convergence to stationary distribution (Menon $\\S$ 1.8, 1.9)\n","\n","_( For now, omit Krauth $\\S$1.1.3 which analyzes the Buffon needle problem. In the long term, it would be nice to practice simulating either the RWs on the symmetric group discussed in a pair of examples, or the $(n+1)$-gram model.  )_"],"metadata":{"id":"0YQdZEkDVqPp"}},{"cell_type":"markdown","source":["$\\quad$ The point of building up to this last section is that it lays the theoretical groundwork for how the MCMC method can be used to sample from a Gibbs distribution, which is an abstraction of the simulations run in the motivating [blog post](http://bit-player.org/2021/three-months-in-monte-carlo). What's next is to describe the Hamiltonian for the Ising model, from which the Gibbs measures and dynamics are automatically defined. "],"metadata":{"id":"i-fduhtxcc1i"}},{"cell_type":"code","source":[""],"metadata":{"id":"UgFDoDdIfUgo"},"execution_count":null,"outputs":[]}]}