{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://the-ninth-wave.github.io/stat-mech/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Lecture 4 ... The Ising model",
            "content": "We use the notation $V Subset mathbb{Z}^d$ for a finite subset of the vertex set of $ mathbb{Z}^d$. Consider a physical system with state space $ Sigma_V triangleq { pm 1 }^V$. The Ising model on $V$, with external (magnetic) field $h in mathbb{R}$ and free boundary conditions (indicated by the &quot;$0$&quot; in the subscript) is characterized by the following Hamiltonian . $$ H_{V, ,h, , 0}^{ small textrm{(Ising)}}(s) triangleq - left( sum_{ substack{x, ,y , in , V x , sim , y}} s_x s_y + h sum_{x , in , V} s_x right), $$where $s in { pm 1 }^V$ is called a spin configuration. The left-most sum is indexed by unordered pairs $x,y$ of adjacent vertices in $V$, inheriting its graph structure from the usual one on the ambient graph $ mathbb{Z}^d$. . Given $V$ as above, we define a configuration of boundary conditions as an element $ omega$ of . $$ { pm1 }^{ mathbb{Z}^d setminus V } $$The Ising model on $V$ with boundary conditions $ omega in { pm1 }^{ mathbb{Z}^d setminus V}$ is characterized by the following Hamiltonian: . $$ H_{V, ,h, , omega}^{ small textrm{(Ising)}}(s) triangleq - left( sum_{ substack{x, ,y , in , V x , sim , y}} s_x s_y + h sum_{x , in , V} s_x right) - H_ partial( omega), $$The last term $H_ partial( omega)$ enforces the boundary conditions $ omega$ as follows: . $$ H_ partial( omega) triangleq - sum_{ substack{ u , sim , v u , in , partial_{ Large circ} V, v , in , partial_{ Large bullet} V } } s_u omega_v . $$Above, $ partial_{ Large circ} V$ is the internal vertex boundary of $V$, . $$ partial_{ Large circ} V triangleq { v in V : exists w in V^{ textrm{c}} text{ such that } v sim w }, $$and $ partial_{ Large bullet} V$ is the external vertex boundary of $V$, . $$ partial_{ Large bullet} V triangleq { v in V^{ , textrm{c}} : exists w in V text{ such that } v sim w }. $$ The Ising model is a nearest-neighbor model, thus all boundary conditions which agree on $ partial_{ Large bullet} V$ induce the same Hamiltonian. For boundary conditions $ omega$ with $ omega_v equiv -1$ (respectively $ omega_v equiv +1$) for all $v in partial_{ Large bullet} V$, we denote the associated Hamiltonian by . $$ H_{V, ,h, ,-}^{ small{ textrm{(Ising)}}} quad , text{(respectively } H_{V, ,h, ,+}^{ small{ textrm{(Ising)}}} ,) $$ For inverse temperature parameter $ beta &gt;0$, define the parition function of this model by . $$ mathcal{Z}_{V, , beta, , h, , omega}^{ , small{ textrm{(Ising)}}} triangleq sum_{s , in , Sigma_V} exp left(- beta H_{V, ,h, , omega}^{ , small{ textrm{(Ising)}}}(s) right) $$ We wish to study the limiting free energy for an exhaustive sequence $V_N uparrow mathbb{Z}^d$, first by showing this limit exists. One can show for $V,W subset mathbb{Z}^d$ disjoint, we have . $$ log left( mathcal{Z}_{V cup W, , beta, , h, ,0}^{ , small{ textrm{(Ising)}}} right) geq log left( mathcal{Z}_{V, , beta, ,h, ,0}^{ , small{ textrm{(Ising)}}} right) + log left( mathcal{Z}_{W, , beta, ,h, ,0}^{ , small{ textrm{(Ising)}}} right) $$ This sub-additivity of the free energy allows one to use Fekete&#39;s lemma to deduce that if $V_N$ is an exhaustive sequence of discrete boxes, i.e. . $$ V_N =_{ small{ textrm{set}}} [-N,N]^d cap mathbb{Z}^d, $$ the limit . $$ lim_{N to infty} frac{1}{ # V_N} log mathcal{Z}_{V_N, , beta, ,h, ,0}^{ , small{ textrm{(Ising)}}} $$exists, and lies in $[0, infty]$. . We&#39;ll now compute this limit exactly in $d=1$. We use the transfer matrix method, discovered in the &#39;30s. This method works nicely when the boundary conditions of the model are periodic. For $V_N$ as just above, and in $d=1$, the _Ising Hamiltonian on $V_N$ with periodic boundary conditions_ is the following function of spin configurations $s in V_N^{ pm1}$ . $$ H_{V_N, , h , , circlearrowright}^{ small{ textrm{(Ising)}}}(s) triangleq H_{V_N, ,h, ,0}^{ small{ textrm{(Ising)}}}(s) + s_{-N} s_N $$We will specialize to this setting. Due to the periodicity in the Hamiltonian, it is nearly equivalent to specialize to the sequence of domains . $$ V_N triangleq [0,N-1] cap mathbb{Z}, $$with Hamiltonian given by . $$ H_N(s) = sum_{ i } s_i s_{i+1} + h sum_{i=1}^N s_i, $$with $s_N triangleq s_0$, which is to say that addition in the subscript is defined modulo $N$. .",
            "url": "https://the-ninth-wave.github.io/stat-mech/jupyter/2019/04/08/M450-Lec4.html",
            "relUrl": "/jupyter/2019/04/08/M450-Lec4.html",
            "date": " • Apr 8, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Lecture 3 ... Large deviations",
            "content": "Recall the definition of a large deviation principle. . . Definition 1.3.1. $ quad$ A sequence of measures $ mu_N$ on a topological space $ mathcal{X}$ satisfies a large deviation principle (LDP) with rate function $I : mathcal{X} to [0, infty]$ if for all $A subset mathcal{X}$, . $$ - inf_{x in A^ circ} I(x) leq liminf_{N to infty} frac{1}{N} log mu_N(A) leq limsup_{N to infty} frac{1}{N} log mu_N(A) leq - inf_{x in overline{A}} I(x), $$where $A^ circ$ and $ overline{A}$ denote the interior of $A$ and the closure of $A$ respectively. . . A sequence $( mu_N)$ satisfying a LDP with rate function $I$ roughly behaves as . $$ mu_N(A) approx exp left( - N inf_{x in A} I(x) right) $$A rate function $I$ is a good rate function if its sublevel sets are compact. . . Theorem 1.3.2 (Varadhan&#39;s Lemma). $ quad$ Let $ mu_N$ be a sequence of probability measures satisfying a LDP with good rate function $I$. Let $ varphi$ be a continuous function with good decay: there is $ gamma &gt;1$ with . $$ limsup_{N to infty} int exp left( , gamma N varphi(x) , right) mu_N( textrm{d} x) &lt; infty. $$Then, . $$ lim_{N to infty} frac{1}{N} log int exp left( , N varphi(x) , right) mu_N( textrm{d} x) = sup_{x in mathcal{X}} left( , varphi(x) -I(x) , right) $$ . Setting $ varphi equiv 0$ recovers the LDP. A good reference is Section 4.3 of Dembo-Zeitouni. . . Theorem 1.3.3 (Bryc&#39;s inverse). $ quad$ Consider a sequence of measures $( mu_N)$ on a topological space $ mathcal{X}$ such that for every bounded, continuous $ varphi : mathcal{X} to mathbb{R}$, we have . $$ lim_{N to infty} frac{1}{N} log int exp( N varphi(x) ) mu_N( textrm{d} x) = sup_{x in mathcal{X}} ( , varphi(x) -I(x) , ). $$Then $( mu_N)$ satisfies a LDP with rate function $I$. . . . Example 2 (Curie-Weiss). $ quad$ This is a mean-field ferromagnet. Let $ Sigma_N = { pm 1 }^N$, and define the disorderless Hamiltonian $H_N : Sigma_N to mathbb{R}$ defined by . $$ H_N^} (s) triangleq frac{1}{N} sum_{i,j} s_i s_j, $$for $s in Sigma_N$. For $ beta &gt;0$, let us introduce the associated Gibbs measure on $ Sigma_N$, it is defined by . $$ mathcal{G}_{N, beta}^{ ,{ small textrm{(cw)}}} (s) triangleq left( mathcal{Z}_{N, beta}^} right)^{-1} exp left( - beta H_N^} (s) right) $$for $s in Sigma_N$, with the usual definition of the partition function: . $$ mathcal{Z}_{N, beta}^} triangleq sum_{s in Sigma_N} exp left( - beta H_N^} (s) right) $$This is the Curie-Weiss model. . . In ferromagnets, the order parameter (object which detects phase transitions) is the magnetization. The finite-N definition of this object is the random variable . $$ m_N (s) triangleq frac{1}{N} sum_{i=1}^N s_i , $$with $$ s sim mathcal{G}_{N, beta}^ { , small{ textrm{(cw)}} } $$ . The magnetization measures the average alignment of the spins. The proof of the following theorem can be found in Bovier&#39;s book on disordered systems, it uses Varadhan&#39;s lemma and Bryc&#39;s inverse. . . Theorem 1.3.4. $ quad$ Below, let $m$ denote the magnetization $m_N(s)$, with $s sim mathcal{G}_{N, beta}^{ small{ textrm{(cw)}}}$. There is $ beta_c &gt; 0$ such that . When $ beta leq beta_c$: for all $ epsilon &gt;0$ there is $c( epsilon) &gt;0$ such that $$ mathcal{G}_{N, beta}^{ , small{ textrm{(cw)}}} left( , m in (- epsilon, epsilon) , right) geq 1- exp(-cN) $$ | When $ beta &gt; beta_c$: there is $m_* equiv m_*( beta) &gt; 0$, such that for all $ epsilon &gt;0$, there is $c( epsilon, beta) &gt;0$ with $$ mathcal{G}_{N, beta}^{ , small{ textrm{(cw)}}} left( , m in (m_* - epsilon, m_* + epsilon) cup (-m_* - epsilon, -m_* + epsilon) , right) geq 1 - exp ( -N c) $$ | . Thus, at high temperatures, namely when $ beta leq beta_c$, the magnetization localizes around $ {0 }$. At low temperatures, it localizes around the two points $ { pm m_*( beta) }$, due to spin-flip symmetry. .",
            "url": "https://the-ninth-wave.github.io/stat-mech/jupyter/2019/04/05/M450-Lec3.html",
            "relUrl": "/jupyter/2019/04/05/M450-Lec3.html",
            "date": " • Apr 5, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Lecture 2 ... Gaussians",
            "content": "Here is the Gaussian integration by parts formula: . . Lemma 1.2.1. $ quad$ Let $X$ be a centered Gaussian random variable with variance $ sigma^2$. Let $f in C^1( mathbb{R})$ with good decay. Then . $$ mathbb{E} X f(X) = sigma^2 mathbb{E} f&#39;(X) ,. $$More generally, let $X equiv (X_1, dots, X_d)$ be a centered Gaussian vector. Let $F : mathbb{R}^d to mathbb{R}$ be a $C^1$ function with good decay. Then, . $$ mathbb{E} X_1 F(X) = sum_{i=1}^d mathbb{E} X_1 X_i partial_i F(X) $$ . The proof is an exercise. We state a version of integration by parts specific to Gibbs measures induced by Gaussian energy landscapes. . Let $ Sigma$ denote a state space, equipped with a deterministic reference measure $ mu$. . Let $H$ be a centered Gaussian process on $( Sigma, mu)$, and let $ mathcal{G}$ denote the associated Gibbs measure ($ beta =_{ text{set}} 1$), given by . $$ mathcal{G}(s) triangleq frac{1}{ mathcal{Z}} exp left( H(s) right) , mu ( textrm{d} s) $$ Let us use $ mathbf{ prec} cdot mathbf{ succ}$ to denote expectation with respect to the product measure $ mathcal{G}^{ , otimes infty}$: for all $k geq 1$ and functions $f : Sigma^k to mathbb{R}$, . $$ mathbf{ prec} , f , mathbf{ succ} equiv mathbf{ prec} f( s^{(1)}, dots,s^{(k)} ) mathbf{ succ} triangleq int_{ Sigma^k} f( s^{(1)}, dots, s^{(k)} ) mathcal{G}( textrm{d} s^{(1)} ) dots mathcal{G}( textrm{d} s^{(k)} ) $$ Above, the sampling from the product measure $ mathcal{G}$ is done with the same realization of the disorder $H$ in every coordinate. . This is to say, $s^{(1)}, dots, s^{(k)}$ are independent samples from the same random measure $ mathcal{G}$, which we refer to as replicas. . Under this notation, the Gibbs-expected value of the observable $R_{1 text{-}2}$ is denoted . $$ mathbf{ prec} R_{1 text{-}2} mathbf{ succ} $$for instance. Note that given any deterministic observable $f$, the object $ mathbf{ prec} f mathbf{ succ}$ is a random variable; it is the average of a deterministic function with respect to a random probability measure. . . Lemma 1.2.2 (Integration by parts with disordered Gibbs measures). $ quad$ Let $( Sigma, H, mathcal{G})$ be as above, and let $X : Sigma to mathbf{R}$ be a Gaussian process. For $s,t in Sigma$, define $$ C(s,t) triangleq mathbb{E} X(s) H(t) . $$ Then, . $$ mathbb{E} mathbf{ prec} X(s) mathbf{ succ} = mathbb{E} mathbf{ prec} C(s^{(1)}, s^{(1)}) - C(s^{(1)}, s^{(2)}) mathbf{ succ}$$ . To be clear, for fixed $s,t in Sigma$, $C(s,t)$ is a constant. We use this to form a deterministic function $f$ of two replicas, specifically . $$ f ( s^{(1)}, s^{(2)} ) equiv C(s^{(1)}, s^{(1)}) - C(s^{(1)}, s^{(2)}) $$The right-hand side of the display of the above lemma is just . $$ mathbb{E} mathbf{ prec} f mathbf{ succ} $$for this specific choice of $f$. .",
            "url": "https://the-ninth-wave.github.io/stat-mech/jupyter/2019/04/03/M450-Lec2.html",
            "relUrl": "/jupyter/2019/04/03/M450-Lec2.html",
            "date": " • Apr 3, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Lecture 1 ... Introduction",
            "content": ". Example (Mixed $p$-spin models). $ quad$ Let $ Sigma_N$ denote either the hypercube $ { pm 1 }^N$ or the sphere . $$ mathbb{S}_N triangleq { x in mathbb{R}^N : | x |^2 = N }, $$which contains the hypercube. Let $( beta_p)_{p geq 1}$ be a sequence of non-negative numbers, and we suppose . $$ sum_{p geq 1} beta_p^2 &lt; infty . $$Given such a sequence, we define the model function $ xi : mathbb{R} to mathbb{R}$ via . $$ xi(x) triangleq sum_{p geq 1} beta_p^2 x^p $$as well as the associated Hamiltonian $H_N : mathbb{S}_N to mathbb{R}$, where for $s in mathbb{S}$, . $$ H_N(s) := sum_{p geq 1} beta_p N^{- (p-1)/2} sum_{ i_1, dots, i_p } g_{i_1, dots, i_p} s_{i_1} cdot dots cdot s_{i_p}, $$where the $g_{i_1, dots, i_p}$ are i.i.d. standard normal random variables. The model function and the associated Hamiltonian constitute the mixed $p$-spin model associated to $( beta_p)$. . The Hamiltonian is a random function on $ mathbb{S}_N$; the model coefficients $ beta_p$ are taken to be square-summable to ensure that the object $H_N$ is a.s. smooth. . The covariance structure of $H_N$ is a function of the sphere&#39;s geometry: letting $ langle cdot, cdot rangle$ denote the Euclidean inner product, . $$ mathbb{E} H_N (s) H_N(t) = N xi left( N^{-1} langle s , t rangle right) $$ . Let us write . $$ R_{1 text{-}2} := N^{-1} langle s, t rangle equiv frac{ langle s, t rangle } { |s | |t | } $$when $s$ and $t$ are implicit, we refer to $R_{1 text{-}2}$ as their overlap. . When $ xi(x) = x^p$, the model is called the pure $p$-spin model. The pure $2$-spin model is more commonly known as the Sherrington-Kirkpatrick model. We prepend &quot;spherical&quot; to the model name when $ Sigma_N = mathbb{S}_N$ and omit this otherwise. . In any case, we regard $H_N$ as an energy landscape. Here are examples of questions one can ask about $H_N$: . Can we describe the asymptotic topology of the sublevel sets of these models? . | What is the asymptotic ground state energy of such models, i.e., can one show for some $c &lt; 0$ that . | . $$ min_{s in mathbb{S}} frac{H_N(s)}{N} to c quad text{a.s.} , ? $$ Given an inverse temperature parameter $ beta &gt;0 $ (not related to the $ beta_p$), we can form the Gibbs measure $ mu_{N, beta}$ associated to the Hamiltonian $H_N$ and this $ beta$. We suppress the dependence of $ mu_{N, beta}$ on the model function $ xi$. . The Gibbs measure $ mu_{N, beta}$ is thus a random probability measure on the state space $ Sigma_N$. . When $ Sigma_N = mathbb{S}_N$ let us conflate the Gibbs measure $ mathcal{G}_{N, beta}$ with the density which defines it, notationally. The density is with respect to the natural volume measure $ textrm{vol}_N$ on $ mathbb{S}_N$. . $$ mathcal{G}_{N, beta}(s) := frac{1}{ mathcal{Z}_{N, beta} } exp left( - beta H_N(s) right) , textrm{vol}_N( textrm{d} s), $$ where $ mathcal{Z}_{N, beta}$ is the partition function of $ mathcal{G}_{N, beta}$, a random normalizing constant of fundamental importance in statistical mechanics. . When $ Sigma_N = { pm 1 }$, the Gibbs measure $ mathcal{G}_{N, beta}$ is defined analogously. Examples of questions one can ask about these new objects: . Can one compute the limiting free energy of the model, $$ lim_{N to infty} frac{1}{N} log mathcal{Z}_{N, beta} $$ if it exists? | . In the setting of the pure $p$-spin model, for instance, fix a realization of the disorder $g_{i_1, dots, i_p}$. For $ beta &gt;0$ given, consider the associated Gibbs measure $ mathcal{G}_{N, beta}$. . Let $ mathcal{G}_{N, beta}^{ otimes 2}$ denote the law of a pair of configurations (elements $s,t$ of the state space $ Sigma_N$), where the two random variables are sampled from $ mathcal{G}_{N, beta}$, independently of one another. . For such a pair $(s,t) equiv (s_{N, beta}, t_{N, beta}) sim mathcal{G}_{N, beta}^{ otimes 2}$, we can form the overlap random variable . $$ R_{1 text{-}2} triangleq R_{1 text{-}2}(s,t) $$ What can be said about the asymptotic behavior of this random variable, especially as a function of $ beta$? | . Studying the overlap random variable to study the associated Gibbs measure is a way to leverage a phrase I hear repeated by experts, roughly: &quot;we don&#39;t know the ground states of the system, but the system does.&quot; .",
            "url": "https://the-ninth-wave.github.io/stat-mech/jupyter/2019/04/01/M450-Lec1.html",
            "relUrl": "/jupyter/2019/04/01/M450-Lec1.html",
            "date": " • Apr 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://the-ninth-wave.github.io/stat-mech/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://the-ninth-wave.github.io/stat-mech/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}