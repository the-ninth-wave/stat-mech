{
  
    
        "post0": {
            "title": "Markov chains, Monte Carlo methods, pattern theory",
            "content": "$ quad$ These notes serve as background for an ongoing project with a good friend, Connor Sempek. We are following this blog post by Brian Hayes. Our current objective is to re-create Hayes&#39; simulations of Ising model dynamics. The underlying canvas of the model is two-dimensional, so the simulation can be thought of as a way to generate a sequence of 100x100 pixel images, with pixel values either black or white. The Ising model dynamics update the current image by changing the value of one pixel at a time. The purpose of these notes is to get a better understanding of how long the simulation must be run so that, for a sparse enough subsequence of the images: . each is sampled approximately from the Gibbs probability measure governing the Ising model, and . | these are nearly independent. . | . $ quad$ The latter property is important for a longer term goal of the project, which is to use these subsequences of images as a computer vision dataset. Importantly, the Gibbs measure we are sampling from always depends on a global temperature parameter $T$, leading to natural classification and regression tasks. . $ quad$ Here is a starting point for formulating a classification task. Suppose that we can simulate Ising model dynamics well at two distinct temperatures. It feels natural to choose one of these, say $T_1$, above the critical temperature $T_c$ of the model, and the other, $T_2$ below it. We then create two image datasets $ mathcal{D}_1$ and $ mathcal{D}_2$ of equal size by simulating as described by the two points above. Merging these datasets to form what we call $ mathcal{D}$ leads to a natural classification problem: given an element $x in mathcal{D}$ chosen uniformly at random, determine whether it came from $ mathcal{D}_1$ or $ mathcal{D}_2$. . $ quad$ The relevance of the Ising model to vision has been explored heavily, it seems. A lot of the text below draws from Govind Menon&#39;s pattern theory notes, where a chapter is devoted to this &quot;energetic&quot; perspective applied to character recognition tasks. Menon&#39;s notes also reference a 1984 paper which develops a way to perform image restoration and enhancement using the Ising model. This makes it a natural model to experiment with, and there are a few directions I&#39;d like to explore longer term: . At the lower temperature, $T_2$, simulating the model is akin to an optimization problem: as $T to 0$, the Gibbs measure concentrates on the set of lowest-energy states, or ground states. Any simulation designed to sample from a low-temperature Gibbs measure needs to navigate the energy landscape of the system from some starting point to a low altitude, with the temperature dictating a small range of desirable low altitudes $ equiv$ low energies. This is effectively an optimization problem with cost function corresponding to the energy landscape. There is also an optimization inherent to the process of training any neural network on the classification task suggested. The question I would like to explore, loosely, is whether there are useful connections between these optimization problems. | We will train a simple convolutional architecture on the dataset $ mathcal{D}$. Whether it is through pooling layers, or the local averaging inherent in the convolution operation, these architectures seem to apply a kind of coarse-graining to input signals $x in mathcal{D}$. This has prompted investigation into the connections between deep learning and the renormalization group of physics. The latter feels like a kind of microscope for theoretical physicists $-$ the renormalization group formalizes a sequence of scaling transformations acting on a physical system, typically zooming out and thereby coarse-graining the preceding image of the system. This parallels feeding the image of the system into a &quot;deep&quot; sequence of convolutional layers. Two references for this are a 2013 paper of Cédric Bény, and a 2014 paper of Pankaj Mehta and David Schwab, and I imagine there is more recent work on this to explore. Dataset $ mathcal{D}$ is both an image dataset and a natural input to the renormalization group. It will give us an opportunity to better understand these connections, in particular we can start by repeating the experiments the latter paper runs on the Ising model. | A goal less directly related to ML is to augment the simulation to reproduce the simulations of Lubetzky and Sly on information percolation (main article with visualization, and this is a shorter expository version, still technical). Information percolation stacks each time-slice of the simulation described above (at some fixed temperature) into a &#39;space-time cylinder&#39;. Multi-colored edges or &#39;bonds&#39; are drawn between pixels in successive time-slices, based on how the simulation dynamics progress. The color of the bonds reflects different possibilities for how the update step is executed at a given pixel: sometimes, the randomness involved in the update is completely independent of everything else, and this is how the simulation gradually loses its memory of its starting point. The information percolation process outputs a graph or network inside the space-time cylinder encoding all aspects of the simulation. The relevance of information percolation is that becoming decoupled from the simulation starting point has the interpretation that the top slice of the space-time cylinder is not connected to the bottom slice through the information perecolation process. This is a way to graphically represent the &#39;mixing&#39; undergone by the Ising dynamics. Lubetzky and Sly use this perspective to deduce lower bounds on how long the simulation dynamics must be run in order to sample effectively from the Gibbs measure of the Ising model. So this is a tool relevant to both bullet points above, and could look cool to simulate and visualize. | . references . Krauth (2006) statistical mechanics: algorithms and computations . | Menon (2020) pattern theory . | . . $ quad$ The Monte Carlo method is increasingly becoming part of the discipline it is meant to study. The Monte Carlo method is a statistical, &quot;almost experimental&quot;, approach to computing integrals using random positions, called samples. Below, we introduce some sampling techniques for continuous and discrete variables. We also discuss the basic principles of statistical data analysis: how to extract results from well-behaved simulations. We also discuss at length the simulations where something goes wrong. . $ quad$ In probabilistic terms, the integrals we desire to approximate correespond to the expected value of some observable, $ mathcal{O} : mathcal{S} to mathbb{R}$. Here we assume some implicit probability measure $ mu$ on $ mathcal{S}$, the latter called the state space. Formally, the expected value $ mathbb{E} mathcal{O}$ corresponds to the integral $$ mathbb{E} mathcal{O} = int_{ mathcal{S} } mathcal{O}(s) , mu( textrm{d} s ) $$ We write this expectation either as $ mathbb{E} mathcal{O}$, or as $ langle mathcal{O} rangle$, the latter notation more common in physics. . $ quad$ We want to simulate most of the objects discussed throughout the notes, so we lose no generality in assuming that $ mathcal{S}$ is finite. $ mathcal{S}$ will be treated as a continuous object when it is more natural, or becomes convenient. . I am tempted to jump to conclusions in the following way:first, note that if $ mathcal{S}$ is finite with $ # mathcal{S} = M$, then $ mu$ corresponds to its probability mass function $p = (p_1, dots, p_M)$. In this case, the above integral becomes the $ sum_{i=1}^M o_i p_i$ for a given vector $o = (o_1, dots, o_M)$. The conclusion I jumped to was that approximating this sum happens first by estimating each $p_i$ through a statistic (like a histogram) $ hat{p}$ of some long-running simulation, and then taking a dot product of $ hat{p}$ with $o$. &gt; To be clear, in both direct sampling and Markov chain sampling, it seems instead that our approximation comes from taking measurements of a system as it evolves, and only after it has been allowed to evolve for some time. The measurements applied to each state of $ mathcal{S}$ are encoded in the measurement of the system, and the probability vector $p$ (of $ mu$) is encoded in the system evolution itself. . $ quad$ We wish to distinguish between two distinct sampling approaches:direct sampling and Markov chain sampling. We postpone the definition of a Markov chain until later; for now it is understood as a random walk. . ... direct sampling ... . (Krauth $ S$ 1.1.1) . $ quad$ Krauth explains direct sampling through a game played with small pebbles. The playing field is a large circle is inscribed into a large square, both drawn in the sand. In direct sampling, one has &#39;access&#39; to the measure $ mu$ sampling from $ mathcal{S}$. In the context of the game, we will assume we can directly sample from $ mu$, which denotes the uniform measure on the square $$ mathcal{S} equiv textsf{square} = [-1,1]^2 ,, $$ a &#39;birds-eye&#39; view of the playing field. Not only this, but we assume that we can also sample directly from $ mu$ multiple times, independently. . The observable considered in the game is related to its goal, namely to compute an approximation of the number $ pi$. The observable $ mathcal{O} : mathcal{S} to mathbb{R}$ is the indicator function of the unit disc, $ textsf{disc} = { s = (x,y) in textsf{square} : x^2 + y^2 leq 1 }$ $$ mathcal{O} equiv mathbf{1}_{ textsf{disc}} = begin{cases} 1 &amp; quad s in textsf{disc} 0 &amp; quad textrm{otherwise} end{cases} $$ . $ quad$ A player throws pebbles at the square, successively. Through the direct sampling, these land uniformly at random. The sampling is performed independently across draws from $ mu$. In particular, no pebble throw ever misses the square. Let $S_1, dots, S_N$ denote the collection of locations sampled from $ textsf{square}$ in the above procedure: each $S_i$ is a random ordered pair $S_i = (x_i, y_i) in textsf{square}$, where all coordinates are mutually independent, $ textrm{Unif}([-1,1])$ random variables. . $ quad$ To describe how the samples $S_i$ are used to approximate the value of $ pi$, we introduce the empirical measure $ mu_N^{ textrm{direct}}$, a random probability measure on $ mathcal{S}$, defined to encode the locations $S_1, dots, S_N$: $$ mu_N^{ textrm{direct}} := frac{1}{N} sum_{i=1}^N delta_{S_i}, $$ where in general $ delta_S$ denotes the delta mass at the random point $S$. Measures are in a sense dual to sets, and the delta mass $ delta_X$ is a measure acting on sets in the following way: given measurable $B subset textsf{square}$, we have $$ delta_S = begin{cases} 1 &amp; quad textrm{ if } S in B, 0 &amp; quad textrm{ otherwise. } end{cases} $$ The empirical measure $ mu_N^{ textrm{direct}}$, by definition, acts on sets $B subset mathcal{S}$ by reporting the fraction of the direct samples $S_i$ landing in $B$. . $ quad$ There is a sense in which the measures $ mu_N^{ textrm{direct}}$ &#39;converge&#39; to $ mu$, and the game, or simulation, is designed to exploit this. Let us enumerate the states of $ mathcal{S} = {s_1, dots, s_M }$. In practice, the size of $M$ will be determined by the &#39;resolution&#39; of the floating points used in sampling. In accordance with the observable $ mathcal{O}$, the set we choose for $B$ is $ textsf{disc}$. This is because $$ mu_N^{ textrm{direct}}( textsf{disc} ) = frac{1}{N} sum_{i=1}^N delta_{S_i}( textsf{disc} ) equiv frac{1}{N} sum_{i=1}^M mathcal{O}(S_i) $$ The sense in which $ mu_N^{ textrm{direct}}$ converges to $ mu$ implies, in particular, that the sequence of numbers $ mu_N^{ textrm{direct}}( textsf{disc})$ converges to $ mu( textsf{disc} )$ as $N to infty$. This convergence can also be seen as a consequence of a law of large numbers. . In the present context, the relevant subset of $ textsf{square}$ is the disc of radius one, centered at the origin. We refer to this as $ textsf{disc}$. The output of the first algorithm below is . $$ 4 cdot mu_N^{ textrm{direct}}( textsf{disc}) approx 4 cdot mu( textsf{disc}) equiv 4 cdot frac{ pi}{4} $$Thus we are approximating $ pi$ by approximating $ pi/4$, which is the probability that a pebble sampled from $ mu$ lands in $ textsf{disc}$. . import numpy as np import sys from numpy.random import default_rng rng = default_rng() . The next function embodies the &#39;direct&#39; sampling we supposedly have access to, with the independence assumptions described above. . . method ... Unif . def Unif(): return rng.uniform() . . Algorithm 1.1 is translated from pseudocode (as it is presented in Krauth) to Python. . . Algorithm 1.1 ... direct_pi . def direct_pi(N = 4000): N_hits = 0 &quot;&quot;&quot; the for loop is basically about computing the sum in mu_N(disc), stored in N_hits &quot;&quot;&quot; for i in range(N): # initialize Unif([-1,1]) r.v.&#39;s x = 2 * Unif() - 1 y = 2 * Unif() - 1 rad_sq = (x ** 2) + (y ** 2) if rad_sq &lt;= 1: N_hits += 1 &quot;&quot;&quot; mu_N(disc) is then computed by dividing by N &quot;&quot;&quot; ratio = N_hits / N return 4 * ratio . . direct_pi() . 3.134 . In Krauth, Table 1.1 gives results of five runs of the above with N = 4000. We return to this table later to compare with Monte Carlo methods. . run N_hits estimate of $ pi$ . 1 | 3156 | 3.156 | . 2 | 3150 | 3.150 | . 3 | 3127 | 3.127 | . 4 | 3171 | 3.171 | . 5 | 3148 | 3.148 | . Krauth remarks that none of the results of this table have fallen in the error bounds known since Archimedes. Another remark of Krauth: &quot;The people adopt a sensible rule: they decide on the total number of throws, before they start the game.&quot; (This is as in the above python code.) &quot;They understood that one must not stop a stochastic calculation simply because the current result appears accurate, nor should they continue to play because the answer they get isn&#39;t close enough to their idealized target.&quot; . ... Markov-chain sampling ... . ( Krauth $ S$ 1.1.2 ) . $ quad$ We now consider a new game being played on the same field, with different rules. We again take a birds-eye view of the playing field, so our state space is again $ mathcal{S} = textsf{square}$. The game starts with a person at the corner of the helipad, corresponding to coordinates $(1,1)$ in $ mathcal{S}$. In Krauth&#39;s description, in each coordinate, the walker takes a step via adding independent, $ textrm{Unif}([- delta, delta])$ random variables. Thus it is possible for a next step which takes the person outside the square. If this is the case, instead of taking the step, the walker does nothing. This step still &#39;counts&#39; in the aggregation we will perform. Krauth describes this procedure in terms of the pebbles: the person takes another pebble from their bag and places it on top of the one pebble (or perhaps pebble pile) marking their current location. . $ quad$ After an &#39;in-bounds&#39; step is taken, the walker places a pebble at their current position. Each walker is equipped with a bag of infinitely many pebbles. These two cases describe the way the new game is played. We make one modification to this, which is that . $ quad$ In the present setting, the samples $X_1, dots, X_N$ are not independent, but rather have the structure of a Markov chain, specifically a random walk with independent increments. Let us use $ xi_1, dots, xi_N$ denote these independent increments. In Krauth&#39;s description, each of these is a $ textrm{Unif}([- delta, delta])$ random variable. For reasons that Krauth discusses later, and we will comment on this, we instead assume that each $ xi_j$ is a discrete random variable uniformly distributed on the two-element set $ { - delta, delta }$. We have distinguished the starting point of the sampling at $X_0 = (1,1)$. Following the above description, one then has $$ X_1 = begin{cases} X_0 + xi_1 ,, &amp; quad textrm{ if } X_0 + xi_1 in textsf{square} X_0 &amp; quad textrm{ otherwise } end{cases} $$ . and in general, . $$ X_j = begin{cases} X_{j-1} + xi_j ,, &amp; quad textrm{ if } X_{j-1} + xi_j in textsf{square} X_{j-1} &amp; quad textrm{ otherwise } end{cases} $$for all $j = 1, dots, N$. . $ quad$ For this new sequence of random $X_i$, we can define the analogous object to $ mu_N^{ textrm{direct}}$, $$ mu_N^{ textrm{Markov}} := frac{1}{N} sum_{i=1}^N delta_{X_i} $$ . The algorithm below returns an approximation to $ pi$ via the same kind of ratio . $$ pi approx 4 cdot mu_N^{ textrm{Markov}}( textsf{disc}) $$ $ quad$ One might be tempted to use random initial conditions, assigning independent $ textrm{Unif}([-1,1])$ random variables to the two coordinates. We avoid this because the Markov chain methods become most useful when no direct sampling method exists. It is for exactly this reason that we also avoid directly sampling from $ textrm{Unif}([- delta, delta])$ random variables: such objects are a linear transformation away from directly sampling from the $ textrm{Unif}([-1,1])$ distribution, perhaps at lower resolution. . $ quad$ Consequently, our Markov-chain sampling simulations start at the corner $(1,1)$ for concreteness. They can also be assumed to start from where a previous simulation left off. Following this convention throughout these notes, we usually focus on going from configuration $i$ to configuration $i+1$. This is the defining property of (discrete-time) Markov chains: the transition probabilities of the process depend only on the current position. . . Algorithm 1.2 ... markov-pi (unmodified) . def markov_pi( delta = .05, N = 40000 ): # initial conditions x = 1 y = 1 N_hits = 0 num_rej_moves = 0 # number of &quot;rejected&quot; moves for i in range(N): # get coordinate increments del_x_big = 2 * Unif() - 1 del_y_big = 2 * Unif() - 1 del_x = delta * del_x_big del_y = delta * del_y_big #print(del_x, del_y) # potential new coordinates x_new = x + del_x y_new = y + del_y # to determine if step leads into disc rad_sq_new = ( x_new ** 2 ) + ( y_new ** 2 ) # ditto for square x_in = x_new &lt;= 1 and x_new &gt;= -1 y_in = y_new &lt;= 1 and y_new &gt;= -1 in_square = x_in and y_in # if move is accepted if in_square: x = x_new y = y_new else: num_rej_moves += 1 # if in disc, +1 to number of hits if rad_sq_new &lt;= 1: N_hits += 1 # we add in this output to # better understand how to pick # delta print( &quot;fraction of successful moves: &quot;, 1 - ( num_rej_moves / N) ) return 4 * ( N_hits / N ) . . markov_pi() . fraction of successful moves: 0.9751 . 3.0945 . $ quad$ Let us discuss how to choose the &#39;throwing range&#39; is $ delta$. It is akin to (largest possible) step size for the walker. Roughly, if $ delta$ is too small, the acceptance rate may be high, but the claim is that the walk will not be able to explore enough to well-approximate the integral. Choosing $ delta$ too large means the walk will have a hard time making successful moves, also hindering exploration. Krauth: &quot;The time-honored rule of thumb consists in choosing $ delta$ neither too large, nor too small, so that the acceptance rate turns our to be on the order of $ frac{1}{2}$.&quot; I didn&#39;t observe results agreeing with their rule of thumb. I found much more success with $ delta = 0.05$ versus their recommendation of $0.3$. I did however observe the approximation worsen as the step size was taken larger past $ delta = 1$. . . Algorithm 1.2&#39; ... markov-pi (modified) . def markov_pi_modified( delta = .05, N = 40000 ): # initial conditions x = 1 y = 1 N_hits = 0 num_rej_moves = 0 # number of &quot;rejected&quot; moves for i in range(N): &quot;&quot;&quot; we use uniform random variables, out of convenience but not to determine coordinate steps directly with these. Instead, we extract a pair of fair coin tosses from two uniform random variables. We use these coin tosses only to determine the sign of the increment in each coordinate. &quot;&quot;&quot; # generate indep uniforms U_1 = Unif() U_2 = Unif() # use these as coin tosses to determine # del_x... if U_1 &lt;= 0.5: del_x = delta else: del_x = (-1) * delta if U_2 &lt;= 0.5: del_y = delta else: del_y = (-1) * delta # potential new coordinates x_new = x + del_x y_new = y + del_y # to determine if step leads into disc rad_sq_new = ( x_new ** 2 ) + ( y_new ** 2 ) # ditto for square x_in = x_new &lt;= 1 and x_new &gt;= -1 y_in = y_new &lt;= 1 and y_new &gt;= -1 in_square = x_in and y_in # if move is accepted if in_square: x = x_new y = y_new else: num_rej_moves += 1 # if in disc, +1 to number of hits if rad_sq_new &lt;= 1: N_hits += 1 # we add in this output to # better understand how to pick # delta print( &quot;fraction of successful moves: &quot;, 1 - ( num_rej_moves / N) ) return 4 * ( N_hits / N ) . . markov_pi_modified() . fraction of successful moves: 0.953025 . 3.1668 . . Q $ quad$ In both the Glauber and Metropolis dynamics for the evolution of the Ising model, the spin variable to be updated ( i.e. the vertex $v in mathbb{T}$ sampled ) is chosen uniformly, through direct sampling. What if this is instead done through Markov-chain sampling? . Also: instead of rejecting the moves outside the box, we can play with periodic boundary conditions for the walks in both simulations. . . ... discussion ... . $ quad$ The Monte Carlo &#39;games&#39; described above epitomize the two basic approaches to sampling from a probability measure $ mu$ on some space $ mathcal{S}$. Both approaches to the sampling evaluate an observable $ mathcal{O} : mathcal{S} to mathbb{R}$. In our examples, $ mathcal{S} = textsf{square}$, and the observable $ mathcal{O}$ is the indicator function of the disc within the square: $$ mathbf{1}_{ textsf{disc}}(s) = begin{cases} 1 &amp; quad s in textsf{disc} 0 &amp; quad textrm{otherwise} end{cases} ,, quad mathbf{1}_{ textsf{disc}} : textsf{square} to mathbb{R} $$ . In either sampling case, we can define $ mathcal{O}_i := mathcal{O}(X_i)$, and note that one evaluates . $$ frac{N_{ textrm{hits}}}{N} = frac{1}{N} sum_{i=1}^N mathcal{O}_i approx langle mathcal{O} rangle, $$where the brackets $ langle , cdot , rangle$ denote the expected value of $ mathcal{O}$ taken with respect to the probability measure $ mu$, which in this case is uniform on $[-1,1]^2$. . $ quad$ Krauth remarks that $ langle mathcal{O} rangle$ is an integral, $$ langle mathcal{O} rangle = int_{-1}^1 textrm{d} x int_{-1}^1 textrm{d}y , mu (x,y) mathcal{O}(x,y) $$ In his notes, Krauth includes a denominator on the right-hand side above, it is: $$ int_{-1}^1 textrm{d} x int_{-1}^1 textrm{d}y , mu (x,y) $$ . My understanding of $ mu(x,y)$ is that it is used to denote the density of probability measure $ mu$, so it integrates to one. Within the above bounds of integration, it can be taken as a constant, namely $1/4$, which is the reciprocal of the area of $[-1,1]^2$. Regardless, there is an integral to be computed, and this is an aspect Krauth remarks on next. . $ quad$ The density $ mu(x,y)$ doesn&#39;t appear in the approximation $ tfrac{1}{N} sum_{i=1}^N mathcal{O}_i$, Krauth: &quot;...rather than being evaluated, it is sampled. This is what defines the Monte Carlo method.&quot; Krauth goes on to point out that the multiple integrals also disappear. &quot;This means that the Monte Carlo method allows the evaluation of high-dimensional integrals, such as appear in statistical physics and other domains, if we can only think of how to generate the samples.&quot; . $ quad$ The multiple integral above is integration in two variables, so it doesn&#39;t feel like it is genuinely a &#39;high-dimensional&#39; integral to compute. On the other hand, we are already know of a kind of Markov chain on a high-dimensional space, namely the Glauber dynamics for the Ising model on $ mathbb{T}$, the two-dimensional torus $( mathbb{Z} , / , (n cdot mathbb{Z}) )^2$. All in all, it seems that one is approximating an integral in an arbitrary number of dimensions (number of spins in the case of Ising model) through what is essentially a path integral. The path integral is a sum of observables of the system, as the system evolves. . $ quad$ Krauth: &quot;Direct sampling, the approach inspired by the game, is like pure gold: a subroutine provides an independent hit at the distribution function $ pi$. Notwithstanding thee randomness in the problem, direct sampling, in computation, plays a role similar to exact solutions in analyitcal work, and the two are closely related. In direct sampling, there is no throwing-range issue, no worrying about initial conditions, and there is a straightforward error analysis -- at least if $ mu$ and $ mathcal{O}$ are well behaved. Many successsful Monte Carlo algorithms contain exact sampling as a key ingredient.&quot; . $ quad$ &quot;Markov-chain sampling, on the other hand, forces us to be much more careful with all aspects of our calculation. The critical issue here is the correlation time, during which the position of the walker retains a memory of the starting configuration. This time can become astronomical. In the usual applications, onee is often satisfied with a handful of independent samples, obtained through week-long calculations, but it can require much throught and experience to ensure that even this modest goal is achieved. We shall continue our discussion of these methods in a later section. We first look briefly at the history of stochastic computing.&quot; . before this, we draw background on Markov chains and Gibbs measures from Menon. . ... Markov chains ... . (Menon $ S$ 1.1, 1.2) . $ quad$ We model the world as a space of random signals. The world is filled with observers, which record part of these signals and then form inferences about the world. The process of inference is modeled with a Bayesian paradigm. We assume an observer has a stochastic model capable of generating signals similar to the true signal. Inference reduces to tuning the model parameters to obtain the best match between the generated and observed signals. . $ quad$ These notes explore stochastic models of progressively increasing complexity, in tandem with numeraical algorithms for optimization. The simplest class of stochastic models we consider are Markov chains. Suppose we are given a finite set $ mathcal{S}$ called the state space of the Markov chain. This will sometimes (such as in text-modeling instances) be called the alphabet, in which case we may use $ mathcal{A}$ in place of $ mathcal{S}$. . . Definition. ( stochastic process ) . $ quad$ A stochastic process is a collection of random variables. . . $ quad$ We study a stochastic process $(X_t)_{t in mathbb{N}}$ taking values in the given finite set $ mathcal{S}$. Thus, the temporal (given by the index $t$ in the stochastic process) and the spatial (given by the state value in $ mathcal{S}$ of the process) aspects of this process are discrete. . . Definition. ( Markov chain ) . $ quad$ The stochastic process $(X_t)_{t in mathbb{N}}$ is a Markov chain if for all $t$, $ mathbb{P} ( X_{t +1} | X_1, dots, X_t ) = mathbb{P} (X_{t+1} | X_t ) $. . . $ quad$ As the above definition indicates, the modeling assumption in Markov process theory is that the future of the process depends on the current state, but not the past. This is the probabilistic analogue of the idea of Newtonian determinism, and is encoded in the forward equation below. Stationary Markov chains are thus characterized by their transition matrix $$ Q(s, tilde{s}) = mathbb{P}( X_2 = tilde{s} | X_1 = s ) ,, $$ with entries indexed by pairs of states $s, tilde{s} in mathcal{S}$. Another interpretation of the Markov property is that the future and past are independent, conditional on the present. We typically simplify the structure of Markov chains with the following assumption. . . Definition. ( stationary Markov chain ) . $ quad$ The Markov chain $(X_t)$ is stationary if $ mathbb{P}(X_{t+1} | X_t ) = mathbb{P} (X_2 | X_1 )$ for all $t in mathbb{N}$. . . $ quad$ The most basic statistic associated to the Markov chain is the pmf( $ equiv$ probability mass function ), or law of the random variable $X_t$. The mass assigned to state $s in mathcal{S}$ under this law is denoted $$ m_t(s) := mathbb{P}(X_t = s) ,, $$ and one obtains the following recurrence relation using the so-called law of total probability, along with the definitions of $m_t$ and $Q$: $$ begin{align} m_{t+1}( tilde{s}) &amp;= mathbb{P} ( X_{t+1} = tilde{s}) &amp;= sum_{s in mathcal{S}} mathbb{P}( X_{t+1} = tilde{s}, , X_t = s ) &amp;= sum_{s in mathcal{S}} mathbb{P} (X_{t+1} = tilde{s} | X_t = s ) mathbb{P}(X_t = s) &amp;= sum_{s in mathcal{S}} Q(s, tilde{s}) m_t(s), end{align} $$ when we assume the chain is stationary. This calculation yields the forward equation $$ m_{t+1} = m_t Q , quad t in mathbb{N} $$ using the convention that the pmf $m_t$ of $X_t$ is regarded as a row vector. The forward equation is linear and explicitly solvable when the chain is stationary. We proceed inductively to find $$ m_t = m_1 Q^t , quad t in mathbb{N} ,, $$ and one of the central questions in Markov chain theory is to understand the behavior of $m_t$ as $t to infty$. This question explains the importance of the eigenvalue spectrum of $Q$ to Markov chain theory: diagonalizing $Q$ is an efficient way to compute $Q^t$ for $t$ large, and the spectrum of $Q$ largely governs the long term behavior of the system. . ... equilibrium measures, ergodicity, random walks ... . (Menon $ S$ 1.2) . . Definition. ( equilibrium / stationary distribution ) . $ quad$ Let $Q$ be the transition matrix of a stationary Markov chain $X = (X_t)$ with state space $ mathcal{S}$, and let $m$ be a probability measure on $ mathcal{S}$, whose pmf we also denote $m$. The measure $m$ is an equilibrium distribution or equivalently, a stationary distribution for $X$ if $$ m = m Q, $$ and if $m$ satisfies the normalization conditions $m(s) geq 0$ for all $s in mathcal{S}$ and $ sum_{s in mathcal{S}} m(s) =1$. . . $ quad$ The above definition is of central importance to our simulation goals. The reason is that, for Markov chains with a unique stationary distribution $m$, the measures $m_t equiv mathbb{P}(X_t = cdot )$ converge to $m$, in an appropriate sense, as $t to infty$. When we simulate a Markov chain, we are sampling from $m_t$, and we want to run the simulation long enough (taking $t$ large enough) so that $m_t approx m$. This is how a Markov chain Monte Carlo (MCMC) simulation can be used to approximate sampling from $m$. In practice, instead of starting with some Markov chain dynamics, we will always start with a measure $m$ on $ mathcal{S}$, which we would like to sample from. . $ quad$ The MCMC method then requires finding a stationary, ergodic Markov chain $X$ with state space $ mathcal{S}$, whose stationary measure coincides with $m$. This is how $X$ is used to simulate sampling from $m$. . $ quad$ In Krauth&#39;s pebble games, there was some discussion of initial conditions of a Markov chain: in the second pebble game, the person playing always had to start at a prescribed corner of the square. These are deterministic initial conditions. In practice, we agreed to assume simulations will start deterministically, or from where a previous simulation left off. In theory, it is useful to have a mathematical object associated to the initial conditions of a Markov chain: a probability measure on $ mathcal{S}$, denoted $m_0$. When initial conditions are deterministic, and start at some distinguished state $s_* in mathcal{S}$, the probability measure $m_0$ corresponds to the $ delta$-mass $ delta_{s_*}$. When $m_0$ is general, it models starting from where some simulation left off, whose information we have no (or only partial) access to. . $ quad$ The first equation in the above definition can be understood as describing the evolution of a Markov chain $X$ whose initial conditions are the stationary distribution $m$ associated to $X$. In this case $m_t = m$ for all $t$ if $m_1 = m$. That is, while the values of $X_t$ change with time (it&#39;s a stochastic process) its probability mass function does not. Thus the equilibrium distribution of a Markov chain captures the intuitive idea of a dynamic equilibrium . . $ quad$ Considering the task of computing $m$ given $Q$: observe that $m$ is a left eigenvector of $Q$ with eigenvalue $1$. Further, $1$ must always be an eigenvalue of $Q$ since it corresponds to the right eigenvector $(1, dots, 1)^T$. Thus, if we know how to solve for eigenvectors, we can determine $m$. Menon: &quot;What is interesting in practice is the converse: for large transition matrices, say when $ # mathcal{S}$ is $10^6 times 10^6$, it is more efficient to use Markov chains to approximate $m$, than to use naive linear algebra This observation plays an important role in web crawlers and search engines.&quot; . $ quad$ Returning to the main question of the Ising simulation: knowing how long to run the MCMC simulation for, or knowing how quickly $m_t$ converges to $m$ is related to how fast the Markov chain mixes. We now explore two examples of Markov chains whose state space is the symmetric group $ mathfrak{S}_{52}$. Each group element can be thought of as a possible shuffling of a standard deck of playing cards. The group identity of $ mathfrak{S}_{52}$ corresponds to some distinguished initial ordering. For each of the two shuffling methods described, the main question can be rephrased as: &quot;how many shuffles does it take to obtain a truly random (uniform) distribution if one begins with a perfectly ordered deck of cards?&quot; The following term will be used. . . Definition. ( ergodic Markov chain ) . $ quad$ A stationary Markov chain $X$ with state space $ mathcal{S}$ and transition matrix $Q$ is ergodic if it has a unique equilibrium distribution. A sufficient condition for ergodicity of $X$ is if for every pair $s, tilde{s} in mathcal{S}$ there is a positive integer $j(s, tilde{s})$ such that $Q^{j(s, tilde{s})} &gt;0 $. . . $ quad$ Many other conditions are known that ensure ergodicity. For example, if all terms of $Q$ are strictly positive, it is possible for any state to get to any other state, and this ensures that the Markov chain is ergodic. This assumption is too restrictive in practice, since we are mainly intersted in Markov chains making &#39;local&#39; moves such as random walks in a large state space. This behavior is captured by the sufficient condition given in the above definition. Intuitively, it means that every state can visit every other state given enough time. We assume this property always holds. . $ quad$ Physicsts define the term ergodic ( describing Markov chain $(X_t)_{t in mathbb{N}}$ ) to mean the following. . $$ lim_{T to infty} sum_{t =1}^T mathcal{O}(X_t) = mathbb{E}_m mathcal{O}, $$where $m = m Q$ and $ mathcal{O}: mathcal{S} to mathbb{R}$ is arbitrary. . $ quad$ This formulation has the interpretation that a Markov chain is ergodic when the left-hand side, the time-average, is equal to the spatial-average (with respect to the equilibrium measure), which is the right hand side. For Markov chains ergodic in the first sense, the second notion of ergodicity follows from an ergodic theorem. Menon notes the following bottlenecks and simplifications: . $ mathcal{S}$ may be very large. For example (in shuffling dynamics we soon describe), $ mathcal{S}$ may be the permutation group. . | $Q$ may be a sparse matrix. . | As discussed previously, we often don&#39;t need $m$ explicitly, we instead want to evaluate $ mathbb{E}_m mathcal{O}$, where $ mathcal{O}: mathcal{S} to mathbb{R}$ is some observable. . | . . Example. ( simple random walk ) . $ quad$ Let $G = ( textrm{V}, textrm{E})$ be a finite graph with vertex set $ textrm{V}$ and edge set $ textrm{E}$. Given $x,y in textrm{V}$, write $x sim y$ if the edge $ { x, y }$ lies in $ textrm{E}$. Let $ textrm{deg}(x) triangleq sum_{x sim y } 1$ be the degree of vertex $x$. A simple random walk on $G$ is a Markov chain with state space $ textrm{V}$ and transition matrix $$ . Q(x,y) . begin{cases} frac{1}{ textrm{deg}(x)}, &amp; quad text{ if } x sim y 0 &amp; quad text{ otherwise. } end{cases} . $$ We leave the initial conditions ambiguous in this definition. . . . Example. ( random walks in the permutation group ) . $ quad$ Let $ mathfrak{S}_n$ denote the permutation group on $n$ symbols. To be clear, let . $$ [n] := { , 1, dots, n , }, $$and recall that a permutation on $[n]$ is a bijection from $[n]$ to itself. The group structure on the collection of such functions, denoted $ mathfrak{S}_n$, is given by function composition. Below we describe two distinct rules for Markovian dynamics on $ mathfrak{S}_n$. Each set of dynamics can be thought of as arising from forgetting the implicit Cayley graph structure on $ mathfrak{S}_n$: this is the complete graph on $n!$ vertices. Below, we consider adjacency structures on $ mathfrak{S}_n$ distinct from this and from one another. . $ quad$ Through the adjacency matrix from the previous example, these new graph structures (writing $ sim&#39;$ and $ sim&#39;&#39;$ in this example) on $ mathfrak{S}_n$ induce simple random walk dynamics, and so each describes a concrete Markov chain on $ mathfrak{S}_n$, modulo initial conditions. . We declare permutations $ sigma$ and $ tau$ adjacent and write $ sigma sim&#39; tau$ if $ sigma$ and $ tau$ are related by a transposition of adjacent elements. The $&#39;$-degree of each permutation in this graph is $n-1$. . | We declare permutations $ sigma$ and $ tau$ adjacent and write $ sigma sim&#39;&#39; tau$ if $ sigma$ and $ tau$ are related by a transposition, not necessarily of adjacent elements. The $&#39;&#39;$-degree of each permutation in this graph is ${n choose 2}$. . | These are examples of distinct ergodic random walks on the same state space, namely $ mathfrak{S}_n$, both of which have the uniform measure, let us still denote this $ mu$, as their equilibrium measure. The simulation &#39;purpose&#39; of both random walks is thus to sample from this uniform measure, and the distinctness of the two walks raises the question of whether one method does the sampling better. . . ... Shannon&#39;s model of text ... . ( Menon $ S$ 1.3 ) . $ quad$ The main idea in Shannon&#39;s model (of text / written language) is that text is text is a stationary, ergodic stochastic process $(X_t)$ taking values in an alphabet $$ mathcal{A} triangleq { , a, ,b, , c, , dots ,, , z, , _ , } $$ This model is easily augmented to include punctuation and case, but we ignore these concepts in the first approximation. Note that we are not assuming that this process is a Markov chain. As discussed, the alphabet is the state space $ mathcal{S}$ from previous sections, but we use this terminology and notation to emphasize the language modeling aspect of the discussion. Note also that stationarity is distinct from the Markov property, in that neither property implies the other. . . Definition. ( stationary stochastic process ) . $ quad$ A discrete time stochastic process $X equiv (X_t)$ is stationary if all abitrarily large, but finite, marginal probabilities are invariant under arbitrary shifts in time. More precisely, . $$ mathbb{P} ( , X_1 = s_1, dots, X_n = s_n ,) = mathbb{P} ( ,X_{1+k} = s_1, dots, X_{n+k} = s_n ,) $$for all positive integers $k$ and $n$. . . $ quad$ Intuitively, this means that if we were to observe strings of length $n$, their statistics are not changed by shifting them forward in time by an integer $k$. For such processes, the notion of ergodicity again means that time averages are equal to spatial averages. From the perspective of observables $ mathcal{O} : mathcal{S}^n to mathbb{R}$, which through the above definition requires have an arbitrarily large &#39;memory&#39; $n$ of the process, one has $$ lim_{T to infty} frac{1}{T} sum_{t =0 }^{T-1} mathcal{O}(X_{t+1}, dots, X_{t+n}) = mathbb{E} mathcal{O}( X_1, dots, X_n) $$ . $ quad$ Menon: &quot;The above assumptions are introduced to conform to the idea that written text contains many heirarchical structures: letters are joined by phonetic rules to form words, words are linked by the rules of grammar into sentences, sentences are organized into paragraphs, and so on. Strange as it may seem at first sight, these rules can be effectively modeled as a stochastic process. The use of stationary ergodic processes provides us with a definition flexible enough to include heirarchical structures (though these may be complex to write down), and simple enough to computationally test.&quot; . $ quad$ As discussed, written text is modeled by a stationary stochastic process $Y$, whose law is denoted $$ mathbb{P}_{ textrm{true}} . $$ The subscript &#39;$ textrm{true}$&#39; indicates that we view $Y$ not as an approximation to language, but as a stream of &#39;correctly written&#39; English text, generated in real-time. This idealization is vague, but the main point is to identify the written language $ mathcal{L}$, in this case English, with a concrete mathematical object. In this case, the object is the stationary, stochastic process $Y$. The law $ mathbb{P}_{ textrm{true}}$ of this process is the analogue of $ mu$ in two previous contexts: the uniform measure on $ mathcal{S} = textsf{square}$, and the uniform measure on $ mathcal{S} = mathfrak{S}_{52}$. In either previous case, the purpose of the Markov chains involved was to approximately sample from $ mu$. Below, we discuss a family of Markov chains for approximately sampling from $ mu = mathbb{P}_{ textrm{true}}$. The way in which this sampling happens seems distinct from before, and worth remarking on after introducing the approximating Markov chains. . $ quad$ In practice, the probabilities relative to $ mathbb{P}_{ textrm{true}}$ below are approximated by mining a large corpus (e.g. the works of Shakespeare) to approximate $ mathbb{P}_{ textrm{true}}(a_1, dots, a_n)$ for an arbitrary string $(a_1, dots, a_n) in mathcal{A}^n$. . . Example. ( digram model ) . ( or 2-gram model ) . $ quad$ This is a Markov chain with state space $ mathcal{A} = { , a, , b, , c, , dots , , ,z, , _ }$. . $$ Q^{(2)}(x,y) = mathbb{P}_{ textrm{true}} ( ,X_2 = y | X_1 = x ,), $$which is to say, we form the character-to-character transition probabilities of a Markov process using the marginals of the full stationary process being approximated. . . $ quad$ We can get better approximations by allowing the Markov chain above to retain more of its history. This means defining a Markov process on $ mathcal{A}^n$ for some $ n geq 2$. When $n=2$, we obtain the so-called trigram model. . . Example. ( trigram model ) . ( or 3-gram model ) . $ quad$ This is a Markov chain with state space $ mathcal{A}^2$, which we style as . $$ { aa, ,ab, ,, dots, az, dots, _a, , dots, , _ , _ } $$Let $x = a_1a_2$ and $y = b_1b_2$. In order to form a text of three letters from $x$ and $y$, we must ensure that $x$ and $y$ agree on their overlap, that is when $a_2 = b_1$, so that $x$ and $y$ combine to give us the string $a_1a_2b_2$. With this restriction on $x$ and $y$, we obtain the associated transition matrix . $$ Q^{(3)} (x,y) = frac{ mathbb{P}_{ textrm{true}} ( a_1 a_2 b_2 ) } { mathbb{P}_{ textrm{true}} (a_1 a_2) } $$and $Q(x,y) =0$ when $a_2 neq b_1$. . Because of the compatability requirement of $a_2 = b_1$, the above Markov chain can still be viewed as a stochastic process taking values in $ mathcal{A}$, it is just no longer a Markov chain from this perspective. This is the sense, however, in which this model is approximating $ mathbb{P}_{ textrm{true}}$. . . . Example. ( $(n+1$)-gram model ) . $ quad$ A Markov chain with state space $ mathcal{A}^{n}$. A state $x$, say $x = (a_1, dots, a_n)$ can be followed by a string $y = (b_1, dots, b_n)$ only if $a_2 = b_1, a_3 = b_2, dots, a_n = b_{n-1}$. This induces transition matrix . $$ Q^{(n+1)} (x,y) = frac{ mathbb{P}_{ textrm{true}} ( a_1 a_2 dots a_n b_n ) } { mathbb{P}_{ textrm{true}} (a_1 a_2 dots a_n) } $$as above. . Following the previous remark, for similar compatibility reasons, this Markov chain can also be interpreted as a stationary stochastic process taking values in $ mathcal{A}$, and as before, this is the sense in which the above process approximates $ mathbb{P}_{ textrm{true}}$. . . $ quad$ Menon: &quot;Shannon proved that as $n to infty$, the law of text generated by the above Markov approximations converges to the law of the true language. Note however that a good proof may not correspond to a good algorithm. For example, as $n$ increases, the size of the state space ( of the approximating Markov process ) grows exponentially. Thus the dimensions of the transition matrix are $ # mathcal{A}^{2n}$, and worse yet, the size of the training data ( to determine $Q^{(n+1)}$ by mining ) also expands exponentially.&quot; . $ quad$ &quot;This follows the rules of everyday language. Once $n$ gets large enough, say $5$ or $6$ in practice, it is much more efficient to make our fundamental unit words, rather than letters, since the number of true words of length $5$ is much lower than the $26^5$ combinations that are possible. This reflects the true nature of the space $ $ character. In effect, we are still using the digram model, though we have switched to a new &#39;alphabet&#39; whose fundamental units are words.&quot;_ . ... information theory ... . references . Ivan Matic, Information Theory. Sanov&#39;s Theorem (notes from July 21, 2018) | . $ quad$ Before going further, let us return to the object $ mathbb{P}_{ textrm{true}}$, playing the role of $ mu$ in previous examples. When recalling previous examples, we identifid $ mu$ as the uniform measure on $ mathcal{S}$, in either case. To be formal, $ mu = mathbb{P}_{ textsf{true}}$ is a probability measure on $ mathcal{S}^ infty equiv mathcal{A}^ infty$, $$ mathcal{A}^ infty := { a equiv (a_j)_{j=1}^ infty : a_j in mathcal{A} textrm{ for all } j } ,, $$ the space of infinite sequences of elements of $ mathcal{A}$. Any discrete-time $ mathcal{A}$-valued stochastic process, given some initial conditions which we ignore, induces a probability measure on $ mathcal{A}^ infty$. This latter space has the interpretation of a &#39;path-space&#39;, as it is meant to contain all possible trajectories of the process. . $ quad$ The digram model naturally gives rise to a Markov chain with state space $ mathcal{A}$. Let us say that it induces measure $ mathbb{P}_{(2)}$ on $ mathcal{A}^ infty$. The trigram model is defined as a Markov chain with state space $ mathcal{A}^2$, but following the above remark, can be interpreted as a stationary stochastic process taking values in $ mathcal{A}$, inducing a measure $ mathbb{P}_{(3)}$ on the path-space $ mathcal{A}^ infty$. In general, the $(n+1)$-gram model induces measure $ mathbb{P}_{(n+1)}$ on $ mathcal{A}^ infty$. My interpretation of what Menon says at the beginning of the most recent quote, is &quot;Shannon proved that...&quot; $ mathbb{P}_{(n+1)}$ converges to $ mathbb{P}_{ textrm{true}}$, in an appropriate sense. . The approximation of $ mathbb{P}_{ textrm{true}}$ by $ mathbb{P}_{(n+1)}$ does not at all seeem to be an approximation of the same nature was is described in Krauth. By this I mean that $ mathcal{A}$-valued Markov chains do not play the same role in each case. In earlier cases, they served as a way to approximately sample from a probability measure on $ mathcal{A}$. In the present context, these objects induce a probability measure on the path-space $ mathcal{A}^ infty$, towards approximating $ mathbb{P}_{ textrm{true}}$, the probability measure on $ mathcal{A}^ infty$ encoding the &#39;$ textrm{true}$&#39; language. . $ quad$ I want to take a detour through some notes of Ivan Matic, because I think they add to the above discussion, as well as to Menon&#39;s upcoming discussion of entropy. Menon: &quot;the entropy of a random variable describes its uncertainty. In particular, the notion of an optimal code tells us that entropy describes the optimal search procedure to determine the value of $X$ through a series of yes / no questions.&quot; Matic&#39;s notes touch on this optimality, from the perspective of data compression. . $ quad$ To integrate Matic&#39;s perspective, he starts by imagining &quot;...we want to store some big quantity of data, such as a picture, a text document, or a book. We start with a collection of uncompressed data, and we identify the smallest building blocks of each datum as symbols. The set of all symbols is called the alphabet.&quot; Matic also denotes this alphabet as $ mathcal{A}$. . It feels worth remarking that languages like Chinese are pictoral, and that Chinese characters can be decomposed into &#39;brushstrokes&#39;. A space character _ perhaps corresponds to moving to the next character. I think this lends itself to the notion that pictoral data can be decomposed into symbols. For images in general, I think &#39;contours&#39; become the analogue of brushstrokes. Perhaps an efficient way to use a space character _ is as denoting the start of a new transparent canvas layer, sitting over the previously drawn layers (copying the functionality of applications like photoshop). My impression of $3$-dim spatial data (e.g. virtual environments for games) is that environment information is efficiently stored in surfaces, the higher-dimensional analogue (by one dimension) of contours. . The above decomposition reminds me of something I heard about the clustering algorithm t-SNE: when applid to MNIST, the number of clusters the algorithm found was more than the number of categories (labeled by digits $0 - 9$). Some digits corresponded to multiple clusters because of there being distinct ways to write this digit, with or without a horizontal slash in the case of $7$, for example. In a sense, the clustering worked too well. . $ quad$ A raw, uncompressed datum will be called a signal. The signal is sampled from the measure $ mathbb{P}_{ mathcal{L}}$ on $ mathcal{A}^ infty$ corresponding to abstract language $ mathcal{L}$ with alphabet $ mathcal{A}$. This measure corresponds to a stochastic process $(X_t)_{t geq 1}$, which we suppose is stationary. Because of the &#39;optimal encoding&#39; perspective, it is important to be able to quantify the relationship between the lengths of a signal and its encoding. In particular, we assume some upper bound $M$ on the signal length to begin with, modeling signals as elements of $ mathcal{A}^M$ for some $M in mathbb{N}$. When referring to $ mathbb{P}_{ mathcal{L}}$ in this context, it is understood as the projection of this measure onto $ mathcal{A}^M$, corresponding to the &#39;truncated&#39; process $(X_t)_{t = 1}^{M}$. Elements of $ mathcal{A}^M$ will also be called signals (Matic calls them messages). . $ quad$ We begin in an even simpler setting than the above language models. Here, we suppose $ mathbb{P}_{ mathcal{L}}$ corresponds to an i.i.d. sequence of random variables $(X_t)_{t = 1}^M$, where each $X_t$ has the same distribution as some &#39;model&#39; random variable $X$, corresponding to probability measure $m$ on $ mathcal{A}$. Because $ mathcal{A}$ is finite, so is the space of signals $ mathcal{A}^M$. A lossless data compression is an injective function on this signal space. . . Definition. ( lossless data compression ) . A lossless data compression is an injective function $$ f : mathcal{A}^M to bigcup_{j=1}^N { 0 , 1 } ^j $$ for some $N in mathbb{N}$. . . $ quad$ Given some lossless data compression $f$, and for each signal $s equiv (s_1, dots, s_M) in mathcal{A}^M$, we let $ textsf{length}_f(s)$ denote the length of the signal encoding $f(s)$. Letting $ mathbb{E}$ denote expectation with respect to the &#39;model&#39; random variable $X$, desirable propereties of the compression $f$ make $$ mathbb{E} , textsf{length}_f(X) $$ as small as possible. Heuristically, this means (1) identifying the signals in $ mathcal{A}^M$ which are typical and atypical, according to the sampling $(X_t)_{t=1}^M$, and then (2) using shorter binary sequences to encode typical signals, leaving longer binary sequences for atypical signals. This means the expectation is placing as little probablistic mass as possible on the longest encodings. . $ quad$ Let us use $m$ to denote the probability measure on $ mathcal{A}$ induced by the random variable $X$, our template for the i.i.d. sequence. Enumerating $ mathcal{A}$ as $ { a_1, dots, a_n }$, let us write $m_i$ for the number $m( { a_i })$. We identify $m$ with the vector $(m_1, dots, m_n)$, which is the probability mass function of $X$. It is $m$ that determines how &#39;typical&#39; a given sequence is. . $ quad$ Let $s equiv (s_1, dots, s_M) in mathcal{A}^M$. Write $ underline{X}$ to denote the random vector $(X_t)_{t=1}^M$. The mutual independence of the $X_t$ implies $$ begin{align} mathbb{P}( , underline{X} = s ) &amp;= mathbb{P} (X_1 = s_1) dots mathbb{P}(X_M = s_M) &amp;= 2^{ sum_{t=1}^M log_2 mathbb{P}(X_t = s_t)} &amp;= 2^{ sum_{i=1}^n n_i log_2 m_i} ,, end{align} $$ where, in going from the second to third line, we have reindexed the sum over the alphabet, using that $m_i equiv mathbb{P}(X = a_i)$, and defining $$ n_i := # { , t in { 1, dots, M } : X_t = a_i , }. $$ Thus, $n_i$ is the number of times symbol $a_i$ appears in the random signal $ underline{X}$. A typical message is one in which the frequency of symbol $a_i$ is close to the corresponding probability $m_i$. Thus, for typical messages, we have $n_i approx m_i M $, and the probability of such a message is approximately $$ 2^{ M sum_{i=1}^n m_i log m_i } $$ . . Definition. ( entropy ) . $ quad$ Let $m$ be a probability measure on finite state space $ mathcal{A}$ with $ # mathcal{A} = n$, corresponding to probability mass function $(m_1, dots, m_n)$. The entropy of $m$ is $$ textsf{S}(m) := - sum_{i=1}^n m_i log_2 m_i $$ . . $ quad$ Note that $ textsf{S}(m) geq 0$, and is $=0$ if and only if $m$ corresponds to a deterministic object, namely a $ delta$-mass at some symbol in the alphabet. In the above setting, we are imagining the length $M$ of the signal is much larger than the size $n$ of the alphabet $ mathcal{A}$. This is so that the law of large numbers has a chance to kick in for each symbol. In this setting, typical messages appear with probability on the order of $2^{-M textsf{S}(m)}$. When $m$ is non-trivial, the positivity of $ textsf{S}(m)$ implies that typical signals are exponentially rare, in the signal length $M$. . $ quad$ A consquence of the intrinsic high-dimensionality of $ underline{X}$ -- which corresponds to a probability measure on $ mathbb{R}^M$, with $M$ large -- is that signals with symbol frequencies close to the corresponding $m_i$ are very common. The next definition formalizes this notion of closeness. . . Definition. ( $ epsilon$-typical signal ) . For fixed $ epsilon &gt; 0$, we say $s in mathcal{A}^M$ is $ epsilon$-typical if $$ mathbb{P}( , underline{X} = s ) in left( 2^{-M( textsf{S}(m) + epsilon) }, , 2^{-M( textsf{S}(m) - epsilon) } right) ,, $$ and the set of all $ epsilon$-typical signals is denoted $ mathcal{A}^M_ epsilon$. . . $ quad$ As discussed, the goal is to efficiently compress signals $ underline{X}$ drawn from the measure $m$. The relevance of the above defininition to compression is: . we first prove that $ underline{X}$ is $ epsilon$-typical with high probability . | we will see that the set of typical signals is nonetheless small: the number of all possible signals is $ # mathcal{A}^M equiv n^M$, and there are only approximately $2^{M textsf{S}(m)}$ . | these facts combine to form a compression strategy: when we see a typical message, we begin its binary encoding (towards defining the lossless compression function $f$) with 0, and we use $M textsf{S}(m)$ bits to compress it. If the message is atypical, we begin its binary encoding with a 1, and are more sloppy with how many bits we use to store. The formal statement of the first point is as follows. . . Theorem. (Matic, Theorem 1) . $ quad$ For each $ delta &gt; 0$, there is $M_0 in mathbb{N}$ such that for every $M geq M_0$ there is a lossless data compression $f$ in which the average length of a message satisfies . $$ mathbb{E} textsf{length}_f ( , underline{X} ) leq M textsf{S}(m) + delta $$ . ( we defer recording the proof ) . Matic: &quot;The algorithm constructed (in the above proof) creates a lossless compression. It is very effective when the raw data consists of independent components. The png format uses a better algorithm.&quot; . $ quad$ To discuss this algorithm, Matic introduces what he calls types. These objects were previously introduced as the frequencies $n_i$ above. . . Definition. ( the type of a signal ) . $ quad$ Given a signal $s equiv (s_t)_{t=1}^M in mathcal{A}^M$, the type of $s$ is the following probability measure on $ mathcal{A}$, constructed through the symbol frequencies of $s$: for $a in mathcal{A}$, $$ m_s( { a } ) := frac{1}{M} sum_{t = 1}^M mathbf{1} { s_t = a } $$ . . $ quad$ As a direct consequence of the above definition, two signals $s, tilde{s} in mathcal{A}^M$ have the same type if and only if the coordinates of $ tilde{s}$ form a permutation of the coordinates of $s$. Let $ textsf{types}( mathcal{A},M)$ denote the collection of all types induced by signals in $ mathcal{A}^M$. The map $ mathcal{A}^M to textsf{types}( mathcal{A},M)$ is effectively a quotient map, and given a probability measure $m in textsf{types}( mathcal{A}, M)$, we let $ textsf{signals}(m)$ denote the collection of signals whose type is $m$: $$ textsf{signals}(m) = { s in mathcal{A}^M : m_s equiv m } $$ . . Theorem. ( Matic, Theorem 2 ) . Consider the set of types, namely $ textsf{types}( mathcal{A}, M)$, with $ # mathcal{A} = n$. One has . $ # , textsf{types}( mathcal{A},M) = {M + n -1 choose M }$ . | $ # , textsf{types}( mathcal{A}, M) leq (M+1)^n$ . | . $ quad$ Let $ mu$ be a general probability measure on $ mathcal{A}$, corresponding to a random variable $Y$ and let $ underline{Y} equiv (Y_t)_{t=1}^M$ be a sequence of i.i.d. random variables. Given fixed signal $s in mathcal{A}^M$, one has $$ begin{align} mathbb{P}( , underline{Y} = s ) &amp;= prod_{t=1}^M mu( Y_t = s_t) &amp;= 2^{ sum_{t=1}^M log_2 mu( Y_t = s_t ) } &amp;= 2^{ sum_{a in mathcal{A} } sum_{t =1}^M mathbf{1} { X_t = a } log_2 mu(Y_t = s_t)} &amp;= 2^{ sum_{a in mathcal{A} } sum_{t =1}^M mathbf{1} { X_t = a } log_2 mu(Y_t = a)} &amp;= 2^{ M left[ sum_{a in mathcal{A} } left( frac{1}{M} sum_{t =1}^M mathbf{1} { X_t = a } right) log_2 mu(Y_t = a) right]} &amp;= 2^{ M left[m_s(a) log_2 mu(a) right] } end{align} $$ . The coefficient of $M$ in the exponent directly above can be written in terms of the relative entropy. . next in section . relative entropy or KL divergence | . . next sections . Gibbs measures (Menon $ S$ 1.4) . | derivation of the Gibbs distribution (from principles of information theory, continuing with Menon $ S$ 1.4) . | decoding scrambled text (from a Bayesian perspective, Menon $ S$ 1.5) . | sampling from a Gibbs distribution (Menon $ S$ 1.6) . | the Metropolis algorithm (Menon $ S$ 1.7, also consulting parts of first chapter of Krauth on detailed balance and Metropolis, respectively $ S$ 1.1.4 and $ S$ 1.1.5) . | Markov chains convergence to stationary distribution (Menon $ S$ 1.8, 1.9) . | . ( For now, omit Krauth $ S$1.1.3 which analyzes the Buffon needle problem. In the long term, it would be nice to practice simulating either the RWs on the symmetric group discussed in a pair of examples, or the $(n+1)$-gram model. ) . $ quad$ The point of building up to this last section is that it lays the theoretical groundwork for how the MCMC method can be used to sample from a Gibbs distribution, which is an abstraction of the simulations run in the motivating blog post. What&#39;s next is to describe the Hamiltonian for the Ising model, from which the Gibbs measures and dynamics are automatically defined. .",
            "url": "https://the-ninth-wave.github.io/stat-mech/2021/12/16/simulation.html",
            "relUrl": "/2021/12/16/simulation.html",
            "date": " • Dec 16, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Lecture 4 ... The Ising model",
            "content": "We use the notation $V Subset mathbb{Z}^d$ for a finite subset of the vertex set of $ mathbb{Z}^d$. Consider a physical system with state space $ Sigma_V triangleq { pm 1 }^V$. The Ising model on $V$, with external (magnetic) field $h in mathbb{R}$ and free boundary conditions (indicated by the &quot;$0$&quot; in the subscript) is characterized by the following Hamiltonian . $$ H_{V, ,h, , 0}^{ small textrm{(Ising)}}(s) triangleq - left( sum_{ substack{x, ,y , in , V x , sim , y}} s_x s_y + h sum_{x , in , V} s_x right), $$where $s in { pm 1 }^V$ is called a spin configuration. The left-most sum is indexed by unordered pairs $x,y$ of adjacent vertices in $V$, inheriting its graph structure from the usual one on the ambient graph $ mathbb{Z}^d$. . Given $V$ as above, we define a configuration of boundary conditions as an element $ omega$ of . $$ { pm1 }^{ mathbb{Z}^d setminus V } $$The Ising model on $V$ with boundary conditions $ omega in { pm1 }^{ mathbb{Z}^d setminus V}$ is characterized by the following Hamiltonian: . $$ H_{V, ,h, , omega}^{ small textrm{(Ising)}}(s) triangleq - left( sum_{ substack{x, ,y , in , V x , sim , y}} s_x s_y + h sum_{x , in , V} s_x right) - H_ partial( omega), $$The last term $H_ partial( omega)$ enforces the boundary conditions $ omega$ as follows: . $$ H_ partial( omega) triangleq - sum_{ substack{ u , sim , v u , in , partial_{ Large circ} V, v , in , partial_{ Large bullet} V } } s_u omega_v . $$Above, $ partial_{ Large circ} V$ is the internal vertex boundary of $V$, . $$ partial_{ Large circ} V triangleq { v in V : exists w in V^{ textrm{c}} text{ such that } v sim w }, $$and $ partial_{ Large bullet} V$ is the external vertex boundary of $V$, . $$ partial_{ Large bullet} V triangleq { v in V^{ , textrm{c}} : exists w in V text{ such that } v sim w }. $$ The Ising model is a nearest-neighbor model, thus all boundary conditions which agree on $ partial_{ Large bullet} V$ induce the same Hamiltonian. For boundary conditions $ omega$ with $ omega_v equiv -1$ (respectively $ omega_v equiv +1$) for all $v in partial_{ Large bullet} V$, we denote the associated Hamiltonian by . $$ H_{V, ,h, ,-}^{ small{ textrm{(Ising)}}} quad , text{(respectively } H_{V, ,h, ,+}^{ small{ textrm{(Ising)}}} ,) $$ For inverse temperature parameter $ beta &gt;0$, define the parition function of this model by . $$ mathcal{Z}_{V, , beta, , h, , omega}^{ , small{ textrm{(Ising)}}} triangleq sum_{s , in , Sigma_V} exp left(- beta H_{V, ,h, , omega}^{ , small{ textrm{(Ising)}}}(s) right) $$ We wish to study the limiting free energy for an exhaustive sequence $V_N uparrow mathbb{Z}^d$, first by showing this limit exists. One can show for $V,W subset mathbb{Z}^d$ disjoint, we have . $$ log left( mathcal{Z}_{V cup W, , beta, , h, ,0}^{ , small{ textrm{(Ising)}}} right) geq log left( mathcal{Z}_{V, , beta, ,h, ,0}^{ , small{ textrm{(Ising)}}} right) + log left( mathcal{Z}_{W, , beta, ,h, ,0}^{ , small{ textrm{(Ising)}}} right) $$ This sub-additivity of the free energy allows one to use Fekete&#39;s lemma to deduce that if $V_N$ is an exhaustive sequence of discrete boxes, i.e. . $$ V_N =_{ small{ textrm{set}}} [-N,N]^d cap mathbb{Z}^d, $$ the limit . $$ lim_{N to infty} frac{1}{ # V_N} log mathcal{Z}_{V_N, , beta, ,h, ,0}^{ , small{ textrm{(Ising)}}} $$exists, and lies in $[0, infty]$. . We&#39;ll now compute this limit exactly in $d=1$. We use the transfer matrix method, discovered in the &#39;30s. This method works nicely when the boundary conditions of the model are periodic. For $V_N$ as just above, and in $d=1$, the _Ising Hamiltonian on $V_N$ with periodic boundary conditions_ is the following function of spin configurations $s in V_N^{ pm1}$ . $$ H_{V_N, , h , , circlearrowright}^{ small{ textrm{(Ising)}}}(s) triangleq H_{V_N, ,h, ,0}^{ small{ textrm{(Ising)}}}(s) + s_{-N} s_N $$We will specialize to this setting. Due to the periodicity in the Hamiltonian, it is nearly equivalent to specialize to the sequence of domains . $$ V_N triangleq [0,N-1] cap mathbb{Z}, $$with Hamiltonian given by . $$ H_N(s) = sum_{ i } s_i s_{i+1} + h sum_{i=1}^N s_i, $$with $s_N triangleq s_0$, which is to say that addition in the subscript is defined modulo $N$. .",
            "url": "https://the-ninth-wave.github.io/stat-mech/jupyter/2019/04/08/M450-Lec4.html",
            "relUrl": "/jupyter/2019/04/08/M450-Lec4.html",
            "date": " • Apr 8, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "Lecture 3 ... Large deviations",
            "content": "Recall the definition of a large deviation principle. . . Definition 1.3.1. $ quad$ A sequence of measures $ mu_N$ on a topological space $ mathcal{X}$ satisfies a large deviation principle (LDP) with rate function $I : mathcal{X} to [0, infty]$ if for all $A subset mathcal{X}$, . $$ - inf_{x in A^ circ} I(x) leq liminf_{N to infty} frac{1}{N} log mu_N(A) leq limsup_{N to infty} frac{1}{N} log mu_N(A) leq - inf_{x in overline{A}} I(x), $$where $A^ circ$ and $ overline{A}$ denote the interior of $A$ and the closure of $A$ respectively. . . A sequence $( mu_N)$ satisfying a LDP with rate function $I$ roughly behaves as . $$ mu_N(A) approx exp left( - N inf_{x in A} I(x) right) $$A rate function $I$ is a good rate function if its sublevel sets are compact. . . Theorem 1.3.2 (Varadhan&#39;s Lemma). $ quad$ Let $ mu_N$ be a sequence of probability measures satisfying a LDP with good rate function $I$. Let $ varphi$ be a continuous function with good decay: there is $ gamma &gt;1$ with . $$ limsup_{N to infty} int exp left( , gamma N varphi(x) , right) mu_N( textrm{d} x) &lt; infty. $$Then, . $$ lim_{N to infty} frac{1}{N} log int exp left( , N varphi(x) , right) mu_N( textrm{d} x) = sup_{x in mathcal{X}} left( , varphi(x) -I(x) , right) $$ . Setting $ varphi equiv 0$ recovers the LDP. A good reference is Section 4.3 of Dembo-Zeitouni. . . Theorem 1.3.3 (Bryc&#39;s inverse). $ quad$ Consider a sequence of measures $( mu_N)$ on a topological space $ mathcal{X}$ such that for every bounded, continuous $ varphi : mathcal{X} to mathbb{R}$, we have . $$ lim_{N to infty} frac{1}{N} log int exp( N varphi(x) ) mu_N( textrm{d} x) = sup_{x in mathcal{X}} ( , varphi(x) -I(x) , ). $$Then $( mu_N)$ satisfies a LDP with rate function $I$. . . . Example 2 (Curie-Weiss). $ quad$ This is a mean-field ferromagnet. Let $ Sigma_N = { pm 1 }^N$, and define the disorderless Hamiltonian $H_N : Sigma_N to mathbb{R}$ defined by . $$ H_N^} (s) triangleq frac{1}{N} sum_{i,j} s_i s_j, $$for $s in Sigma_N$. For $ beta &gt;0$, let us introduce the associated Gibbs measure on $ Sigma_N$, it is defined by . $$ mathcal{G}_{N, beta}^{ ,{ small textrm{(cw)}}} (s) triangleq left( mathcal{Z}_{N, beta}^} right)^{-1} exp left( - beta H_N^} (s) right) $$for $s in Sigma_N$, with the usual definition of the partition function: . $$ mathcal{Z}_{N, beta}^} triangleq sum_{s in Sigma_N} exp left( - beta H_N^} (s) right) $$This is the Curie-Weiss model. . . In ferromagnets, the order parameter (object which detects phase transitions) is the magnetization. The finite-N definition of this object is the random variable . $$ m_N (s) triangleq frac{1}{N} sum_{i=1}^N s_i , $$with $$ s sim mathcal{G}_{N, beta}^ { , small{ textrm{(cw)}} } $$ . The magnetization measures the average alignment of the spins. The proof of the following theorem can be found in Bovier&#39;s book on disordered systems, it uses Varadhan&#39;s lemma and Bryc&#39;s inverse. . . Theorem 1.3.4. $ quad$ Below, let $m$ denote the magnetization $m_N(s)$, with $s sim mathcal{G}_{N, beta}^{ small{ textrm{(cw)}}}$. There is $ beta_c &gt; 0$ such that . When $ beta leq beta_c$: for all $ epsilon &gt;0$ there is $c( epsilon) &gt;0$ such that $$ mathcal{G}_{N, beta}^{ , small{ textrm{(cw)}}} left( , m in (- epsilon, epsilon) , right) geq 1- exp(-cN) $$ | When $ beta &gt; beta_c$: there is $m_* equiv m_*( beta) &gt; 0$, such that for all $ epsilon &gt;0$, there is $c( epsilon, beta) &gt;0$ with $$ mathcal{G}_{N, beta}^{ , small{ textrm{(cw)}}} left( , m in (m_* - epsilon, m_* + epsilon) cup (-m_* - epsilon, -m_* + epsilon) , right) geq 1 - exp ( -N c) $$ | . Thus, at high temperatures, namely when $ beta leq beta_c$, the magnetization localizes around $ {0 }$. At low temperatures, it localizes around the two points $ { pm m_*( beta) }$, due to spin-flip symmetry. .",
            "url": "https://the-ninth-wave.github.io/stat-mech/jupyter/2019/04/05/M450-Lec3.html",
            "relUrl": "/jupyter/2019/04/05/M450-Lec3.html",
            "date": " • Apr 5, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "Lecture 2 ... Gaussians",
            "content": "Here is the Gaussian integration by parts formula: . . Lemma 1.2.1. $ quad$ Let $X$ be a centered Gaussian random variable with variance $ sigma^2$. Let $f in C^1( mathbb{R})$ with good decay. Then . $$ mathbb{E} X f(X) = sigma^2 mathbb{E} f&#39;(X) ,. $$More generally, let $X equiv (X_1, dots, X_d)$ be a centered Gaussian vector. Let $F : mathbb{R}^d to mathbb{R}$ be a $C^1$ function with good decay. Then, . $$ mathbb{E} X_1 F(X) = sum_{i=1}^d mathbb{E} X_1 X_i partial_i F(X) $$ . The proof is an exercise. We state a version of integration by parts specific to Gibbs measures induced by Gaussian energy landscapes. . Let $ Sigma$ denote a state space, equipped with a deterministic reference measure $ mu$. . Let $H$ be a centered Gaussian process on $( Sigma, mu)$, and let $ mathcal{G}$ denote the associated Gibbs measure ($ beta =_{ text{set}} 1$), given by . $$ mathcal{G}(s) triangleq frac{1}{ mathcal{Z}} exp left( H(s) right) , mu ( textrm{d} s) $$ Let us use $ mathbf{ prec} cdot mathbf{ succ}$ to denote expectation with respect to the product measure $ mathcal{G}^{ , otimes infty}$: for all $k geq 1$ and functions $f : Sigma^k to mathbb{R}$, . $$ mathbf{ prec} , f , mathbf{ succ} equiv mathbf{ prec} f( s^{(1)}, dots,s^{(k)} ) mathbf{ succ} triangleq int_{ Sigma^k} f( s^{(1)}, dots, s^{(k)} ) mathcal{G}( textrm{d} s^{(1)} ) dots mathcal{G}( textrm{d} s^{(k)} ) $$ Above, the sampling from the product measure $ mathcal{G}$ is done with the same realization of the disorder $H$ in every coordinate. . This is to say, $s^{(1)}, dots, s^{(k)}$ are independent samples from the same random measure $ mathcal{G}$, which we refer to as replicas. . Under this notation, the Gibbs-expected value of the observable $R_{1 text{-}2}$ is denoted . $$ mathbf{ prec} R_{1 text{-}2} mathbf{ succ} $$for instance. Note that given any deterministic observable $f$, the object $ mathbf{ prec} f mathbf{ succ}$ is a random variable; it is the average of a deterministic function with respect to a random probability measure. . . Lemma 1.2.2 (Integration by parts with disordered Gibbs measures). $ quad$ Let $( Sigma, H, mathcal{G})$ be as above, and let $X : Sigma to mathbf{R}$ be a Gaussian process. For $s,t in Sigma$, define $$ C(s,t) triangleq mathbb{E} X(s) H(t) . $$ Then, . $$ mathbb{E} mathbf{ prec} X(s) mathbf{ succ} = mathbb{E} mathbf{ prec} C(s^{(1)}, s^{(1)}) - C(s^{(1)}, s^{(2)}) mathbf{ succ}$$ . To be clear, for fixed $s,t in Sigma$, $C(s,t)$ is a constant. We use this to form a deterministic function $f$ of two replicas, specifically . $$ f ( s^{(1)}, s^{(2)} ) equiv C(s^{(1)}, s^{(1)}) - C(s^{(1)}, s^{(2)}) $$The right-hand side of the display of the above lemma is just . $$ mathbb{E} mathbf{ prec} f mathbf{ succ} $$for this specific choice of $f$. .",
            "url": "https://the-ninth-wave.github.io/stat-mech/jupyter/2019/04/03/M450-Lec2.html",
            "relUrl": "/jupyter/2019/04/03/M450-Lec2.html",
            "date": " • Apr 3, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Lecture 1 ... Introduction",
            "content": ". Example (Mixed $p$-spin models). $ quad$ Let $ Sigma_N$ denote either the hypercube $ { pm 1 }^N$ or the sphere . $$ mathbb{S}_N triangleq { x in mathbb{R}^N : | x |^2 = N }, $$which contains the hypercube. Let $( beta_p)_{p geq 1}$ be a sequence of non-negative numbers, and we suppose . $$ sum_{p geq 1} beta_p^2 &lt; infty . $$Given such a sequence, we define the model function $ xi : mathbb{R} to mathbb{R}$ via . $$ xi(x) triangleq sum_{p geq 1} beta_p^2 x^p $$as well as the associated Hamiltonian $H_N : mathbb{S}_N to mathbb{R}$, where for $s in mathbb{S}$, . $$ H_N(s) := sum_{p geq 1} beta_p N^{- (p-1)/2} sum_{ i_1, dots, i_p } g_{i_1, dots, i_p} s_{i_1} cdot dots cdot s_{i_p}, $$where the $g_{i_1, dots, i_p}$ are i.i.d. standard normal random variables. The model function and the associated Hamiltonian constitute the mixed $p$-spin model associated to $( beta_p)$. . The Hamiltonian is a random function on $ mathbb{S}_N$; the model coefficients $ beta_p$ are taken to be square-summable to ensure that the object $H_N$ is a.s. smooth. . The covariance structure of $H_N$ is a function of the sphere&#39;s geometry: letting $ langle cdot, cdot rangle$ denote the Euclidean inner product, . $$ mathbb{E} H_N (s) H_N(t) = N xi left( N^{-1} langle s , t rangle right) $$ . Let us write . $$ R_{1 text{-}2} := N^{-1} langle s, t rangle equiv frac{ langle s, t rangle } { |s | |t | } $$when $s$ and $t$ are implicit, we refer to $R_{1 text{-}2}$ as their overlap. . When $ xi(x) = x^p$, the model is called the pure $p$-spin model. The pure $2$-spin model is more commonly known as the Sherrington-Kirkpatrick model. We prepend &quot;spherical&quot; to the model name when $ Sigma_N = mathbb{S}_N$ and omit this otherwise. . In any case, we regard $H_N$ as an energy landscape. Here are examples of questions one can ask about $H_N$: . Can we describe the asymptotic topology of the sublevel sets of these models? . | What is the asymptotic ground state energy of such models, i.e., can one show for some $c &lt; 0$ that . | . $$ min_{s in mathbb{S}} frac{H_N(s)}{N} to c quad text{a.s.} , ? $$ Given an inverse temperature parameter $ beta &gt;0 $ (not related to the $ beta_p$), we can form the Gibbs measure $ mu_{N, beta}$ associated to the Hamiltonian $H_N$ and this $ beta$. We suppress the dependence of $ mu_{N, beta}$ on the model function $ xi$. . The Gibbs measure $ mu_{N, beta}$ is thus a random probability measure on the state space $ Sigma_N$. . When $ Sigma_N = mathbb{S}_N$ let us conflate the Gibbs measure $ mathcal{G}_{N, beta}$ with the density which defines it, notationally. The density is with respect to the natural volume measure $ textrm{vol}_N$ on $ mathbb{S}_N$. . $$ mathcal{G}_{N, beta}(s) := frac{1}{ mathcal{Z}_{N, beta} } exp left( - beta H_N(s) right) , textrm{vol}_N( textrm{d} s), $$ where $ mathcal{Z}_{N, beta}$ is the partition function of $ mathcal{G}_{N, beta}$, a random normalizing constant of fundamental importance in statistical mechanics. . When $ Sigma_N = { pm 1 }$, the Gibbs measure $ mathcal{G}_{N, beta}$ is defined analogously. Examples of questions one can ask about these new objects: . Can one compute the limiting free energy of the model, $$ lim_{N to infty} frac{1}{N} log mathcal{Z}_{N, beta} $$ if it exists? | . In the setting of the pure $p$-spin model, for instance, fix a realization of the disorder $g_{i_1, dots, i_p}$. For $ beta &gt;0$ given, consider the associated Gibbs measure $ mathcal{G}_{N, beta}$. . Let $ mathcal{G}_{N, beta}^{ otimes 2}$ denote the law of a pair of configurations (elements $s,t$ of the state space $ Sigma_N$), where the two random variables are sampled from $ mathcal{G}_{N, beta}$, independently of one another. . For such a pair $(s,t) equiv (s_{N, beta}, t_{N, beta}) sim mathcal{G}_{N, beta}^{ otimes 2}$, we can form the overlap random variable . $$ R_{1 text{-}2} triangleq R_{1 text{-}2}(s,t) $$ What can be said about the asymptotic behavior of this random variable, especially as a function of $ beta$? | . Studying the overlap random variable to study the associated Gibbs measure is a way to leverage a phrase I hear repeated by experts, roughly: &quot;we don&#39;t know the ground states of the system, but the system does.&quot; .",
            "url": "https://the-ninth-wave.github.io/stat-mech/jupyter/2019/04/01/M450-Lec1.html",
            "relUrl": "/jupyter/2019/04/01/M450-Lec1.html",
            "date": " • Apr 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://the-ninth-wave.github.io/stat-mech/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://the-ninth-wave.github.io/stat-mech/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}